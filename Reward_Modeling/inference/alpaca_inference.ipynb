{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-09-24 15:30:01,098] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/user/al4263/rlhf/Reward_Modeling\")\n",
    "# change it to you home dir\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import utils\n",
    "from models.reward_enn import RewardENN, RewardENNConfig\n",
    "from models.vanilla_reward import VanillaReward, VanillaRewardConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "HFValidationError",
     "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/shared/share_mala/leon/reward-enn/alpaca_human_preference/lr1e-05-gradient_acc1-train_batch_size4-ref_size10-enn_dim64-num_ref_train10-weight_decay0.01'. Use `repo_type` argument if needed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m#AutoConfig.register(\"reward_enn\", RewardENNConfig)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m config \u001b[39m=\u001b[39m RewardENNConfig(\n\u001b[1;32m      7\u001b[0m     backbone_model_name_or_path\u001b[39m=\u001b[39mbackbone_model_dir,\n\u001b[1;32m      8\u001b[0m     ref_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     lmbda\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0m model \u001b[39m=\u001b[39m RewardENN\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     16\u001b[0m     model_dir,\n\u001b[1;32m     17\u001b[0m     flash_attn\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     18\u001b[0m     fp16\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     19\u001b[0m     bf16\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     20\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m     21\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/transformers/modeling_utils.py:2397\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[39mif\u001b[39;00m is_peft_available():\n\u001b[1;32m   2396\u001b[0m     \u001b[39mif\u001b[39;00m _adapter_model_path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2397\u001b[0m         _adapter_model_path \u001b[39m=\u001b[39m find_adapter_config_file(\n\u001b[1;32m   2398\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   2399\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2400\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2401\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2402\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2403\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2404\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2405\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2406\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2407\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2408\u001b[0m         )\n\u001b[1;32m   2409\u001b[0m     \u001b[39mif\u001b[39;00m _adapter_model_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(_adapter_model_path):\n\u001b[1;32m   2410\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(_adapter_model_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/transformers/utils/peft_utils.py:87\u001b[0m, in \u001b[0;36mfind_adapter_config_file\u001b[0;34m(model_id, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, _commit_hash)\u001b[0m\n\u001b[1;32m     85\u001b[0m         adapter_cached_filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_id, ADAPTER_CONFIG_NAME)\n\u001b[1;32m     86\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     adapter_cached_filename \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m     88\u001b[0m         model_id,\n\u001b[1;32m     89\u001b[0m         ADAPTER_CONFIG_NAME,\n\u001b[1;32m     90\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m     91\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m     92\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m     93\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m     94\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m     95\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m     96\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m     97\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m     98\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49m_commit_hash,\n\u001b[1;32m     99\u001b[0m         _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    100\u001b[0m         _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m adapter_cached_filename\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRepo id must be a string, not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(repo_id)\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/shared/share_mala/leon/reward-enn/alpaca_human_preference/lr1e-05-gradient_acc1-train_batch_size4-ref_size10-enn_dim64-num_ref_train10-weight_decay0.01'. Use `repo_type` argument if needed."
     ]
    }
   ],
   "source": [
    "model_dir = \"/shared/share_mala/leon/reward-enn/alpaca_human_preference/lr1e-05-gradient_acc1-train_batch_size4-ref_size10-enn_dim64-num_ref_train10-weight_decay0.01\"\n",
    "backbone_model_dir = \"/shared/share_mala/leon/llama-3b-sft\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('/shared/share_mala/leon/llama-3b-sft')\n",
    "#AutoConfig.register(\"reward_enn\", RewardENNConfig)\n",
    "\n",
    "config = RewardENNConfig(\n",
    "    backbone_model_name_or_path=backbone_model_dir,\n",
    "    ref_size=10,\n",
    "    enn_hidden_size=64,\n",
    "    enn_output_size=1,\n",
    "    enn_gain=1,\n",
    "    lmbda=1,\n",
    "    )\n",
    "\n",
    "model = RewardENN.from_pretrained(\n",
    "    model_dir,\n",
    "    flash_attn=False,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardENN(\n",
       "  (backbone_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(32001, 3200, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "            (k_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "            (v_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "            (o_proj): Linear(in_features=3200, out_features=3200, bias=False)\n",
       "            (rotary_emb): LlamaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "            (up_proj): Linear(in_features=3200, out_features=8640, bias=False)\n",
       "            (down_proj): Linear(in_features=8640, out_features=3200, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm()\n",
       "          (post_attention_layernorm): LlamaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3200, out_features=32001, bias=False)\n",
       "  )\n",
       "  (reward_head): Linear(in_features=3200, out_features=1, bias=True)\n",
       "  (eta_net): Epinet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=3210, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (p_net): Epinet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=3210, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.eval()\n",
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.weight Parameter containing:\n",
      "tensor([[ 3.5889e-02, -1.8311e-02,  3.7842e-02,  ..., -3.2959e-02,\n",
      "         -3.8574e-02,  9.7046e-03],\n",
      "        [-3.7842e-02, -2.0630e-02,  3.2471e-02,  ..., -1.8799e-02,\n",
      "         -3.4668e-02, -4.1748e-02],\n",
      "        [-3.5645e-02, -1.1536e-02, -2.2217e-02,  ...,  2.6489e-02,\n",
      "          4.3640e-03,  2.8076e-02],\n",
      "        ...,\n",
      "        [ 2.2949e-02, -3.8818e-02, -3.3203e-02,  ...,  4.5300e-05,\n",
      "         -4.4556e-03, -3.1494e-02],\n",
      "        [ 3.9307e-02, -2.7832e-02,  3.4027e-03,  ..., -2.6245e-02,\n",
      "          2.8198e-02,  3.9795e-02],\n",
      "        [-1.6113e-02, -2.8809e-02,  4.0771e-02,  ...,  2.5635e-02,\n",
      "         -2.9663e-02, -2.8931e-02]], dtype=torch.bfloat16, requires_grad=True)\n",
      "model.0.bias Parameter containing:\n",
      "tensor([ 7.6771e-05, -8.6784e-05,  8.0109e-05, -7.2002e-05,  1.4305e-05,\n",
      "        -8.0585e-05, -4.7922e-05, -2.8968e-05, -1.0490e-04, -5.5313e-05,\n",
      "         3.3081e-06,  7.1228e-06, -8.0466e-06, -5.7220e-05,  6.1989e-06,\n",
      "         9.9182e-05, -1.8239e-05,  1.3161e-04, -5.1260e-06,  7.9632e-05,\n",
      "        -1.2457e-05,  2.3723e-05,  1.1846e-06, -9.1076e-05, -4.1246e-05,\n",
      "        -5.7220e-05,  1.0967e-05, -1.0431e-05,  1.4231e-06,  8.6784e-05,\n",
      "        -9.2506e-05, -2.9683e-05, -2.0266e-05, -1.0252e-04, -3.0398e-06,\n",
      "        -7.7248e-05,  1.8835e-05, -1.3733e-04,  8.9645e-05, -1.2398e-04,\n",
      "        -1.4305e-04,  3.3140e-05, -2.5749e-04, -1.0803e-06, -1.9431e-05,\n",
      "        -1.1349e-04, -1.8501e-04, -5.5075e-05,  1.6093e-05, -3.4809e-05,\n",
      "        -6.0558e-05, -1.1921e-04, -4.5300e-05,  1.1873e-04,  2.5034e-05,\n",
      "         5.7044e-08, -1.9073e-04,  5.8889e-05, -1.7583e-06,  2.4080e-05,\n",
      "        -3.4571e-05, -3.7193e-05,  3.9339e-05, -8.2493e-05],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n",
      "model.2.weight Parameter containing:\n",
      "tensor([[ 0.0101,  0.2090, -0.0938,  ...,  0.0820,  0.1377,  0.1943],\n",
      "        [ 0.0801,  0.0432, -0.1484,  ...,  0.0952, -0.0942,  0.1201],\n",
      "        [ 0.0035,  0.1953, -0.2168,  ...,  0.0393, -0.0981, -0.1914],\n",
      "        ...,\n",
      "        [-0.0942,  0.1836, -0.0255,  ..., -0.1396,  0.1826, -0.1924],\n",
      "        [-0.2109, -0.1245, -0.1147,  ..., -0.0588, -0.1445,  0.0242],\n",
      "        [-0.1836, -0.1748,  0.1377,  ...,  0.1436,  0.0723, -0.0884]],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n",
      "model.2.bias Parameter containing:\n",
      "tensor([-4.2439e-05, -1.0252e-04, -1.1015e-04, -5.5790e-05, -1.1635e-04,\n",
      "         4.7445e-05,  1.0729e-05, -4.8876e-05, -1.1587e-04,  3.0160e-05,\n",
      "        -9.1553e-05,  1.2875e-05,  4.1008e-05,  1.1742e-05, -8.5354e-05,\n",
      "        -8.1062e-05, -9.1553e-05, -2.5272e-05, -1.0061e-04, -5.4121e-05,\n",
      "        -6.2943e-05, -5.3883e-05, -8.4400e-05, -2.7537e-05, -6.2466e-05,\n",
      "         3.4571e-05, -3.6001e-05, -7.8678e-05, -4.7445e-05, -1.5974e-05,\n",
      "         8.2016e-05, -3.8385e-05, -1.2064e-04, -4.4584e-05, -5.6267e-05,\n",
      "        -3.9816e-05, -1.5163e-04,  5.7220e-05, -1.5545e-04,  3.6240e-05,\n",
      "         7.6294e-05,  7.4863e-05, -4.6730e-05,  4.9114e-05, -5.8413e-06,\n",
      "        -1.9193e-05, -8.9645e-05,  1.6809e-05, -5.5075e-05, -2.8610e-05,\n",
      "        -6.0797e-05, -6.5804e-05,  6.9618e-05,  2.2173e-05, -1.1563e-05,\n",
      "         2.2888e-05, -7.0095e-05,  4.6492e-05, -1.0312e-05, -7.7724e-05,\n",
      "         2.0266e-05, -2.3007e-05, -2.5868e-05, -3.1471e-05],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n",
      "model.4.weight Parameter containing:\n",
      "tensor([[ 0.0457, -0.1216,  0.2275,  0.1309, -0.0439,  0.2363, -0.1074, -0.1768,\n",
      "          0.0571, -0.1475,  0.2637,  0.2832,  0.1836,  0.0222,  0.0034, -0.2246,\n",
      "          0.0369, -0.0347, -0.1299,  0.1216, -0.2285, -0.2734,  0.1494, -0.1475,\n",
      "          0.2334,  0.0645, -0.2715, -0.1011,  0.0527, -0.1494, -0.2432, -0.0542,\n",
      "          0.0879, -0.1367,  0.0182, -0.1660,  0.0996,  0.1396,  0.2441, -0.0569,\n",
      "          0.0283, -0.1436,  0.2461,  0.0635, -0.1797,  0.0022,  0.2344,  0.1108,\n",
      "         -0.1641, -0.0879, -0.2422,  0.0830,  0.1006, -0.1650, -0.1875, -0.2129,\n",
      "          0.2178,  0.1797,  0.0019,  0.1650, -0.0503,  0.2021,  0.2139, -0.0801],\n",
      "        [-0.1025, -0.1426,  0.2773, -0.2832, -0.1025,  0.1992, -0.1846, -0.2266,\n",
      "          0.2080, -0.0459,  0.2500,  0.0757,  0.0042,  0.0566,  0.0139, -0.2812,\n",
      "         -0.0540,  0.2275,  0.1064,  0.2100,  0.1328, -0.2520, -0.0510,  0.0767,\n",
      "          0.0505, -0.0292,  0.0028, -0.0120, -0.0033, -0.0189, -0.1260, -0.1484,\n",
      "         -0.1689,  0.0427, -0.2363, -0.1387, -0.0012,  0.2041,  0.2559, -0.0835,\n",
      "         -0.0669,  0.2256,  0.1201, -0.0603,  0.0884, -0.0302,  0.0972, -0.1533,\n",
      "          0.1147,  0.0464,  0.0630,  0.2832,  0.1523,  0.0806, -0.2676,  0.2139,\n",
      "          0.1162,  0.1709, -0.0342, -0.1187,  0.0427,  0.2080, -0.0128,  0.2246],\n",
      "        [-0.1143, -0.1689, -0.2461, -0.2617, -0.2695, -0.0337,  0.0850,  0.0781,\n",
      "         -0.0535,  0.2441, -0.1279,  0.0162, -0.2197,  0.1001, -0.1660, -0.0128,\n",
      "          0.1011,  0.0016,  0.0723,  0.2002,  0.0947, -0.1777, -0.1680, -0.1162,\n",
      "         -0.1348,  0.2412, -0.0408, -0.1021,  0.0850,  0.1245, -0.2578,  0.2754,\n",
      "         -0.1318, -0.0457,  0.2471, -0.1079, -0.0034, -0.1465, -0.2090,  0.2559,\n",
      "          0.2793,  0.2598, -0.1689, -0.1953, -0.2275, -0.0408,  0.0986,  0.2520,\n",
      "          0.2637,  0.2334,  0.1299, -0.0500, -0.1777,  0.0282, -0.0762,  0.0977,\n",
      "         -0.1172,  0.0972, -0.2070,  0.1089,  0.2207,  0.0033,  0.0957,  0.0366],\n",
      "        [ 0.0481, -0.0679,  0.0801, -0.0605,  0.1348, -0.0413, -0.2402, -0.1099,\n",
      "         -0.2676,  0.0052,  0.2314, -0.1230,  0.0250, -0.2148,  0.1250,  0.1660,\n",
      "          0.2109, -0.2441, -0.2393,  0.1006, -0.0106, -0.1973,  0.1826,  0.2734,\n",
      "          0.0850, -0.2031,  0.1138,  0.2773,  0.0547, -0.0854, -0.2598, -0.2617,\n",
      "          0.1079,  0.1592,  0.1680, -0.0096,  0.0435, -0.2656, -0.1543, -0.1074,\n",
      "          0.1943,  0.2236,  0.0864, -0.0483, -0.1230,  0.0688, -0.0342, -0.0547,\n",
      "          0.1436,  0.0427,  0.0056,  0.1904, -0.0227, -0.1562, -0.2715, -0.0991,\n",
      "          0.1953, -0.2021, -0.0566,  0.2441, -0.0312,  0.1147,  0.0206,  0.0688],\n",
      "        [-0.2812, -0.2246,  0.0181, -0.1973,  0.1235,  0.0298,  0.1797, -0.0215,\n",
      "         -0.0245, -0.1113,  0.0479, -0.2129,  0.0354,  0.0311, -0.2793,  0.0815,\n",
      "         -0.0593, -0.2500, -0.0579,  0.2812,  0.0275,  0.0039, -0.1992,  0.0718,\n",
      "         -0.1187,  0.0986,  0.2314, -0.0261, -0.0486, -0.1377,  0.0630,  0.2598,\n",
      "         -0.0903, -0.2109, -0.1553,  0.2695,  0.0452,  0.2793,  0.0051,  0.1602,\n",
      "         -0.2812,  0.2773, -0.2520, -0.2070,  0.2559, -0.0557,  0.2578, -0.1104,\n",
      "          0.1494, -0.0732, -0.1914, -0.1377,  0.2793, -0.2539,  0.2168, -0.0859,\n",
      "          0.0603, -0.0967,  0.2119, -0.0474, -0.0688, -0.0109, -0.0435, -0.0025],\n",
      "        [ 0.0559, -0.0771, -0.2090,  0.1104,  0.0045,  0.0884, -0.0078, -0.1226,\n",
      "         -0.1055,  0.2461,  0.0344, -0.1406, -0.0698,  0.0879,  0.2373,  0.2559,\n",
      "         -0.1338,  0.0845, -0.2012, -0.1895, -0.1572, -0.2617, -0.0767,  0.2031,\n",
      "          0.2715,  0.2236,  0.2217,  0.0928,  0.2832,  0.0659, -0.2178, -0.0330,\n",
      "         -0.2520, -0.1680,  0.0092,  0.1660, -0.0967,  0.2832,  0.2236, -0.1289,\n",
      "          0.1089,  0.2695, -0.1108,  0.0913, -0.2441, -0.2051, -0.0518,  0.0461,\n",
      "         -0.0278, -0.1836,  0.1406,  0.0219, -0.1650, -0.2090, -0.1206,  0.1904,\n",
      "         -0.2246, -0.0141,  0.1309,  0.1108, -0.1396, -0.0148, -0.1729,  0.2490],\n",
      "        [-0.0986,  0.1406, -0.1553, -0.1299, -0.0923,  0.0801, -0.1934,  0.2451,\n",
      "          0.2812,  0.1377, -0.2578, -0.0942,  0.1436, -0.2441, -0.1641, -0.1387,\n",
      "          0.2041,  0.2500, -0.1885,  0.0215, -0.1138, -0.0217,  0.1621,  0.1494,\n",
      "          0.0967, -0.0415, -0.1357,  0.0258, -0.2246,  0.0081,  0.0189,  0.0610,\n",
      "          0.0219,  0.2041,  0.0139,  0.0713,  0.2520,  0.2197,  0.1699,  0.1079,\n",
      "         -0.1455,  0.1094,  0.2139,  0.0742,  0.1245, -0.2275,  0.2832, -0.1177,\n",
      "          0.0305, -0.2676, -0.0432, -0.0201, -0.0371,  0.0562, -0.0352, -0.0664,\n",
      "          0.1357, -0.2412,  0.2002,  0.2793, -0.0107,  0.2334, -0.1494, -0.1875],\n",
      "        [-0.2461,  0.1738, -0.1099, -0.1289,  0.0247,  0.0309, -0.0820, -0.0593,\n",
      "         -0.0884,  0.1748, -0.1162,  0.0374,  0.1230,  0.0530,  0.2422,  0.1660,\n",
      "          0.1021, -0.0449, -0.1602, -0.2021,  0.2324, -0.2852, -0.0092,  0.1904,\n",
      "         -0.2246,  0.0938,  0.1387,  0.0903, -0.0004, -0.0364, -0.2100,  0.2471,\n",
      "          0.1777,  0.1475, -0.1797,  0.2793, -0.1797, -0.1118,  0.0547,  0.0527,\n",
      "         -0.0481,  0.2393,  0.2490,  0.0064, -0.0264,  0.2617, -0.0952, -0.0718,\n",
      "          0.1113,  0.2617, -0.2207,  0.0096,  0.2080, -0.0248, -0.2070,  0.0615,\n",
      "         -0.2461, -0.0359, -0.2598, -0.1738, -0.2012,  0.0991,  0.1963,  0.1855],\n",
      "        [ 0.2021, -0.1602, -0.1143, -0.2539, -0.1738, -0.1069, -0.2461, -0.0593,\n",
      "          0.2090,  0.0952,  0.1357,  0.1387,  0.0238, -0.1196,  0.0889,  0.1108,\n",
      "         -0.0275, -0.0588,  0.0132, -0.1533,  0.0209,  0.2754, -0.0522,  0.2031,\n",
      "          0.2178,  0.0703, -0.1309, -0.2012,  0.1592,  0.2812, -0.0884, -0.2598,\n",
      "          0.1562, -0.0947, -0.0613,  0.2295, -0.0933,  0.0859,  0.1069, -0.1553,\n",
      "          0.1611, -0.2119, -0.1221,  0.2324, -0.2012,  0.0481, -0.0112, -0.0157,\n",
      "          0.0417, -0.2021,  0.1543,  0.2676,  0.1187,  0.0825, -0.2715,  0.1367,\n",
      "          0.0603,  0.0718, -0.0325,  0.2451, -0.0038,  0.1680,  0.1250,  0.0461],\n",
      "        [-0.1338,  0.0503,  0.2695, -0.2070,  0.0391,  0.0200, -0.2578,  0.0613,\n",
      "         -0.0593, -0.0540, -0.0684, -0.1436, -0.0757,  0.2832,  0.1074,  0.2695,\n",
      "         -0.2578,  0.2598,  0.1523,  0.0537,  0.1699,  0.2676, -0.1445,  0.1338,\n",
      "         -0.2197,  0.0077,  0.1030,  0.1416,  0.2422, -0.0786, -0.0031,  0.1934,\n",
      "         -0.1865,  0.1953, -0.1719, -0.2432,  0.0684, -0.0339, -0.0583,  0.2070,\n",
      "         -0.2773,  0.1211, -0.1514,  0.1709,  0.2041,  0.0996,  0.0566,  0.1348,\n",
      "         -0.1719,  0.1562, -0.0889,  0.1797, -0.1318, -0.1729,  0.1641,  0.1699,\n",
      "         -0.0245,  0.1523, -0.2393,  0.1582,  0.2656, -0.0369,  0.0302,  0.2637]],\n",
      "       dtype=torch.bfloat16, requires_grad=True)\n",
      "model.4.bias Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.bfloat16,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# print model.eta_net parameters in for loop\n",
    "for name, param in model.eta_net.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0928],\n",
       "        [ 0.3691],\n",
       "        [-0.0977],\n",
       "        [ 2.2344],\n",
       "        [ 1.1719],\n",
       "        [-0.1465],\n",
       "        [ 2.1562],\n",
       "        [-0.0957],\n",
       "        [ 1.9844],\n",
       "        [ 0.6328]], dtype=torch.bfloat16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 10\n",
    "x_data = torch.randn(10 , 3200)\n",
    "z = torch.randn(5, 10)\n",
    "x_data_repeat = x_data.unsqueeze(1).repeat(1, 5, 1)\n",
    "x_data_repeat = x_data_repeat.to(torch.bfloat16)\n",
    "z = z.to(torch.bfloat16)\n",
    "\n",
    "output = model.eta_net(x = x_data_repeat, z=z, return_full_z=False)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from data.data_loader import pairwise_data_tokenized, PairwiseDyadicAugmentedTokenizedData\n",
    "data_dir = \"/user/al4263/rlhf/Reward_Modeling/data/dataset/alpaca_human_preference\"\n",
    "eval_dir = data_dir + \"/eval.json\"\n",
    "joint_eval_dir = data_dir + \"/joint_eval.json\"\n",
    "\n",
    "with open(eval_dir) as f:\n",
    "    eval_raw_data = json.load(f)\n",
    "with open(joint_eval_dir) as f:\n",
    "    joint_eval_raw_data = json.load(f)\n",
    "\n",
    "model_max_length = 512\n",
    "ref_size = 10\n",
    "num_ref_joint_eval = 100\n",
    "project = \"reward-enn\"\n",
    "\n",
    "# round down to the nearst multiple of 8: \n",
    "eval_dataset = pairwise_data_tokenized(eval_raw_data, tokenizer, model_max_length)\n",
    "joint_eval_dataset = PairwiseDyadicAugmentedTokenizedData(joint_eval_raw_data, tokenizer, model_max_length, ref_size, num_ref_joint_eval, project)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=1, shuffle= False)\n",
    "joint_eval_dataloader = torch.utils.data.DataLoader(joint_eval_dataset, batch_size=1, shuffle= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100000\n",
    "ref_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_out_diff tensor([[0.0469]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0032, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(-0.0002, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.0488, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[-1.5234]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(-0.1143, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(-0.0325, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(-1.6719, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[-0.0801]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0029, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(-0.0055, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(-0.0840, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.6758]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0479, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(-0.0048, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.7188, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.9219]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0703, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0337, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(1.0234, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.1348]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0154, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(-0.0139, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.1387, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.4180]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0195, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0111, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.4473, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[-0.2656]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0095, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0009, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(-0.2578, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[-0.8242]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(-0.0505, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(-0.0183, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(-0.8984, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.0938]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(-0.0017, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0101, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.1016, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.4102]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(-0.0317, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(-0.0015, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.3789, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.0918]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0034, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0070, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.1055, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.1133]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0098, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0087, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.1367, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[0.8672]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0311, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0093, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(0.9023, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n",
      "model_out_diff tensor([[1.2344]], device='cuda:0', dtype=torch.bfloat16)\n",
      "eta_diff tensor(0.0649, device='cuda:0', dtype=torch.bfloat16)\n",
      "p_diff tensor(0.0552, device='cuda:0', dtype=torch.bfloat16)\n",
      "reward_diff tensor(1.3438, device='cuda:0', dtype=torch.bfloat16)\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m z_samples \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mrandn(num_samples, ref_size))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m z_samples \u001b[39m=\u001b[39m z_samples\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mbfloat16)\n\u001b[0;32m---> 15\u001b[0m reward_1 \u001b[39m=\u001b[39m model(p_1, p_1_att, z_samples, return_full_z \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     16\u001b[0m reward_2 \u001b[39m=\u001b[39m model(p_2, p_2_att, z_samples, return_full_z \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m \u001b[39m#print('model_out 1', reward_1.model_out)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#print('model_out 2', reward_2.model_out)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/rlhf/Reward_Modeling/models/reward_enn.py:99\u001b[0m, in \u001b[0;36mRewardENN.forward\u001b[0;34m(self, input_ids, attention_mask, z_samples, return_full_z, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_ids, attention_mask, z_samples, return_full_z \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     97\u001b[0m     \u001b[39m# We only compute the rewards and don't compute the logistic regression loss in this function so that it's\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39m# easier to use for later stages of reranking / RL training.\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone_model\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    100\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     \u001b[39m# print('input_ids', input_ids.shape)\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     last_hidden_state \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[1;32m    704\u001b[0m         attention_mask,\n\u001b[1;32m    705\u001b[0m         position_ids,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    709\u001b[0m         hidden_states,\n\u001b[1;32m    710\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    711\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    712\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    713\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    714\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    717\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/rlhf_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_out_diff = []\n",
    "\n",
    "for _, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        p_1, p_2, p_1_att, p_2_att, label = batch\n",
    "        p_1 = p_1.to(device)\n",
    "        p_2 = p_2.to(device)\n",
    "        p_1_att = p_1_att.to(device)\n",
    "        p_2_att = p_2_att.to(device)\n",
    "        label = label.to(device)\n",
    "        # change the mean of z_samples to 10\n",
    "\n",
    "        z_samples = (torch.randn(num_samples, ref_size)).to(device)\n",
    "        z_samples = z_samples.to(torch.bfloat16)\n",
    "        reward_1 = model(p_1, p_1_att, z_samples, return_full_z = True)\n",
    "        reward_2 = model(p_2, p_2_att, z_samples, return_full_z = True)\n",
    "        #print('model_out 1', reward_1.model_out)\n",
    "        #print('model_out 2', reward_2.model_out)\n",
    "        print('model_out_diff', reward_1.model_out - reward_2.model_out)\n",
    "        print('eta_diff', reward_1.eta_list.mean() - reward_2.eta_list.mean())\n",
    "        print('p_diff', reward_1.p_list.mean() - reward_2.p_list.mean())\n",
    "        print('reward_diff', reward_1.reward_list.mean() - reward_2.reward_list.mean())\n",
    "        print('-----------------------------------------')\n",
    "        #print('eta_out 1', reward_1.eta_list.mean())\n",
    "        #print('eta_out 2', reward_2.eta_list.mean())\n",
    "        #print('p_out 1', reward_1.p_list.mean())    \n",
    "        #print('p_out 2', reward_2.p_list.mean())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random tensor of this shape (batch_size, num_Z_size, self.ref_size, self.output_size)\n",
    "batch_size = 1\n",
    "num_Z_size = 100\n",
    "ref_size = 10\n",
    "output_size = 1\n",
    "x_tilda_og = torch.randn(batch_size, num_Z_size, ref_size, output_size)\n",
    "\n",
    "x_tilda_list = []\n",
    "for i in range(500):\n",
    "    z = torch.randn(num_Z_size, ref_size)\n",
    "    z = z.unsqueeze(0).repeat(x_tilda_og.shape[0], 1, 1)\n",
    "    x_tilda = torch.transpose(x_tilda_og, 2, 3) @ z.unsqueeze(3)\n",
    "    x_tilda = torch.mean(x_tilda, dim=1)\n",
    "    x_tilda = x_tilda.view(-1, 1)\n",
    "    x_tilda_list.append(x_tilda.squeeze(1))\n",
    "\n",
    "\n",
    "data = x_tilda_list\n",
    "\n",
    "# Convert to numpy\n",
    "data_np = np.array(data)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(data_np, bins=100, density=True, alpha=0.6, color='g')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data\n",
    "data = x_tilda_list\n",
    "\n",
    "# Convert to numpy\n",
    "data_np = np.array(data)\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(data_np, bins=100, density=True, alpha=0.6, color='g')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlhf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
