[2023-09-24 23:56:31,103] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:38,637] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:56,438] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:56,477] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:56,800] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:56,941] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:56,971] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:56,979] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:57,018] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:56:57,018] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:57:04,589] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:04,589] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:04,681] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:04,681] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:05,287] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:05,287] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:05,362] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:05,362] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:05,376] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:05,377] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:05,383] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:05,383] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:05,390] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:05,390] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:05,412] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:57:05,412] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:57:05,412] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim64-num_ref_train10-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 242
cuda seed 242
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
torch seed 442
cuda seed 442
torch seed 542
cuda seed 542
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-24 23:57:44,337] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-24 23:57:47,062] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-24 23:57:47,063] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-24 23:57:47,063] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-24 23:57:47,069] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-24 23:57:47,069] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-24 23:57:47,069] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-24 23:57:47,069] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-24 23:57:47,069] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-24 23:57:47,069] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-24 23:57:47,069] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-24 23:58:00,045] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-24 23:58:00,046] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-24 23:58:00,048] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.67 GB, percent = 6.5%
[2023-09-24 23:58:00,157] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-24 23:58:00,157] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-24 23:58:00,159] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.67 GB, percent = 6.5%
[2023-09-24 23:58:00,160] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-24 23:58:00,251] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-24 23:58:00,251] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-24 23:58:00,253] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.67 GB, percent = 6.5%
[2023-09-24 23:58:00,255] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-24 23:58:00,263] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-24 23:58:00,264] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-24 23:58:00,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-24 23:58:00,268] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-24 23:58:00,270] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-24 23:58:00,271] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-24 23:58:00,275] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-24 23:58:00,275] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-24 23:58:00,276] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-24 23:58:00,279] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-24 23:58:00,279] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-24 23:58:00,281] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-24 23:58:00,281] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-24 23:58:00,282] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcf881ae690>
[2023-09-24 23:58:00,282] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-24 23:58:00,284] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-24 23:58:00,285] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-24 23:58:00,285] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-24 23:58:00,286] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-24 23:58:00,286] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-24 23:58:00,287] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-24 23:58:00,287] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-24 23:58:00,289] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-24 23:58:00,289] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-24 23:58:00,290] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-24 23:58:00,290] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-24 23:58:00,291] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-24 23:58:00,291] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-24 23:58:00,292] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-24 23:58:00,292] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-24 23:58:00,293] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-24 23:58:00,293] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-24 23:58:00,294] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-24 23:58:00,294] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-24 23:58:00,295] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-24 23:58:00,296] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-24 23:58:00,296] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-24 23:58:00,297] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-24 23:58:00,297] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-24 23:58:00,298] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-24 23:58:00,298] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-24 23:58:00,300] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-24 23:58:00,300] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-24 23:58:00,301] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-24 23:58:00,302] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-24 23:58:00,304] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-24 23:58:00,305] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-24 23:58:00,306] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-24 23:58:00,306] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-24 23:58:00,309] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-24 23:58:00,310] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-24 23:58:00,311] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-24 23:58:00,312] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-24 23:58:00,312] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-24 23:58:00,313] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-24 23:58:00,314] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-24 23:58:00,315] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-24 23:58:00,316] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-24 23:58:00,316] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-24 23:58:00,317] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-24 23:58:00,317] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-24 23:58:00,318] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-24 23:58:00,319] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-24 23:58:00,321] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-24 23:58:00,321] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-24 23:58:00,322] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-24 23:58:00,322] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-24 23:58:00,323] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-24 23:58:00,323] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-24 23:58:00,323] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-24 23:58:00,323] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-24 23:58:00,323] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-24 23:58:00,323] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-24 23:58:00,323] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.2339, accuracy: 0.5059
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5208, true_2_rate: 0.4899
log joint likelihood: tensor(-689.9257812500) joint log likelihood: tensor(-5590.9375000000)
r_win_average: -1.2176, r_win_min: -5.9062, r_win_max: 2.8906, r_win_std: 1.4138
r_lose_average: -3.2494, r_lose_min: -7.2812, r_lose_max: 0.6250, r_lose_std: 1.6379
eta_win_average: -0.3043, eta_win_min: -1.3750, eta_win_max: 0.5312, eta_win_std: 0.2285
eta_lose_average: -0.3958, eta_lose_min: -1.2188, eta_lose_max: 0.2266, eta_lose_std: 0.2127
p_win_average: -0.2813, p_win_min: -0.7656, p_win_max: 0.3750, p_win_std: 0.1644
p_lose_average: -0.3311, p_lose_min: -0.8672, p_lose_max: 0.2227, p_lose_std: 0.1846

eval_z_samples_size: 1000
eval_loss: 1.1807, accuracy: 0.5137
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.5208, true_2_rate: 0.5061
log joint likelihood: tensor(-677.0117187500) joint log likelihood: tensor(-5605.1562500000)
r_win_average: -0.4342, r_win_min: -4.5312, r_win_max: 3.2344, r_win_std: 1.3264
r_lose_average: -2.3230, r_lose_min: -6.4375, r_lose_max: 1.1250, r_lose_std: 1.5864
eta_win_average: 0.0633, eta_win_min: -0.1611, eta_win_max: 0.3223, eta_win_std: 0.0900
eta_lose_average: 0.0573, eta_lose_min: -0.1865, eta_lose_max: 0.3730, eta_lose_std: 0.0902
p_win_average: 0.1268, p_win_min: -0.1963, p_win_max: 0.3477, p_win_std: 0.0750
p_lose_average: 0.1494, p_lose_min: -0.1963, p_lose_max: 0.3516, p_lose_std: 0.0756

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.5103, accuracy: 0.7695
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.7472, true_2_rate: 0.7935
log joint likelihood: tensor(-1005.6523437500) joint log likelihood: tensor(-1564.1250000000)
r_win_average: -0.4407, r_win_min: -2.9219, r_win_max: 1.5078, r_win_std: 0.6297
r_lose_average: -1.3798, r_lose_min: -3.6719, r_lose_max: 1.0625, r_lose_std: 0.7567
eta_win_average: -0.1525, eta_win_min: -0.5742, eta_win_max: 0.1221, eta_win_std: 0.0748
eta_lose_average: -0.1443, eta_lose_min: -0.5859, eta_lose_max: 0.0388, eta_lose_std: 0.0638
p_win_average: -0.4459, p_win_min: -0.6836, p_win_max: -0.0095, p_win_std: 0.0759
p_lose_average: -0.4663, p_lose_min: -0.7695, p_lose_max: -0.0022, p_lose_std: 0.0659

eval_z_samples_size: 1000
eval_loss: 0.5132, accuracy: 0.7734
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.7547, true_2_rate: 0.7935
log joint likelihood: tensor(-995.4375000000) joint log likelihood: tensor(-1561.3671875000)
r_win_average: 0.3682, r_win_min: -2.0000, r_win_max: 2.5312, r_win_std: 0.6123
r_lose_average: -0.5559, r_lose_min: -2.7500, r_lose_max: 1.8984, r_lose_std: 0.7426
eta_win_average: 0.0451, eta_win_min: -0.0449, eta_win_max: 0.1553, eta_win_std: 0.0207
eta_lose_average: 0.0495, eta_lose_min: -0.0306, eta_lose_max: 0.2598, eta_lose_std: 0.0221
p_win_average: 0.1616, p_win_min: 0.0325, p_win_max: 0.3438, p_win_std: 0.0258
p_lose_average: 0.1673, p_lose_min: -0.0576, p_lose_max: 0.2500, p_lose_std: 0.0282

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4365, accuracy: 0.7812
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7962, true_2_rate: 0.7652
log joint likelihood: tensor(-881.1367187500) joint log likelihood: tensor(-1271.9687500000)
r_win_average: 0.3104, r_win_min: -2.8906, r_win_max: 2.4375, r_win_std: 0.9769
r_lose_average: -1.2235, r_lose_min: -4.6250, r_lose_max: 2.1094, r_lose_std: 1.3013
eta_win_average: 0.0266, eta_win_min: -0.4688, eta_win_max: 0.1865, eta_win_std: 0.0636
eta_lose_average: 0.0082, eta_lose_min: -0.1133, eta_lose_max: 0.2168, eta_lose_std: 0.0444
p_win_average: 0.1015, p_win_min: -0.1074, p_win_max: 0.3008, p_win_std: 0.0476
p_lose_average: 0.1152, p_lose_min: -0.0762, p_lose_max: 0.3281, p_lose_std: 0.0434

eval_z_samples_size: 1000
eval_loss: 0.4365, accuracy: 0.7812
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7925, true_2_rate: 0.7692
log joint likelihood: tensor(-877.0839843750) joint log likelihood: tensor(-1275.1386718750)
r_win_average: 0.2355, r_win_min: -2.7969, r_win_max: 2.3438, r_win_std: 0.9556
r_lose_average: -1.2742, r_lose_min: -4.7500, r_lose_max: 1.9844, r_lose_std: 1.2691
eta_win_average: 0.0765, eta_win_min: -0.0056, eta_win_max: 0.1108, eta_win_std: 0.0193
eta_lose_average: 0.0836, eta_lose_min: 0.0122, eta_lose_max: 0.1152, eta_lose_std: 0.0148
p_win_average: -0.0237, p_win_min: -0.1318, p_win_max: 0.0330, p_win_std: 0.0233
p_lose_average: -0.0103, p_lose_min: -0.1201, p_lose_max: 0.0317, p_lose_std: 0.0204

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.4211, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.8075, true_2_rate: 0.8057
log joint likelihood: tensor(-814.7172851562) joint log likelihood: tensor(-1214.5483398438)
r_win_average: -1.4492, r_win_min: -6.2188, r_win_max: 0.9492, r_win_std: 1.1690
r_lose_average: -3.5360, r_lose_min: -7.6875, r_lose_max: 0.5391, r_lose_std: 1.6563
eta_win_average: -0.6513, eta_win_min: -0.9844, eta_win_max: -0.4707, eta_win_std: 0.0895
eta_lose_average: -0.5778, eta_lose_min: -0.9062, eta_lose_max: -0.3965, eta_lose_std: 0.0818
p_win_average: -0.3285, p_win_min: -0.5156, p_win_max: -0.1523, p_win_std: 0.0494
p_lose_average: -0.3493, p_lose_min: -0.5234, p_lose_max: -0.1699, p_lose_std: 0.0464

eval_z_samples_size: 1000
eval_loss: 0.4229, accuracy: 0.8086
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.8075, true_2_rate: 0.8097
log joint likelihood: tensor(-802.1977539062) joint log likelihood: tensor(-1207.4868164062)
r_win_average: -0.1966, r_win_min: -5.0625, r_win_max: 2.2969, r_win_std: 1.1898
r_lose_average: -2.3184, r_lose_min: -6.4375, r_lose_max: 1.8438, r_lose_std: 1.6799
eta_win_average: 0.1715, eta_win_min: 0.1162, eta_win_max: 0.2324, eta_win_std: 0.0151
eta_lose_average: 0.1675, eta_lose_min: 0.1177, eta_lose_max: 0.2100, eta_lose_std: 0.0147
p_win_average: 0.1013, p_win_min: 0.0162, p_win_max: 0.1699, p_win_std: 0.0273
p_lose_average: 0.1227, p_lose_min: 0.0188, p_lose_max: 0.1914, p_lose_std: 0.0248

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.4272, accuracy: 0.8086
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.8226, true_2_rate: 0.7935
log joint likelihood: tensor(-930.3398437500) joint log likelihood: tensor(-1214.9316406250)
r_win_average: -0.5046, r_win_min: -3.5625, r_win_max: 2.1250, r_win_std: 0.9135
r_lose_average: -1.9932, r_lose_min: -5.6250, r_lose_max: 1.4766, r_lose_std: 1.2152
eta_win_average: -0.2700, eta_win_min: -0.5195, eta_win_max: -0.0282, eta_win_std: 0.0447
eta_lose_average: -0.2851, eta_lose_min: -0.4062, eta_lose_max: -0.1406, eta_lose_std: 0.0370
p_win_average: -0.6100, p_win_min: -0.9375, p_win_max: -0.3613, p_win_std: 0.0676
p_lose_average: -0.6656, p_lose_min: -0.8359, p_lose_max: -0.3613, p_lose_std: 0.0631

eval_z_samples_size: 1000
eval_loss: 0.4309, accuracy: 0.8145
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.8302, true_2_rate: 0.7976
log joint likelihood: tensor(-923.6660156250) joint log likelihood: tensor(-1209.7421875000)
r_win_average: 0.3871, r_win_min: -2.5000, r_win_max: 2.7812, r_win_std: 0.8489
r_lose_average: -1.0205, r_lose_min: -4.4688, r_lose_max: 2.1875, r_lose_std: 1.1507
eta_win_average: 0.0931, eta_win_min: -0.0991, eta_win_max: 0.1611, eta_win_std: 0.0204
eta_lose_average: 0.0863, eta_lose_min: -0.1416, eta_lose_max: 0.1338, eta_lose_std: 0.0201
p_win_average: -0.0814, p_win_min: -0.1768, p_win_max: -0.0112, p_win_std: 0.0255
p_lose_average: -0.0642, p_lose_min: -0.1660, p_lose_max: 0.0003, p_lose_std: 0.0237

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.3928, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8075, true_2_rate: 0.8381
log joint likelihood: tensor(-778.0424804688) joint log likelihood: tensor(-1102.6171875000)
r_win_average: -0.6368, r_win_min: -5.5938, r_win_max: 2.0156, r_win_std: 1.3180
r_lose_average: -2.9699, r_lose_min: -8.3125, r_lose_max: 1.1406, r_lose_std: 1.7847
eta_win_average: -0.0147, eta_win_min: -0.2041, eta_win_max: 0.4453, eta_win_std: 0.0760
eta_lose_average: 0.0745, eta_lose_min: -0.1895, eta_lose_max: 0.3691, eta_lose_std: 0.0900
p_win_average: -0.3354, p_win_min: -0.6172, p_win_max: -0.0030, p_win_std: 0.0802
p_lose_average: -0.4006, p_lose_min: -0.7227, p_lose_max: -0.0923, p_lose_std: 0.0879

eval_z_samples_size: 1000
eval_loss: 0.3923, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8113, true_2_rate: 0.8462
log joint likelihood: tensor(-765.5004882812) joint log likelihood: tensor(-1104.1796875000)
r_win_average: -0.6957, r_win_min: -5.8125, r_win_max: 2.1250, r_win_std: 1.3451
r_lose_average: -3.0786, r_lose_min: -8.2500, r_lose_max: 1.1094, r_lose_std: 1.8264
eta_win_average: -0.2146, eta_win_min: -0.2773, eta_win_max: -0.1328, eta_win_std: 0.0167
eta_lose_average: -0.2103, eta_lose_min: -0.2695, eta_lose_max: -0.0894, eta_lose_std: 0.0180
p_win_average: -0.1948, p_win_min: -0.2852, p_win_max: -0.0806, p_win_std: 0.0295
p_lose_average: -0.2250, p_lose_min: -0.3105, p_lose_max: -0.1147, p_lose_std: 0.0322

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.4050, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.8151, true_2_rate: 0.8300
log joint likelihood: tensor(-889.5195312500) joint log likelihood: tensor(-1131.9648437500)
r_win_average: -0.7280, r_win_min: -3.8594, r_win_max: 1.1016, r_win_std: 0.8383
r_lose_average: -2.1824, r_lose_min: -5.2812, r_lose_max: 0.3457, r_lose_std: 1.0738
eta_win_average: -0.2777, eta_win_min: -0.5430, eta_win_max: -0.0679, eta_win_std: 0.0574
eta_lose_average: -0.2595, eta_lose_min: -0.4824, eta_lose_max: -0.1279, eta_lose_std: 0.0500
p_win_average: -0.5568, p_win_min: -0.8672, p_win_max: -0.2891, p_win_std: 0.0566
p_lose_average: -0.5613, p_lose_min: -0.8125, p_lose_max: -0.4023, p_lose_std: 0.0516

eval_z_samples_size: 1000
eval_loss: 0.4060, accuracy: 0.8164
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.8075, true_2_rate: 0.8259
log joint likelihood: tensor(-884.1132812500) joint log likelihood: tensor(-1128.9648437500)
r_win_average: 0.1072, r_win_min: -2.9688, r_win_max: 1.9453, r_win_std: 0.8431
r_lose_average: -1.3460, r_lose_min: -4.3750, r_lose_max: 1.1953, r_lose_std: 1.0833
eta_win_average: 0.1220, eta_win_min: 0.0835, eta_win_max: 0.2031, eta_win_std: 0.0150
eta_lose_average: 0.1311, eta_lose_min: 0.0447, eta_lose_max: 0.1748, eta_lose_std: 0.0160
p_win_average: -0.1219, p_win_min: -0.1904, p_win_max: -0.0718, p_win_std: 0.0197
p_lose_average: -0.1155, p_lose_min: -0.1846, p_lose_max: -0.0703, p_lose_std: 0.0186

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.3711, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4824, prediction_2_rate: 0.5176
true_1_rate: 0.8000, true_2_rate: 0.8583
log joint likelihood: tensor(-732.5959472656) joint log likelihood: tensor(-1066.6533203125)
r_win_average: -1.5636, r_win_min: -7.0625, r_win_max: 2.1406, r_win_std: 1.4416
r_lose_average: -4.0278, r_lose_min: -8.3750, r_lose_max: 0.2471, r_lose_std: 1.7589
eta_win_average: -0.5421, eta_win_min: -0.7188, eta_win_max: -0.4414, eta_win_std: 0.0362
eta_lose_average: -0.5258, eta_lose_min: -0.6328, eta_lose_max: -0.4238, eta_lose_std: 0.0324
p_win_average: -0.3690, p_win_min: -0.4922, p_win_max: -0.1426, p_win_std: 0.0484
p_lose_average: -0.3938, p_lose_min: -0.5742, p_lose_max: -0.2129, p_lose_std: 0.0398

eval_z_samples_size: 1000
eval_loss: 0.3730, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4766, prediction_2_rate: 0.5234
true_1_rate: 0.7887, true_2_rate: 0.8583
log joint likelihood: tensor(-720.4584960938) joint log likelihood: tensor(-1064.2983398438)
r_win_average: -1.0417, r_win_min: -6.5312, r_win_max: 2.6094, r_win_std: 1.4298
r_lose_average: -3.4875, r_lose_min: -7.8750, r_lose_max: 0.7695, r_lose_std: 1.7486
eta_win_average: -0.2345, eta_win_min: -0.3027, eta_win_max: -0.1709, eta_win_std: 0.0148
eta_lose_average: -0.2288, eta_lose_min: -0.2734, eta_lose_max: -0.1797, eta_lose_std: 0.0136
p_win_average: -0.1548, p_win_min: -0.2168, p_win_max: -0.0762, p_win_std: 0.0231
p_lose_average: -0.1505, p_lose_min: -0.2090, p_lose_max: -0.0767, p_lose_std: 0.0221

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.3750, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4844, prediction_2_rate: 0.5156
true_1_rate: 0.7962, true_2_rate: 0.8502
log joint likelihood: tensor(-781.7724609375) joint log likelihood: tensor(-1040.9101562500)
r_win_average: 0.1892, r_win_min: -4.2188, r_win_max: 2.8438, r_win_std: 1.1559
r_lose_average: -1.7860, r_lose_min: -5.4688, r_lose_max: 1.7188, r_lose_std: 1.4383
eta_win_average: 0.4900, eta_win_min: 0.1895, eta_win_max: 0.7539, eta_win_std: 0.0569
eta_lose_average: 0.5002, eta_lose_min: 0.0928, eta_lose_max: 0.6445, eta_lose_std: 0.0530
p_win_average: -0.1749, p_win_min: -0.3965, p_win_max: -0.0349, p_win_std: 0.0493
p_lose_average: -0.1689, p_lose_min: -0.4902, p_lose_max: -0.0703, p_lose_std: 0.0431

eval_z_samples_size: 1000
eval_loss: 0.3743, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.8000, true_2_rate: 0.8502
log joint likelihood: tensor(-777.2246093750) joint log likelihood: tensor(-1042.5751953125)
r_win_average: 0.0654, r_win_min: -4.4375, r_win_max: 2.7500, r_win_std: 1.1719
r_lose_average: -1.9242, r_lose_min: -5.4062, r_lose_max: 1.6250, r_lose_std: 1.4520
eta_win_average: 0.1216, eta_win_min: 0.0977, eta_win_max: 0.2080, eta_win_std: 0.0107
eta_lose_average: 0.1221, eta_lose_min: 0.0952, eta_lose_max: 0.1777, eta_lose_std: 0.0103
p_win_average: 0.0696, p_win_min: 0.0117, p_win_max: 0.1133, p_win_std: 0.0168
p_lose_average: 0.0714, p_lose_min: 0.0078, p_lose_max: 0.1079, p_lose_std: 0.0146

------------------------------------------------------------------------------------------
[2023-09-25 01:25:13,942] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:33,160] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:34,804] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:35,208] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:35,237] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:35,244] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:35,320] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:35,358] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:35,364] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 01:25:40,507] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:40,507] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 01:25:40,507] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 01:25:42,929] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:42,929] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 01:25:43,475] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:43,475] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 01:25:43,477] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:43,477] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 01:25:43,536] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:43,536] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 01:25:43,545] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:43,545] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 01:25:43,579] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:43,579] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 01:25:43,580] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 01:25:43,581] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 442
cuda seed 442
torch seed 242
cuda seed 242
torch seed 542
cuda seed 542torch seed
 342
cuda seed 342
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim64-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed torch seed142 
42
cuda seed cuda seed142 
42
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-25 01:26:23,167] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 01:26:28,699] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 01:26:28,701] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 01:26:28,701] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 01:26:28,707] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 01:26:28,707] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 01:26:28,708] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 01:26:28,711] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 01:26:28,712] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 01:26:28,712] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 01:26:28,712] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-25 01:26:42,645] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 01:26:42,646] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 01:26:42,647] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.89 GB, percent = 6.3%
[2023-09-25 01:26:42,752] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 01:26:42,753] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 01:26:42,754] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.89 GB, percent = 6.3%
[2023-09-25 01:26:42,755] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 01:26:42,850] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 01:26:42,851] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 01:26:42,854] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.89 GB, percent = 6.3%
[2023-09-25 01:26:42,856] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 01:26:42,857] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 01:26:42,858] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 01:26:42,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 01:26:42,859] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 01:26:42,862] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 01:26:42,863] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 01:26:42,864] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 01:26:42,864] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 01:26:42,865] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 01:26:42,866] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 01:26:42,866] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 01:26:42,867] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 01:26:42,867] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 01:26:42,868] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5cc060e590>
[2023-09-25 01:26:42,868] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 01:26:42,870] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 01:26:42,871] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 01:26:42,871] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 01:26:42,872] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 01:26:42,872] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 01:26:42,873] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 01:26:42,873] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 01:26:42,873] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 01:26:42,874] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 01:26:42,874] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 01:26:42,875] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 01:26:42,875] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 01:26:42,876] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 01:26:42,876] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 01:26:42,878] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 01:26:42,878] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 01:26:42,880] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 01:26:42,880] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 01:26:42,881] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 01:26:42,882] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 01:26:42,885] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 01:26:42,885] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 01:26:42,886] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 01:26:42,886] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 01:26:42,887] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 01:26:42,887] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 01:26:42,889] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 01:26:42,889] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 01:26:42,890] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 01:26:42,890] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 01:26:42,891] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 01:26:42,891] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 01:26:42,892] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 01:26:42,892] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 01:26:42,893] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 01:26:42,894] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 01:26:42,895] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 01:26:42,899] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 01:26:42,900] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 01:26:42,900] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 01:26:42,901] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 01:26:42,902] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 01:26:42,902] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 01:26:42,903] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 01:26:42,903] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 01:26:42,904] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 01:26:42,904] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 01:26:42,905] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 01:26:42,905] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 01:26:42,906] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 01:26:42,906] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 01:26:42,907] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 01:26:42,907] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 01:26:42,908] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 01:26:42,908] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 01:26:42,909] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 01:26:42,909] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 01:26:42,910] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 01:26:42,910] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.2339, accuracy: 0.5059
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5208, true_2_rate: 0.4899
log joint likelihood: tensor(-689.9257812500) joint log likelihood: tensor(-5590.9375000000)
r_win_average: -1.2176, r_win_min: -5.9062, r_win_max: 2.8906, r_win_std: 1.4138
r_lose_average: -3.2494, r_lose_min: -7.2812, r_lose_max: 0.6250, r_lose_std: 1.6379
eta_win_average: -0.3043, eta_win_min: -1.3750, eta_win_max: 0.5312, eta_win_std: 0.2285
eta_lose_average: -0.3958, eta_lose_min: -1.2188, eta_lose_max: 0.2266, eta_lose_std: 0.2127
p_win_average: -0.2813, p_win_min: -0.7656, p_win_max: 0.3750, p_win_std: 0.1644
p_lose_average: -0.3311, p_lose_min: -0.8672, p_lose_max: 0.2227, p_lose_std: 0.1846

eval_z_samples_size: 1000
eval_loss: 1.1807, accuracy: 0.5137
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.5208, true_2_rate: 0.5061
log joint likelihood: tensor(-677.0117187500) joint log likelihood: tensor(-5605.1562500000)
r_win_average: -0.4342, r_win_min: -4.5312, r_win_max: 3.2344, r_win_std: 1.3264
r_lose_average: -2.3230, r_lose_min: -6.4375, r_lose_max: 1.1250, r_lose_std: 1.5864
eta_win_average: 0.0633, eta_win_min: -0.1611, eta_win_max: 0.3223, eta_win_std: 0.0900
eta_lose_average: 0.0573, eta_lose_min: -0.1865, eta_lose_max: 0.3730, eta_lose_std: 0.0902
p_win_average: 0.1268, p_win_min: -0.1963, p_win_max: 0.3477, p_win_std: 0.0750
p_lose_average: 0.1494, p_lose_min: -0.1963, p_lose_max: 0.3516, p_lose_std: 0.0756

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.4985, accuracy: 0.7812
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.7774, true_2_rate: 0.7854
log joint likelihood: tensor(-955.3437500000) joint log likelihood: tensor(-1529.5312500000)
r_win_average: 0.1354, r_win_min: -2.4062, r_win_max: 2.4688, r_win_std: 0.6596
r_lose_average: -0.8671, r_lose_min: -3.1719, r_lose_max: 1.5859, r_lose_std: 0.8218
eta_win_average: 0.1793, eta_win_min: -0.2910, eta_win_max: 0.6484, eta_win_std: 0.0667
eta_lose_average: 0.1896, eta_lose_min: -0.1187, eta_lose_max: 0.4004, eta_lose_std: 0.0509
p_win_average: -0.1456, p_win_min: -0.5938, p_win_max: 0.1689, p_win_std: 0.0615
p_lose_average: -0.1589, p_lose_min: -0.9219, p_lose_max: 0.0835, p_lose_std: 0.0584

eval_z_samples_size: 1000
eval_loss: 0.4968, accuracy: 0.7812
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.7698, true_2_rate: 0.7935
log joint likelihood: tensor(-946.2929687500) joint log likelihood: tensor(-1535.9140625000)
r_win_average: 0.0665, r_win_min: -2.2969, r_win_max: 2.3438, r_win_std: 0.6398
r_lose_average: -0.9254, r_lose_min: -3.3125, r_lose_max: 1.4375, r_lose_std: 0.8069
eta_win_average: -0.0344, eta_win_min: -0.1611, eta_win_max: 0.0835, eta_win_std: 0.0242
eta_lose_average: -0.0440, eta_lose_min: -0.2500, eta_lose_max: 0.0457, eta_lose_std: 0.0259
p_win_average: -0.0015, p_win_min: -0.1807, p_win_max: 0.1128, p_win_std: 0.0381
p_lose_average: 0.0170, p_lose_min: -0.1807, p_lose_max: 0.1045, p_lose_std: 0.0369

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4551, accuracy: 0.7930
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.7962, true_2_rate: 0.7895
log joint likelihood: tensor(-947.7656250000) joint log likelihood: tensor(-1301.2773437500)
r_win_average: 0.2124, r_win_min: -3.1094, r_win_max: 2.2812, r_win_std: 0.7551
r_lose_average: -0.9762, r_lose_min: -3.4062, r_lose_max: 1.4531, r_lose_std: 1.0007
eta_win_average: 0.1339, eta_win_min: -0.1709, eta_win_max: 0.2402, eta_win_std: 0.0496
eta_lose_average: 0.1222, eta_lose_min: -0.0718, eta_lose_max: 0.2393, eta_lose_std: 0.0388
p_win_average: -0.2074, p_win_min: -0.3984, p_win_max: 0.0018, p_win_std: 0.0413
p_lose_average: -0.1937, p_lose_min: -0.4844, p_lose_max: -0.0292, p_lose_std: 0.0400

eval_z_samples_size: 1000
eval_loss: 0.4548, accuracy: 0.7832
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7925, true_2_rate: 0.7733
log joint likelihood: tensor(-945.8632812500) joint log likelihood: tensor(-1305.4257812500)
r_win_average: 0.4916, r_win_min: -2.7656, r_win_max: 2.5625, r_win_std: 0.7494
r_lose_average: -0.6909, r_lose_min: -3.1094, r_lose_max: 1.8906, r_lose_std: 0.9948
eta_win_average: 0.0462, eta_win_min: -0.0265, eta_win_max: 0.1206, eta_win_std: 0.0168
eta_lose_average: 0.0432, eta_lose_min: -0.0053, eta_lose_max: 0.1484, eta_lose_std: 0.0159
p_win_average: 0.1579, p_win_min: 0.0530, p_win_max: 0.2217, p_win_std: 0.0256
p_lose_average: 0.1722, p_lose_min: 0.0796, p_lose_max: 0.2363, p_lose_std: 0.0220

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.4250, accuracy: 0.8008
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.7925, true_2_rate: 0.8097
log joint likelihood: tensor(-780.9238281250) joint log likelihood: tensor(-1245.7890625000)
r_win_average: -0.3212, r_win_min: -4.5938, r_win_max: 1.9297, r_win_std: 1.0937
r_lose_average: -2.3961, r_lose_min: -6.7188, r_lose_max: 1.1172, r_lose_std: 1.6818
eta_win_average: -0.1376, eta_win_min: -0.3672, eta_win_max: 0.0825, eta_win_std: 0.0463
eta_lose_average: -0.1250, eta_lose_min: -0.5273, eta_lose_max: 0.0194, eta_lose_std: 0.0526
p_win_average: 0.1895, p_win_min: -0.0154, p_win_max: 0.3457, p_win_std: 0.0610
p_lose_average: 0.2219, p_lose_min: -0.0260, p_lose_max: 0.4238, p_lose_std: 0.0630

eval_z_samples_size: 1000
eval_loss: 0.4233, accuracy: 0.8047
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.7962, true_2_rate: 0.8138
log joint likelihood: tensor(-777.5361328125) joint log likelihood: tensor(-1241.3554687500)
r_win_average: -0.0971, r_win_min: -4.5000, r_win_max: 2.1719, r_win_std: 1.1128
r_lose_average: -2.2148, r_lose_min: -6.6562, r_lose_max: 1.4375, r_lose_std: 1.7172
eta_win_average: 0.3035, eta_win_min: 0.2383, eta_win_max: 0.3652, eta_win_std: 0.0143
eta_lose_average: 0.3019, eta_lose_min: 0.2285, eta_lose_max: 0.3457, eta_lose_std: 0.0155
p_win_average: -0.0275, p_win_min: -0.0732, p_win_max: 0.0383, p_win_std: 0.0174
p_lose_average: -0.0236, p_lose_min: -0.0869, p_lose_max: 0.0549, p_lose_std: 0.0185

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.4326, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8151, true_2_rate: 0.7976
log joint likelihood: tensor(-922.5312500000) joint log likelihood: tensor(-1241.7226562500)
r_win_average: 0.7602, r_win_min: -1.9219, r_win_max: 2.5938, r_win_std: 0.7906
r_lose_average: -0.6021, r_lose_min: -4.1562, r_lose_max: 2.1250, r_lose_std: 1.1397
eta_win_average: 0.2128, eta_win_min: -0.0518, eta_win_max: 0.4043, eta_win_std: 0.0518
eta_lose_average: 0.1821, eta_lose_min: -0.0586, eta_lose_max: 0.3906, eta_lose_std: 0.0536
p_win_average: 0.2267, p_win_min: -0.1846, p_win_max: 0.4004, p_win_std: 0.0742
p_lose_average: 0.2828, p_lose_min: 0.0192, p_lose_max: 0.4727, p_lose_std: 0.0679

eval_z_samples_size: 1000
eval_loss: 0.4351, accuracy: 0.8125
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.8151, true_2_rate: 0.8097
log joint likelihood: tensor(-915.1113281250) joint log likelihood: tensor(-1236.4511718750)
r_win_average: 0.6525, r_win_min: -2.1094, r_win_max: 2.6250, r_win_std: 0.7999
r_lose_average: -0.7145, r_lose_min: -4.2188, r_lose_max: 2.0000, r_lose_std: 1.1457
eta_win_average: 0.0709, eta_win_min: -0.0171, eta_win_max: 0.1621, eta_win_std: 0.0189
eta_lose_average: 0.0657, eta_lose_min: -0.0170, eta_lose_max: 0.1514, eta_lose_std: 0.0171
p_win_average: 0.2602, p_win_min: 0.0942, p_win_max: 0.3496, p_win_std: 0.0352
p_lose_average: 0.2875, p_lose_min: 0.1445, p_lose_max: 0.3906, p_lose_std: 0.0335

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.3953, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8000, true_2_rate: 0.8381
log joint likelihood: tensor(-778.1596679688) joint log likelihood: tensor(-1135.6420898438)
r_win_average: -1.4385, r_win_min: -5.6250, r_win_max: 1.1484, r_win_std: 1.2592
r_lose_average: -3.7020, r_lose_min: -8.6875, r_lose_max: 0.2598, r_lose_std: 1.7693
eta_win_average: -0.1173, eta_win_min: -0.3242, eta_win_max: 0.1699, eta_win_std: 0.0548
eta_lose_average: -0.0939, eta_lose_min: -0.4492, eta_lose_max: 0.1357, eta_lose_std: 0.0620
p_win_average: -0.9980, p_win_min: -1.3125, p_win_max: -0.5898, p_win_std: 0.0995
p_lose_average: -1.0818, p_lose_min: -1.4453, p_lose_max: -0.6406, p_lose_std: 0.1006

eval_z_samples_size: 1000
eval_loss: 0.3943, accuracy: 0.8164
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.7925, true_2_rate: 0.8421
log joint likelihood: tensor(-765.5366210938) joint log likelihood: tensor(-1138.0253906250)
r_win_average: -0.5167, r_win_min: -4.6562, r_win_max: 2.1406, r_win_std: 1.2178
r_lose_average: -2.7184, r_lose_min: -7.5938, r_lose_max: 1.2344, r_lose_std: 1.7303
eta_win_average: -0.0919, eta_win_min: -0.2002, eta_win_max: -0.0097, eta_win_std: 0.0152
eta_lose_average: -0.0866, eta_lose_min: -0.1357, eta_lose_max: 0.0079, eta_lose_std: 0.0151
p_win_average: -0.1026, p_win_min: -0.1934, p_win_max: -0.0055, p_win_std: 0.0253
p_lose_average: -0.1040, p_lose_min: -0.2598, p_lose_max: -0.0095, p_lose_std: 0.0259

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.4111, accuracy: 0.8086
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.8038, true_2_rate: 0.8138
log joint likelihood: tensor(-881.1386718750) joint log likelihood: tensor(-1165.4902343750)
r_win_average: -0.3626, r_win_min: -3.0312, r_win_max: 1.4766, r_win_std: 0.8345
r_lose_average: -1.8135, r_lose_min: -5.0312, r_lose_max: 0.7695, r_lose_std: 1.1124
eta_win_average: -0.3205, eta_win_min: -0.4727, eta_win_max: -0.1895, eta_win_std: 0.0288
eta_lose_average: -0.3053, eta_lose_min: -0.4590, eta_lose_max: -0.1201, eta_lose_std: 0.0325
p_win_average: -0.1401, p_win_min: -0.2480, p_win_max: 0.0198, p_win_std: 0.0353
p_lose_average: -0.1464, p_lose_min: -0.2852, p_lose_max: 0.0072, p_lose_std: 0.0334

eval_z_samples_size: 1000
eval_loss: 0.4106, accuracy: 0.8105
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.8000, true_2_rate: 0.8219
log joint likelihood: tensor(-874.5156250000) joint log likelihood: tensor(-1161.9921875000)
r_win_average: 0.0227, r_win_min: -2.6406, r_win_max: 1.8594, r_win_std: 0.8292
r_lose_average: -1.4266, r_lose_min: -4.5625, r_lose_max: 1.0938, r_lose_std: 1.1087
eta_win_average: -0.0719, eta_win_min: -0.1328, eta_win_max: -0.0457, eta_win_std: 0.0093
eta_lose_average: -0.0758, eta_lose_min: -0.1079, eta_lose_max: -0.0500, eta_lose_std: 0.0090
p_win_average: -0.0034, p_win_min: -0.0752, p_win_max: 0.0703, p_win_std: 0.0212
p_lose_average: 0.0112, p_lose_min: -0.0552, p_lose_max: 0.0981, p_lose_std: 0.0209

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.3752, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4785, prediction_2_rate: 0.5215
true_1_rate: 0.7925, true_2_rate: 0.8583
log joint likelihood: tensor(-764.5058593750) joint log likelihood: tensor(-1087.8857421875)
r_win_average: -2.2193, r_win_min: -6.5938, r_win_max: 0.5156, r_win_std: 1.2308
r_lose_average: -4.3734, r_lose_min: -9.0625, r_lose_max: -0.6562, r_lose_std: 1.5945
eta_win_average: -1.0674, eta_win_min: -1.3047, eta_win_max: -0.8828, eta_win_std: 0.0443
eta_lose_average: -1.0809, eta_lose_min: -1.2500, eta_lose_max: -0.9727, eta_lose_std: 0.0414
p_win_average: -0.6970, p_win_min: -0.9375, p_win_max: -0.3867, p_win_std: 0.0699
p_lose_average: -0.7428, p_lose_min: -1.0078, p_lose_max: -0.5273, p_lose_std: 0.0663

eval_z_samples_size: 1000
eval_loss: 0.3755, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4785, prediction_2_rate: 0.5215
true_1_rate: 0.7962, true_2_rate: 0.8623
log joint likelihood: tensor(-751.8593750000) joint log likelihood: tensor(-1081.8476562500)
r_win_average: -0.2001, r_win_min: -4.4062, r_win_max: 2.5312, r_win_std: 1.1809
r_lose_average: -2.2845, r_lose_min: -6.6250, r_lose_max: 1.3125, r_lose_std: 1.5391
eta_win_average: 0.1799, eta_win_min: 0.1465, eta_win_max: 0.2480, eta_win_std: 0.0126
eta_lose_average: 0.1719, eta_lose_min: 0.1250, eta_lose_max: 0.2197, eta_lose_std: 0.0134
p_win_average: 0.0745, p_win_min: 0.0031, p_win_max: 0.1309, p_win_std: 0.0241
p_lose_average: 0.0942, p_lose_min: -0.0005, p_lose_max: 0.1533, p_lose_std: 0.0216

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.3750, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8075, true_2_rate: 0.8462
log joint likelihood: tensor(-733.8271484375) joint log likelihood: tensor(-1075.5605468750)
r_win_average: -0.3216, r_win_min: -4.9062, r_win_max: 2.5469, r_win_std: 1.2828
r_lose_average: -2.5816, r_lose_min: -7.0625, r_lose_max: 1.4297, r_lose_std: 1.6802
eta_win_average: 0.1522, eta_win_min: -0.0118, eta_win_max: 0.2812, eta_win_std: 0.0468
eta_lose_average: 0.1570, eta_lose_min: -0.1553, eta_lose_max: 0.3145, eta_lose_std: 0.0456
p_win_average: -0.1830, p_win_min: -0.3516, p_win_max: -0.0015, p_win_std: 0.0528
p_lose_average: -0.1370, p_lose_min: -0.3496, p_lose_max: 0.0801, p_lose_std: 0.0590

eval_z_samples_size: 1000
eval_loss: 0.3774, accuracy: 0.8320
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.8113, true_2_rate: 0.8543
log joint likelihood: tensor(-730.8657226562) joint log likelihood: tensor(-1081.6201171875)
r_win_average: 0.1801, r_win_min: -4.4688, r_win_max: 3.0781, r_win_std: 1.3033
r_lose_average: -2.1171, r_lose_min: -6.6250, r_lose_max: 1.9844, r_lose_std: 1.7082
eta_win_average: 0.4964, eta_win_min: 0.4160, eta_win_max: 0.5781, eta_win_std: 0.0181
eta_lose_average: 0.5052, eta_lose_min: 0.3926, eta_lose_max: 0.5859, eta_lose_std: 0.0212
p_win_average: -0.0265, p_win_min: -0.0923, p_win_max: 0.0261, p_win_std: 0.0208
p_lose_average: -0.0196, p_lose_min: -0.0811, p_lose_max: 0.0530, p_lose_std: 0.0208

------------------------------------------------------------------------------------------
[2023-09-25 02:52:50,703] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:09,282] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:09,358] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:09,577] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:09,803] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:10,012] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:10,030] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:10,030] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:10,051] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:53:17,075] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,075] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:53:17,123] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,123] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:53:17,407] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,407] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:53:17,618] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,618] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:53:17,804] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,805] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:53:17,805] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 02:53:17,813] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,813] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:53:17,829] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,829] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:53:17,830] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:53:17,831] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 642
cuda seed 642
torch seed 742
cuda seed 742
torch seed 242
cuda seed 242
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim64-num_ref_train500-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 342
cuda seed 342
torch seed 142
cuda seed 142
torch seed 542
cuda seed 542
torch seed 442
cuda seed 442
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-25 02:53:59,052] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 02:54:02,017] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 02:54:02,017] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 02:54:02,018] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 02:54:02,023] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 02:54:02,024] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 02:54:02,024] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 02:54:02,025] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 02:54:02,026] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 02:54:02,026] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 02:54:02,026] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-25 02:54:13,894] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 02:54:13,895] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 02:54:13,895] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.84 GB, percent = 6.3%
[2023-09-25 02:54:13,999] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 02:54:13,999] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 02:54:14,000] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.84 GB, percent = 6.3%
[2023-09-25 02:54:14,002] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 02:54:14,096] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 02:54:14,097] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 02:54:14,098] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.84 GB, percent = 6.3%
[2023-09-25 02:54:14,100] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 02:54:14,100] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 02:54:14,101] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 02:54:14,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 02:54:14,103] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 02:54:14,104] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 02:54:14,104] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 02:54:14,104] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 02:54:14,106] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 02:54:14,106] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 02:54:14,107] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 02:54:14,107] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 02:54:14,108] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 02:54:14,108] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 02:54:14,108] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1a33192390>
[2023-09-25 02:54:14,109] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 02:54:14,109] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 02:54:14,110] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 02:54:14,110] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 02:54:14,111] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 02:54:14,111] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 02:54:14,112] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 02:54:14,112] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 02:54:14,113] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 02:54:14,113] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 02:54:14,114] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 02:54:14,114] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 02:54:14,115] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 02:54:14,115] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 02:54:14,116] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 02:54:14,116] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 02:54:14,117] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 02:54:14,117] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 02:54:14,118] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 02:54:14,118] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 02:54:14,119] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 02:54:14,120] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 02:54:14,121] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 02:54:14,121] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 02:54:14,122] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 02:54:14,123] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 02:54:14,123] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 02:54:14,124] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 02:54:14,125] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 02:54:14,125] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 02:54:14,126] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 02:54:14,127] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 02:54:14,128] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 02:54:14,128] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 02:54:14,129] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 02:54:14,130] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 02:54:14,132] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 02:54:14,133] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 02:54:14,134] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 02:54:14,135] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 02:54:14,135] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 02:54:14,140] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 02:54:14,141] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 02:54:14,141] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 02:54:14,142] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 02:54:14,142] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 02:54:14,143] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 02:54:14,143] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 02:54:14,145] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 02:54:14,145] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 02:54:14,150] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 02:54:14,150] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 02:54:14,151] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 02:54:14,151] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 02:54:14,152] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 02:54:14,152] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 02:54:14,153] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 02:54:14,154] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 02:54:14,155] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 02:54:14,155] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.2339, accuracy: 0.5059
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5208, true_2_rate: 0.4899
log joint likelihood: tensor(-689.9257812500) joint log likelihood: tensor(-5590.9375000000)
r_win_average: -1.2176, r_win_min: -5.9062, r_win_max: 2.8906, r_win_std: 1.4138
r_lose_average: -3.2494, r_lose_min: -7.2812, r_lose_max: 0.6250, r_lose_std: 1.6379
eta_win_average: -0.3043, eta_win_min: -1.3750, eta_win_max: 0.5312, eta_win_std: 0.2285
eta_lose_average: -0.3958, eta_lose_min: -1.2188, eta_lose_max: 0.2266, eta_lose_std: 0.2127
p_win_average: -0.2813, p_win_min: -0.7656, p_win_max: 0.3750, p_win_std: 0.1644
p_lose_average: -0.3311, p_lose_min: -0.8672, p_lose_max: 0.2227, p_lose_std: 0.1846

eval_z_samples_size: 1000
eval_loss: 1.1807, accuracy: 0.5137
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.5208, true_2_rate: 0.5061
log joint likelihood: tensor(-677.0117187500) joint log likelihood: tensor(-5605.1562500000)
r_win_average: -0.4342, r_win_min: -4.5312, r_win_max: 3.2344, r_win_std: 1.3264
r_lose_average: -2.3230, r_lose_min: -6.4375, r_lose_max: 1.1250, r_lose_std: 1.5864
eta_win_average: 0.0633, eta_win_min: -0.1611, eta_win_max: 0.3223, eta_win_std: 0.0900
eta_lose_average: 0.0573, eta_lose_min: -0.1865, eta_lose_max: 0.3730, eta_lose_std: 0.0902
p_win_average: 0.1268, p_win_min: -0.1963, p_win_max: 0.3477, p_win_std: 0.0750
p_lose_average: 0.1494, p_lose_min: -0.1963, p_lose_max: 0.3516, p_lose_std: 0.0756

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.5090, accuracy: 0.7676
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.7660, true_2_rate: 0.7692
log joint likelihood: tensor(-971.0507812500) joint log likelihood: tensor(-1527.1953125000)
r_win_average: 1.2724, r_win_min: -1.1953, r_win_max: 3.5156, r_win_std: 0.6513
r_lose_average: 0.2866, r_lose_min: -2.2969, r_lose_max: 2.9844, r_lose_std: 0.8120
eta_win_average: 0.8013, eta_win_min: 0.0933, eta_win_max: 1.1406, eta_win_std: 0.1276
eta_lose_average: 0.8035, eta_lose_min: -0.2539, eta_lose_max: 1.0938, eta_lose_std: 0.1372
p_win_average: 0.3914, p_win_min: 0.1250, p_win_max: 0.9844, p_win_std: 0.0868
p_lose_average: 0.3767, p_lose_min: -0.0547, p_lose_max: 0.7305, p_lose_std: 0.0845

eval_z_samples_size: 1000
eval_loss: 0.4946, accuracy: 0.7852
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.7736, true_2_rate: 0.7976
log joint likelihood: tensor(-963.8320312500) joint log likelihood: tensor(-1530.4531250000)
r_win_average: -0.1265, r_win_min: -2.5625, r_win_max: 2.1094, r_win_std: 0.6595
r_lose_average: -1.1131, r_lose_min: -3.4688, r_lose_max: 1.1953, r_lose_std: 0.7987
eta_win_average: 0.0417, eta_win_min: -0.0586, eta_win_max: 0.1504, eta_win_std: 0.0207
eta_lose_average: 0.0429, eta_lose_min: -0.0447, eta_lose_max: 0.0981, eta_lose_std: 0.0179
p_win_average: -0.2560, p_win_min: -0.4492, p_win_max: -0.1553, p_win_std: 0.0351
p_lose_average: -0.2539, p_lose_min: -0.4883, p_lose_max: -0.1631, p_lose_std: 0.0291

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4512, accuracy: 0.7871
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7925, true_2_rate: 0.7814
log joint likelihood: tensor(-923.6796875000) joint log likelihood: tensor(-1284.9531250000)
r_win_average: 1.0473, r_win_min: -2.1875, r_win_max: 3.0625, r_win_std: 0.7940
r_lose_average: -0.1919, r_lose_min: -2.6562, r_lose_max: 2.5469, r_lose_std: 1.0210
eta_win_average: 0.2845, eta_win_min: 0.1318, eta_win_max: 0.5586, eta_win_std: 0.0550
eta_lose_average: 0.2493, eta_lose_min: 0.0981, eta_lose_max: 0.4863, eta_lose_std: 0.0476
p_win_average: 0.5347, p_win_min: 0.2773, p_win_max: 0.7969, p_win_std: 0.0665
p_lose_average: 0.5823, p_lose_min: 0.2031, p_lose_max: 1.0703, p_lose_std: 0.0653

eval_z_samples_size: 1000
eval_loss: 0.4526, accuracy: 0.7832
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7887, true_2_rate: 0.7773
log joint likelihood: tensor(-924.5000000000) joint log likelihood: tensor(-1291.7578125000)
r_win_average: 0.3127, r_win_min: -2.8438, r_win_max: 2.3438, r_win_std: 0.7728
r_lose_average: -0.9005, r_lose_min: -3.2656, r_lose_max: 1.7969, r_lose_std: 1.0104
eta_win_average: -0.0116, eta_win_min: -0.1387, eta_win_max: 0.0444, eta_win_std: 0.0223
eta_lose_average: -0.0006, eta_lose_min: -0.0874, eta_lose_max: 0.0752, eta_lose_std: 0.0201
p_win_average: 0.0963, p_win_min: -0.0227, p_win_max: 0.1963, p_win_std: 0.0319
p_lose_average: 0.1242, p_lose_min: -0.0264, p_lose_max: 0.2207, p_lose_std: 0.0317

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.4221, accuracy: 0.8105
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.8000, true_2_rate: 0.8219
log joint likelihood: tensor(-786.7548828125) joint log likelihood: tensor(-1236.4736328125)
r_win_average: -0.6587, r_win_min: -5.0938, r_win_max: 1.7344, r_win_std: 1.1515
r_lose_average: -2.8151, r_lose_min: -7.3125, r_lose_max: 0.7891, r_lose_std: 1.7533
eta_win_average: -0.5030, eta_win_min: -0.7812, eta_win_max: -0.1670, eta_win_std: 0.0906
eta_lose_average: -0.6014, eta_lose_min: -0.9219, eta_lose_max: -0.2793, eta_lose_std: 0.1054
p_win_average: 0.1831, p_win_min: -0.0698, p_win_max: 0.3848, p_win_std: 0.0666
p_lose_average: 0.2357, p_lose_min: 0.0410, p_lose_max: 0.5312, p_lose_std: 0.0692

eval_z_samples_size: 1000
eval_loss: 0.4207, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.7962, true_2_rate: 0.8178
log joint likelihood: tensor(-769.6035156250) joint log likelihood: tensor(-1229.1132812500)
r_win_average: -0.2831, r_win_min: -4.5000, r_win_max: 1.9922, r_win_std: 1.1175
r_lose_average: -2.3830, r_lose_min: -6.7500, r_lose_max: 1.2188, r_lose_std: 1.6953
eta_win_average: 0.0645, eta_win_min: 0.0190, eta_win_max: 0.1074, eta_win_std: 0.0111
eta_lose_average: 0.0614, eta_lose_min: 0.0112, eta_lose_max: 0.1123, eta_lose_std: 0.0122
p_win_average: -0.0093, p_win_min: -0.0684, p_win_max: 0.0527, p_win_std: 0.0197
p_lose_average: 0.0053, p_lose_min: -0.0713, p_lose_max: 0.0615, p_lose_std: 0.0207

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.4355, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8151, true_2_rate: 0.7976
log joint likelihood: tensor(-922.4238281250) joint log likelihood: tensor(-1243.6718750000)
r_win_average: 0.2484, r_win_min: -2.5938, r_win_max: 2.2500, r_win_std: 0.8260
r_lose_average: -1.1504, r_lose_min: -4.7812, r_lose_max: 1.5703, r_lose_std: 1.1681
eta_win_average: 0.0002, eta_win_min: -0.0879, eta_win_max: 0.0552, eta_win_std: 0.0183
eta_lose_average: -0.0079, eta_lose_min: -0.0933, eta_lose_max: 0.0928, eta_lose_std: 0.0206
p_win_average: -0.1301, p_win_min: -0.2197, p_win_max: -0.0332, p_win_std: 0.0186
p_lose_average: -0.1346, p_lose_min: -0.1885, p_lose_max: -0.0515, p_lose_std: 0.0167

eval_z_samples_size: 1000
eval_loss: 0.4375, accuracy: 0.8027
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8113, true_2_rate: 0.7935
log joint likelihood: tensor(-915.2695312500) joint log likelihood: tensor(-1239.5449218750)
r_win_average: 0.6582, r_win_min: -2.0938, r_win_max: 2.6250, r_win_std: 0.8051
r_lose_average: -0.7113, r_lose_min: -4.2500, r_lose_max: 1.9375, r_lose_std: 1.1457
eta_win_average: 0.0864, eta_win_min: 0.0080, eta_win_max: 0.1230, eta_win_std: 0.0133
eta_lose_average: 0.0798, eta_lose_min: 0.0356, eta_lose_max: 0.1206, eta_lose_std: 0.0136
p_win_average: 0.1935, p_win_min: 0.0762, p_win_max: 0.2832, p_win_std: 0.0286
p_lose_average: 0.2163, p_lose_min: 0.1167, p_lose_max: 0.3438, p_lose_std: 0.0299

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.3918, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.8038, true_2_rate: 0.8462
log joint likelihood: tensor(-767.2719726562) joint log likelihood: tensor(-1127.0595703125)
r_win_average: -1.7841, r_win_min: -5.9375, r_win_max: 0.7930, r_win_std: 1.2619
r_lose_average: -4.0703, r_lose_min: -9.1875, r_lose_max: -0.1045, r_lose_std: 1.7874
eta_win_average: -0.8644, eta_win_min: -1.0938, eta_win_max: -0.6289, eta_win_std: 0.0624
eta_lose_average: -0.9186, eta_lose_min: -1.2578, eta_lose_max: -0.7383, eta_lose_std: 0.0758
p_win_average: -0.4932, p_win_min: -0.7305, p_win_max: -0.3887, p_win_std: 0.0453
p_lose_average: -0.4871, p_lose_min: -0.8359, p_lose_max: -0.3848, p_lose_std: 0.0467

eval_z_samples_size: 1000
eval_loss: 0.3918, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8038, true_2_rate: 0.8421
log joint likelihood: tensor(-749.5854492188) joint log likelihood: tensor(-1132.0556640625)
r_win_average: -0.5219, r_win_min: -4.6562, r_win_max: 2.0938, r_win_std: 1.2350
r_lose_average: -2.7513, r_lose_min: -7.6562, r_lose_max: 1.1641, r_lose_std: 1.7406
eta_win_average: -0.0343, eta_win_min: -0.0835, eta_win_max: -0.0061, eta_win_std: 0.0080
eta_lose_average: -0.0347, eta_lose_min: -0.0747, eta_lose_max: 0.0187, eta_lose_std: 0.0094
p_win_average: -0.0622, p_win_min: -0.1221, p_win_max: -0.0046, p_win_std: 0.0174
p_lose_average: -0.0508, p_lose_min: -0.1270, p_lose_max: 0.0088, p_lose_std: 0.0175

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.4092, accuracy: 0.8145
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.8075, true_2_rate: 0.8219
log joint likelihood: tensor(-877.8164062500) joint log likelihood: tensor(-1162.8125000000)
r_win_average: -0.9672, r_win_min: -3.6719, r_win_max: 0.9414, r_win_std: 0.8374
r_lose_average: -2.4170, r_lose_min: -5.6562, r_lose_max: 0.1377, r_lose_std: 1.1151
eta_win_average: -1.0553, eta_win_min: -1.3359, eta_win_max: -0.8867, eta_win_std: 0.0472
eta_lose_average: -1.0276, eta_lose_min: -1.2266, eta_lose_max: -0.5547, eta_lose_std: 0.0555
p_win_average: 0.0110, p_win_min: -0.1309, p_win_max: 0.1914, p_win_std: 0.0466
p_lose_average: 0.0003, p_lose_min: -0.1357, p_lose_max: 0.1914, p_lose_std: 0.0428

eval_z_samples_size: 1000
eval_loss: 0.4072, accuracy: 0.8145
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.8075, true_2_rate: 0.8219
log joint likelihood: tensor(-869.1308593750) joint log likelihood: tensor(-1161.0664062500)
r_win_average: -0.2196, r_win_min: -2.9531, r_win_max: 1.6406, r_win_std: 0.8455
r_lose_average: -1.6965, r_lose_min: -4.8750, r_lose_max: 0.8789, r_lose_std: 1.1254
eta_win_average: -0.1869, eta_win_min: -0.2734, eta_win_max: -0.1250, eta_win_std: 0.0180
eta_lose_average: -0.2041, eta_lose_min: -0.2715, eta_lose_max: -0.1406, eta_lose_std: 0.0197
p_win_average: -0.1101, p_win_min: -0.1924, p_win_max: -0.0413, p_win_std: 0.0206
p_lose_average: -0.1026, p_lose_min: -0.1914, p_lose_max: -0.0007, p_lose_std: 0.0210

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.3748, accuracy: 0.8320
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4785, prediction_2_rate: 0.5215
true_1_rate: 0.8000, true_2_rate: 0.8664
log joint likelihood: tensor(-755.3496093750) joint log likelihood: tensor(-1084.1298828125)
r_win_average: 0.4045, r_win_min: -3.6875, r_win_max: 3.3125, r_win_std: 1.1545
r_lose_average: -1.6368, r_lose_min: -5.8438, r_lose_max: 1.8594, r_lose_std: 1.5042
eta_win_average: 0.2897, eta_win_min: 0.1494, eta_win_max: 0.4277, eta_win_std: 0.0321
eta_lose_average: 0.2708, eta_lose_min: 0.1699, eta_lose_max: 0.3809, eta_lose_std: 0.0362
p_win_average: 0.5374, p_win_min: 0.1670, p_win_max: 0.7578, p_win_std: 0.0804
p_lose_average: 0.5947, p_lose_min: 0.2793, p_lose_max: 0.8711, p_lose_std: 0.0749

eval_z_samples_size: 1000
eval_loss: 0.3735, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4766, prediction_2_rate: 0.5234
true_1_rate: 0.7962, true_2_rate: 0.8664
log joint likelihood: tensor(-742.4511718750) joint log likelihood: tensor(-1082.2070312500)
r_win_average: -0.2656, r_win_min: -4.4375, r_win_max: 2.4531, r_win_std: 1.1740
r_lose_average: -2.3370, r_lose_min: -6.6562, r_lose_max: 1.2578, r_lose_std: 1.5191
eta_win_average: 0.0954, eta_win_min: 0.0574, eta_win_max: 0.1572, eta_win_std: 0.0118
eta_lose_average: 0.0958, eta_lose_min: 0.0654, eta_lose_max: 0.1348, eta_lose_std: 0.0116
p_win_average: 0.0611, p_win_min: -0.0092, p_win_max: 0.1211, p_win_std: 0.0184
p_lose_average: 0.0705, p_lose_min: -0.0030, p_lose_max: 0.1182, p_lose_std: 0.0172

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.3755, accuracy: 0.8418
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4844, prediction_2_rate: 0.5156
true_1_rate: 0.8151, true_2_rate: 0.8704
log joint likelihood: tensor(-727.8032226562) joint log likelihood: tensor(-1077.0625000000)
r_win_average: 0.5511, r_win_min: -4.0938, r_win_max: 3.6094, r_win_std: 1.3188
r_lose_average: -1.7752, r_lose_min: -6.3750, r_lose_max: 2.2969, r_lose_std: 1.7226
eta_win_average: 0.5304, eta_win_min: 0.4297, eta_win_max: 0.6953, eta_win_std: 0.0419
eta_lose_average: 0.4913, eta_lose_min: 0.3457, eta_lose_max: 0.6211, eta_lose_std: 0.0426
p_win_average: 0.2661, p_win_min: 0.0208, p_win_max: 0.4766, p_win_std: 0.0604
p_lose_average: 0.2906, p_lose_min: 0.0845, p_lose_max: 0.4980, p_lose_std: 0.0557

eval_z_samples_size: 1000
eval_loss: 0.3743, accuracy: 0.8340
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4844, prediction_2_rate: 0.5156
true_1_rate: 0.8075, true_2_rate: 0.8623
log joint likelihood: tensor(-720.0195312500) joint log likelihood: tensor(-1080.5571289062)
r_win_average: -0.3029, r_win_min: -4.9375, r_win_max: 2.6406, r_win_std: 1.3127
r_lose_average: -2.6145, r_lose_min: -7.2188, r_lose_max: 1.4297, r_lose_std: 1.7152
eta_win_average: -0.1850, eta_win_min: -0.2656, eta_win_max: -0.0869, eta_win_std: 0.0256
eta_lose_average: -0.2063, eta_lose_min: -0.3008, eta_lose_max: -0.1099, eta_lose_std: 0.0276
p_win_average: 0.1272, p_win_min: 0.0457, p_win_max: 0.2217, p_win_std: 0.0279
p_lose_average: 0.1494, p_lose_min: 0.0322, p_lose_max: 0.2148, p_lose_std: 0.0254

------------------------------------------------------------------------------------------
[2023-09-25 04:21:10,840] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:27,522] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:28,471] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:28,654] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:28,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:28,919] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:28,966] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:28,966] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:28,972] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:21:34,497] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:34,497] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:21:35,489] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:35,489] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:21:35,765] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:35,765] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:21:35,838] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:35,838] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:21:35,974] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:35,975] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:21:36,020] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:36,021] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:21:36,021] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:36,021] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:21:36,021] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 04:21:36,049] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:21:36,049] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 742
cuda seed 742
torch seed 342
cuda seed 342
torch seed 442
cuda seed 442
torch seed 142
cuda seed 142
torch seed 242
cuda seed 242
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim128-num_ref_train10-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 642
cuda seed 642
torch seed 542
cuda seed 542
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-25 04:22:15,483] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 04:22:20,474] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 04:22:20,475] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 04:22:20,475] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 04:22:20,480] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 04:22:20,480] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 04:22:20,480] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 04:22:20,480] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 04:22:20,480] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 04:22:20,480] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 04:22:20,480] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-25 04:22:33,269] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 04:22:33,270] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 04:22:33,270] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.39 GB, percent = 6.2%
[2023-09-25 04:22:33,368] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 04:22:33,369] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 04:22:33,369] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.39 GB, percent = 6.2%
[2023-09-25 04:22:33,369] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 04:22:33,457] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 04:22:33,458] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 04:22:33,458] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.39 GB, percent = 6.2%
[2023-09-25 04:22:33,459] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 04:22:33,459] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 04:22:33,459] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 04:22:33,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 04:22:33,460] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2c1c5d6190>
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 04:22:33,460] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 04:22:33,461] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 04:22:33,461] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.0767, accuracy: 0.5117
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.5094, true_2_rate: 0.5142
log joint likelihood: tensor(-647.5312500000) joint log likelihood: tensor(-5955.0625000000)
r_win_average: -2.6057, r_win_min: -6.2812, r_win_max: 2.5781, r_win_std: 1.3715
r_lose_average: -4.4027, r_lose_min: -8.1875, r_lose_max: -0.4668, r_lose_std: 1.3920
eta_win_average: -0.5828, eta_win_min: -1.6016, eta_win_max: 0.6641, eta_win_std: 0.3011
eta_lose_average: -0.6857, eta_lose_min: -1.6328, eta_lose_max: 0.4375, eta_lose_std: 0.3068
p_win_average: -1.3579, p_win_min: -2.5781, p_win_max: 0.6328, p_win_std: 0.5596
p_lose_average: -1.2278, p_lose_min: -2.6094, p_lose_max: 0.2871, p_lose_std: 0.6303

eval_z_samples_size: 1000
eval_loss: 1.2144, accuracy: 0.5098
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5245, true_2_rate: 0.4939
log joint likelihood: tensor(-643.5273437500) joint log likelihood: tensor(-5951.7812500000)
r_win_average: -0.3894, r_win_min: -4.4688, r_win_max: 3.2031, r_win_std: 1.3413
r_lose_average: -2.3354, r_lose_min: -6.5000, r_lose_max: 1.1094, r_lose_std: 1.6202
eta_win_average: -0.1085, eta_win_min: -0.3438, eta_win_max: 0.0688, eta_win_std: 0.0552
eta_lose_average: -0.1007, eta_lose_min: -0.3438, eta_lose_max: 0.0796, eta_lose_std: 0.0586
p_win_average: 0.3430, p_win_min: 0.0267, p_win_max: 0.6406, p_win_std: 0.0979
p_lose_average: 0.2952, p_lose_min: -0.0356, p_lose_max: 0.5586, p_lose_std: 0.1173

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.5308, accuracy: 0.7500
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.7283, true_2_rate: 0.7733
log joint likelihood: tensor(-951.7890625000) joint log likelihood: tensor(-1726.2226562500)
r_win_average: 0.3271, r_win_min: -2.1406, r_win_max: 3.2500, r_win_std: 0.6288
r_lose_average: -0.5939, r_lose_min: -3.6406, r_lose_max: 1.8203, r_lose_std: 0.7459
eta_win_average: -0.0516, eta_win_min: -0.5781, eta_win_max: 0.3555, eta_win_std: 0.0786
eta_lose_average: -0.0480, eta_lose_min: -0.5703, eta_lose_max: 0.4922, eta_lose_std: 0.0799
p_win_average: -0.1552, p_win_min: -0.6367, p_win_max: -0.0091, p_win_std: 0.0713
p_lose_average: -0.1596, p_lose_min: -0.5742, p_lose_max: 0.0479, p_lose_std: 0.0651

eval_z_samples_size: 1000
eval_loss: 0.5266, accuracy: 0.7461
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.7283, true_2_rate: 0.7652
log joint likelihood: tensor(-949.3593750000) joint log likelihood: tensor(-1718.0859375000)
r_win_average: 0.4821, r_win_min: -1.9766, r_win_max: 3.2656, r_win_std: 0.6318
r_lose_average: -0.4449, r_lose_min: -2.9844, r_lose_max: 1.8906, r_lose_std: 0.7427
eta_win_average: 0.0286, eta_win_min: -0.0708, eta_win_max: 0.2090, eta_win_std: 0.0245
eta_lose_average: 0.0331, eta_lose_min: -0.0654, eta_lose_max: 0.2051, eta_lose_std: 0.0235
p_win_average: -0.0833, p_win_min: -0.3242, p_win_max: 0.0557, p_win_std: 0.0324
p_lose_average: -0.0888, p_lose_min: -0.2500, p_lose_max: 0.0840, p_lose_std: 0.0303

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4578, accuracy: 0.7969
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7962, true_2_rate: 0.7976
log joint likelihood: tensor(-941.9335937500) joint log likelihood: tensor(-1331.1250000000)
r_win_average: 0.2309, r_win_min: -2.2812, r_win_max: 2.7812, r_win_std: 0.7753
r_lose_average: -0.9620, r_lose_min: -3.1406, r_lose_max: 1.7578, r_lose_std: 0.9428
eta_win_average: -0.3176, eta_win_min: -0.5195, eta_win_max: -0.1279, eta_win_std: 0.0407
eta_lose_average: -0.2902, eta_lose_min: -0.5000, eta_lose_max: -0.1768, eta_lose_std: 0.0427
p_win_average: 0.2277, p_win_min: -0.0598, p_win_max: 0.3691, p_win_std: 0.0442
p_lose_average: 0.2274, p_lose_min: 0.0454, p_lose_max: 0.3574, p_lose_std: 0.0382

eval_z_samples_size: 1000
eval_loss: 0.4536, accuracy: 0.7910
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.7887, true_2_rate: 0.7935
log joint likelihood: tensor(-934.5117187500) joint log likelihood: tensor(-1324.0781250000)
r_win_average: 0.0535, r_win_min: -2.5625, r_win_max: 2.6562, r_win_std: 0.7961
r_lose_average: -1.1796, r_lose_min: -3.5469, r_lose_max: 1.6172, r_lose_std: 0.9790
eta_win_average: -0.0625, eta_win_min: -0.1699, eta_win_max: 0.0304, eta_win_std: 0.0213
eta_lose_average: -0.0654, eta_lose_min: -0.1621, eta_lose_max: -0.0063, eta_lose_std: 0.0177
p_win_average: -0.2057, p_win_min: -0.2715, p_win_max: -0.0603, p_win_std: 0.0244
p_lose_average: -0.2142, p_lose_min: -0.2598, p_lose_max: -0.0923, p_lose_std: 0.0189

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.4238, accuracy: 0.8008
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.7887, true_2_rate: 0.8138
log joint likelihood: tensor(-762.0693359375) joint log likelihood: tensor(-1274.6953125000)
r_win_average: 0.6326, r_win_min: -3.8438, r_win_max: 2.8438, r_win_std: 1.0431
r_lose_average: -1.2883, r_lose_min: -5.5000, r_lose_max: 2.3594, r_lose_std: 1.5741
eta_win_average: 0.5496, eta_win_min: 0.0820, eta_win_max: 0.8516, eta_win_std: 0.0760
eta_lose_average: 0.6098, eta_lose_min: 0.2461, eta_lose_max: 0.9570, eta_lose_std: 0.0845
p_win_average: 0.2777, p_win_min: 0.0359, p_win_max: 0.5312, p_win_std: 0.0610
p_lose_average: 0.3149, p_lose_min: 0.0486, p_lose_max: 0.5430, p_lose_std: 0.0675

eval_z_samples_size: 1000
eval_loss: 0.4236, accuracy: 0.8027
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.8038, true_2_rate: 0.8016
log joint likelihood: tensor(-751.9667968750) joint log likelihood: tensor(-1275.5625000000)
r_win_average: 0.0222, r_win_min: -4.8125, r_win_max: 2.3750, r_win_std: 1.1005
r_lose_average: -2.0162, r_lose_min: -6.4375, r_lose_max: 1.8516, r_lose_std: 1.6833
eta_win_average: -0.1765, eta_win_min: -0.2480, eta_win_max: -0.0618, eta_win_std: 0.0231
eta_lose_average: -0.1888, eta_lose_min: -0.2539, eta_lose_max: -0.0703, eta_lose_std: 0.0236
p_win_average: 0.3920, p_win_min: 0.2158, p_win_max: 0.4746, p_win_std: 0.0312
p_lose_average: 0.3873, p_lose_min: 0.2812, p_lose_max: 0.4863, p_lose_std: 0.0288

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.4399, accuracy: 0.7988
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.7962, true_2_rate: 0.8016
log joint likelihood: tensor(-903.1835937500) joint log likelihood: tensor(-1273.1171875000)
r_win_average: 0.1003, r_win_min: -2.5781, r_win_max: 2.4062, r_win_std: 0.7895
r_lose_average: -1.2377, r_lose_min: -4.5938, r_lose_max: 1.2812, r_lose_std: 1.1109
eta_win_average: -0.0232, eta_win_min: -0.1416, eta_win_max: 0.2422, eta_win_std: 0.0361
eta_lose_average: -0.0368, eta_lose_min: -0.1826, eta_lose_max: 0.1279, eta_lose_std: 0.0362
p_win_average: -0.1909, p_win_min: -0.3418, p_win_max: -0.0337, p_win_std: 0.0436
p_lose_average: -0.1779, p_lose_min: -0.3008, p_lose_max: 0.0894, p_lose_std: 0.0450

eval_z_samples_size: 1000
eval_loss: 0.4382, accuracy: 0.8027
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.8000, true_2_rate: 0.8057
log joint likelihood: tensor(-896.9980468750) joint log likelihood: tensor(-1273.8203125000)
r_win_average: 0.3554, r_win_min: -2.2969, r_win_max: 2.5156, r_win_std: 0.7857
r_lose_average: -0.9880, r_lose_min: -4.2812, r_lose_max: 1.5547, r_lose_std: 1.1126
eta_win_average: -0.1635, eta_win_min: -0.2275, eta_win_max: -0.1167, eta_win_std: 0.0126
eta_lose_average: -0.1629, eta_lose_min: -0.2334, eta_lose_max: -0.1167, eta_lose_std: 0.0116
p_win_average: 0.2045, p_win_min: 0.0923, p_win_max: 0.2578, p_win_std: 0.0165
p_lose_average: 0.1982, p_lose_min: 0.1455, p_lose_max: 0.2656, p_lose_std: 0.0157

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.3987, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4844, prediction_2_rate: 0.5156
true_1_rate: 0.7962, true_2_rate: 0.8502
log joint likelihood: tensor(-703.1757812500) joint log likelihood: tensor(-1185.9887695312)
r_win_average: 0.8158, r_win_min: -3.8750, r_win_max: 3.3906, r_win_std: 1.2578
r_lose_average: -1.4536, r_lose_min: -6.7188, r_lose_max: 2.5938, r_lose_std: 1.7841
eta_win_average: 0.2354, eta_win_min: 0.0267, eta_win_max: 0.5078, eta_win_std: 0.0642
eta_lose_average: 0.2815, eta_lose_min: 0.0532, eta_lose_max: 0.5195, eta_lose_std: 0.0766
p_win_average: 0.7902, p_win_min: 0.4121, p_win_max: 1.0781, p_win_std: 0.0826
p_lose_average: 0.8259, p_lose_min: 0.4668, p_lose_max: 1.0234, p_lose_std: 0.0837

eval_z_samples_size: 1000
eval_loss: 0.3984, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8038, true_2_rate: 0.8502
log joint likelihood: tensor(-703.2612304688) joint log likelihood: tensor(-1185.6259765625)
r_win_average: -0.3416, r_win_min: -5.2188, r_win_max: 2.1250, r_win_std: 1.3093
r_lose_average: -2.6987, r_lose_min: -8.0000, r_lose_max: 1.5234, r_lose_std: 1.8610
eta_win_average: -0.0269, eta_win_min: -0.0781, eta_win_max: 0.0138, eta_win_std: 0.0128
eta_lose_average: -0.0298, eta_lose_min: -0.1504, eta_lose_max: 0.0153, eta_lose_std: 0.0140
p_win_average: -0.1059, p_win_min: -0.1699, p_win_max: -0.0089, p_win_std: 0.0234
p_lose_average: -0.1068, p_lose_min: -0.1768, p_lose_max: -0.0074, p_lose_std: 0.0277

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.4146, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.8264, true_2_rate: 0.8340
log joint likelihood: tensor(-853.4628906250) joint log likelihood: tensor(-1184.0175781250)
r_win_average: 0.0980, r_win_min: -2.9219, r_win_max: 1.8984, r_win_std: 0.7999
r_lose_average: -1.2686, r_lose_min: -3.9531, r_lose_max: 1.1250, r_lose_std: 1.0197
eta_win_average: 0.4401, eta_win_min: 0.1162, eta_win_max: 0.6914, eta_win_std: 0.0561
eta_lose_average: 0.4503, eta_lose_min: 0.2539, eta_lose_max: 0.7617, eta_lose_std: 0.0526
p_win_average: -0.5573, p_win_min: -0.7461, p_win_max: -0.2617, p_win_std: 0.0646
p_lose_average: -0.5236, p_lose_min: -0.7461, p_lose_max: -0.2100, p_lose_std: 0.0701

eval_z_samples_size: 1000
eval_loss: 0.4111, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.8113, true_2_rate: 0.8259
log joint likelihood: tensor(-848.3105468750) joint log likelihood: tensor(-1181.3789062500)
r_win_average: 0.0859, r_win_min: -3.0938, r_win_max: 1.9141, r_win_std: 0.8284
r_lose_average: -1.3348, r_lose_min: -4.2812, r_lose_max: 1.1250, r_lose_std: 1.0716
eta_win_average: -0.0012, eta_win_min: -0.0437, eta_win_max: 0.0811, eta_win_std: 0.0166
eta_lose_average: -0.0063, eta_lose_min: -0.0918, eta_lose_max: 0.0845, eta_lose_std: 0.0162
p_win_average: -0.1291, p_win_min: -0.1797, p_win_max: -0.0364, p_win_std: 0.0178
p_lose_average: -0.1322, p_lose_min: -0.1943, p_lose_max: -0.0835, p_lose_std: 0.0156

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.3804, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8075, true_2_rate: 0.8543
log joint likelihood: tensor(-726.5341796875) joint log likelihood: tensor(-1109.0859375000)
r_win_average: -0.5411, r_win_min: -5.3438, r_win_max: 2.2031, r_win_std: 1.2095
r_lose_average: -2.6424, r_lose_min: -6.7812, r_lose_max: 1.3828, r_lose_std: 1.5493
eta_win_average: -0.1173, eta_win_min: -0.4648, eta_win_max: 0.1680, eta_win_std: 0.0783
eta_lose_average: -0.2076, eta_lose_min: -0.5938, eta_lose_max: 0.0405, eta_lose_std: 0.0980
p_win_average: -0.1294, p_win_min: -0.2812, p_win_max: 0.0400, p_win_std: 0.0553
p_lose_average: -0.1396, p_lose_min: -0.3828, p_lose_max: 0.0713, p_lose_std: 0.0627

eval_z_samples_size: 1000
eval_loss: 0.3787, accuracy: 0.8340
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8113, true_2_rate: 0.8583
log joint likelihood: tensor(-722.6171875000) joint log likelihood: tensor(-1111.7500000000)
r_win_average: -0.5581, r_win_min: -5.0938, r_win_max: 2.1094, r_win_std: 1.1597
r_lose_average: -2.5673, r_lose_min: -6.5625, r_lose_max: 1.1797, r_lose_std: 1.4822
eta_win_average: -0.0595, eta_win_min: -0.1504, eta_win_max: 0.0138, eta_win_std: 0.0220
eta_lose_average: -0.0511, eta_lose_min: -0.1660, eta_lose_max: 0.0053, eta_lose_std: 0.0205
p_win_average: -0.2048, p_win_min: -0.3008, p_win_max: -0.0928, p_win_std: 0.0269
p_lose_average: -0.2205, p_lose_min: -0.2969, p_lose_max: -0.1172, p_lose_std: 0.0241

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.3821, accuracy: 0.8359
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.8226, true_2_rate: 0.8502
log joint likelihood: tensor(-700.5185546875) joint log likelihood: tensor(-1103.7089843750)
r_win_average: 0.0838, r_win_min: -5.3125, r_win_max: 3.0312, r_win_std: 1.3438
r_lose_average: -2.2631, r_lose_min: -6.8750, r_lose_max: 2.0000, r_lose_std: 1.7542
eta_win_average: 0.3828, eta_win_min: 0.1494, eta_win_max: 0.7266, eta_win_std: 0.0605
eta_lose_average: 0.3212, eta_lose_min: 0.0442, eta_lose_max: 0.4863, eta_lose_std: 0.0742
p_win_average: -0.1815, p_win_min: -0.3184, p_win_max: 0.2061, p_win_std: 0.0692
p_lose_average: -0.1764, p_lose_min: -0.3398, p_lose_max: 0.0176, p_lose_std: 0.0648

eval_z_samples_size: 1000
eval_loss: 0.3796, accuracy: 0.8359
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.8226, true_2_rate: 0.8502
log joint likelihood: tensor(-696.7758789062) joint log likelihood: tensor(-1104.5615234375)
r_win_average: -0.2352, r_win_min: -5.7188, r_win_max: 2.7031, r_win_std: 1.3472
r_lose_average: -2.5988, r_lose_min: -7.4062, r_lose_max: 1.5859, r_lose_std: 1.7793
eta_win_average: -0.1351, eta_win_min: -0.3457, eta_win_max: 0.1279, eta_win_std: 0.0504
eta_lose_average: -0.1962, eta_lose_min: -0.3594, eta_lose_max: 0.0464, eta_lose_std: 0.0605
p_win_average: 0.0173, p_win_min: -0.1060, p_win_max: 0.1201, p_win_std: 0.0292
p_lose_average: 0.0048, p_lose_min: -0.1030, p_lose_max: 0.0898, p_lose_std: 0.0298

------------------------------------------------------------------------------------------
[2023-09-25 05:49:00,035] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:14,780] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:14,920] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:15,140] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:15,202] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:15,285] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:15,337] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:15,345] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:15,360] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 05:49:20,784] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:20,784] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 05:49:21,132] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:21,132] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 05:49:21,374] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:21,375] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 05:49:21,526] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:21,526] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 05:49:21,544] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:21,544] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 05:49:21,555] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:21,555] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 05:49:21,555] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 05:49:21,557] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:21,557] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 05:49:21,600] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 05:49:21,600] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 742
cuda seed 742
torch seed 242
cuda seed 242
torch seed 142
cuda seed 142
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim128-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 442
cuda seed 442
torch seed 642
cuda seed 642
torch seed 342
cuda seed 342
torch seed 542
cuda seed 542
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-25 05:50:00,727] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 05:50:05,354] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 05:50:05,355] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 05:50:05,355] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 05:50:05,360] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 05:50:05,360] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 05:50:05,360] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 05:50:05,360] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 05:50:05,360] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 05:50:05,360] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 05:50:05,360] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-25 05:50:16,244] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 05:50:16,245] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 05:50:16,245] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.41 GB, percent = 6.2%
[2023-09-25 05:50:16,344] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 05:50:16,345] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 05:50:16,345] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.41 GB, percent = 6.2%
[2023-09-25 05:50:16,345] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 05:50:16,434] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 05:50:16,434] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 05:50:16,434] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.41 GB, percent = 6.2%
[2023-09-25 05:50:16,436] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 05:50:16,436] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 05:50:16,436] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 05:50:16,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 05:50:16,436] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 05:50:16,436] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 05:50:16,436] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 05:50:16,436] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 05:50:16,436] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd125406990>
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 05:50:16,437] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 05:50:16,438] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 05:50:16,438] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 05:50:16,438] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 05:50:16,438] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 05:50:16,438] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.0767, accuracy: 0.5117
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.5094, true_2_rate: 0.5142
log joint likelihood: tensor(-647.5312500000) joint log likelihood: tensor(-5955.0625000000)
r_win_average: -2.6057, r_win_min: -6.2812, r_win_max: 2.5781, r_win_std: 1.3715
r_lose_average: -4.4027, r_lose_min: -8.1875, r_lose_max: -0.4668, r_lose_std: 1.3920
eta_win_average: -0.5828, eta_win_min: -1.6016, eta_win_max: 0.6641, eta_win_std: 0.3011
eta_lose_average: -0.6857, eta_lose_min: -1.6328, eta_lose_max: 0.4375, eta_lose_std: 0.3068
p_win_average: -1.3579, p_win_min: -2.5781, p_win_max: 0.6328, p_win_std: 0.5596
p_lose_average: -1.2278, p_lose_min: -2.6094, p_lose_max: 0.2871, p_lose_std: 0.6303

eval_z_samples_size: 1000
eval_loss: 1.2144, accuracy: 0.5098
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5245, true_2_rate: 0.4939
log joint likelihood: tensor(-643.5273437500) joint log likelihood: tensor(-5951.7812500000)
r_win_average: -0.3894, r_win_min: -4.4688, r_win_max: 3.2031, r_win_std: 1.3413
r_lose_average: -2.3354, r_lose_min: -6.5000, r_lose_max: 1.1094, r_lose_std: 1.6202
eta_win_average: -0.1085, eta_win_min: -0.3438, eta_win_max: 0.0688, eta_win_std: 0.0552
eta_lose_average: -0.1007, eta_lose_min: -0.3438, eta_lose_max: 0.0796, eta_lose_std: 0.0586
p_win_average: 0.3430, p_win_min: 0.0267, p_win_max: 0.6406, p_win_std: 0.0979
p_lose_average: 0.2952, p_lose_min: -0.0356, p_lose_max: 0.5586, p_lose_std: 0.1173

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.4927, accuracy: 0.7871
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.7887, true_2_rate: 0.7854
log joint likelihood: tensor(-949.1718750000) joint log likelihood: tensor(-1596.8828125000)
r_win_average: 0.1434, r_win_min: -2.3438, r_win_max: 2.3594, r_win_std: 0.6821
r_lose_average: -0.9160, r_lose_min: -3.3438, r_lose_max: 1.8359, r_lose_std: 0.8618
eta_win_average: -0.2271, eta_win_min: -0.5508, eta_win_max: 0.2363, eta_win_std: 0.0970
eta_lose_average: -0.2890, eta_lose_min: -0.4707, eta_lose_max: 0.2891, eta_lose_std: 0.0845
p_win_average: 0.0029, p_win_min: -0.4414, p_win_max: 0.1279, p_win_std: 0.0667
p_lose_average: -0.0026, p_lose_min: -0.5156, p_lose_max: 0.1729, p_lose_std: 0.0621

eval_z_samples_size: 1000
eval_loss: 0.4968, accuracy: 0.7832
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.7774, true_2_rate: 0.7895
log joint likelihood: tensor(-942.3828125000) joint log likelihood: tensor(-1591.7500000000)
r_win_average: 0.2680, r_win_min: -2.1250, r_win_max: 2.3125, r_win_std: 0.6437
r_lose_average: -0.7320, r_lose_min: -2.9219, r_lose_max: 1.5312, r_lose_std: 0.8045
eta_win_average: 0.0312, eta_win_min: -0.1406, eta_win_max: 0.1250, eta_win_std: 0.0355
eta_lose_average: 0.0413, eta_lose_min: -0.1436, eta_lose_max: 0.1172, eta_lose_std: 0.0321
p_win_average: -0.1330, p_win_min: -0.2217, p_win_max: 0.0938, p_win_std: 0.0351
p_lose_average: -0.1466, p_lose_min: -0.2197, p_lose_max: 0.0420, p_lose_std: 0.0291

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4561, accuracy: 0.7910
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.8038, true_2_rate: 0.7773
log joint likelihood: tensor(-950.) joint log likelihood: tensor(-1318.3242187500)
r_win_average: 0.6529, r_win_min: -2.4531, r_win_max: 2.6875, r_win_std: 0.7738
r_lose_average: -0.5434, r_lose_min: -2.9688, r_lose_max: 2.1875, r_lose_std: 0.9865
eta_win_average: 0.4423, eta_win_min: 0.2363, eta_win_max: 0.5547, eta_win_std: 0.0482
eta_lose_average: 0.4664, eta_lose_min: 0.1787, eta_lose_max: 0.5859, eta_lose_std: 0.0410
p_win_average: -0.1273, p_win_min: -0.2812, p_win_max: 0.1748, p_win_std: 0.0523
p_lose_average: -0.1363, p_lose_min: -0.2969, p_lose_max: 0.0481, p_lose_std: 0.0397

eval_z_samples_size: 1000
eval_loss: 0.4517, accuracy: 0.7852
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.7887, true_2_rate: 0.7814
log joint likelihood: tensor(-944.1523437500) joint log likelihood: tensor(-1307.3007812500)
r_win_average: 0.2054, r_win_min: -2.9219, r_win_max: 2.3281, r_win_std: 0.7950
r_lose_average: -1.0258, r_lose_min: -3.4531, r_lose_max: 1.5859, r_lose_std: 1.0187
eta_win_average: 0.0284, eta_win_min: -0.0476, eta_win_max: 0.1069, eta_win_std: 0.0169
eta_lose_average: 0.0249, eta_lose_min: -0.0630, eta_lose_max: 0.0664, eta_lose_std: 0.0145
p_win_average: -0.1613, p_win_min: -0.2637, p_win_max: -0.0066, p_win_std: 0.0261
p_lose_average: -0.1767, p_lose_min: -0.2383, p_lose_max: -0.0601, p_lose_std: 0.0243

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.4231, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.7962, true_2_rate: 0.8178
log joint likelihood: tensor(-771.4531250000) joint log likelihood: tensor(-1235.2490234375)
r_win_average: 0.3159, r_win_min: -3.7656, r_win_max: 2.6562, r_win_std: 1.1121
r_lose_average: -1.7424, r_lose_min: -5.7812, r_lose_max: 2.0469, r_lose_std: 1.6311
eta_win_average: 0.2467, eta_win_min: 0.0143, eta_win_max: 0.3691, eta_win_std: 0.0388
eta_lose_average: 0.2908, eta_lose_min: 0.1084, eta_lose_max: 0.4434, eta_lose_std: 0.0454
p_win_average: 0.3756, p_win_min: 0.1396, p_win_max: 0.5703, p_win_std: 0.0434
p_lose_average: 0.3847, p_lose_min: 0.2754, p_lose_max: 0.5391, p_lose_std: 0.0398

eval_z_samples_size: 1000
eval_loss: 0.4226, accuracy: 0.8105
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.8075, true_2_rate: 0.8138
log joint likelihood: tensor(-763.5957031250) joint log likelihood: tensor(-1237.7187500000)
r_win_average: -0.4871, r_win_min: -4.7188, r_win_max: 1.9141, r_win_std: 1.1466
r_lose_average: -2.6124, r_lose_min: -6.8438, r_lose_max: 1.1250, r_lose_std: 1.6846
eta_win_average: 0.0699, eta_win_min: -0.0029, eta_win_max: 0.1216, eta_win_std: 0.0183
eta_lose_average: 0.0779, eta_lose_min: -0.0325, eta_lose_max: 0.1299, eta_lose_std: 0.0189
p_win_average: -0.2508, p_win_min: -0.3418, p_win_max: -0.1660, p_win_std: 0.0231
p_lose_average: -0.2721, p_lose_min: -0.3340, p_lose_max: -0.1758, p_lose_std: 0.0258

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.4341, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8151, true_2_rate: 0.7976
log joint likelihood: tensor(-919.5664062500) joint log likelihood: tensor(-1220.5937500000)
r_win_average: 1.8607, r_win_min: -0.8750, r_win_max: 3.8906, r_win_std: 0.7960
r_lose_average: 0.5057, r_lose_min: -2.7656, r_lose_max: 3.1719, r_lose_std: 1.1128
eta_win_average: 0.0265, eta_win_min: -0.1865, eta_win_max: 0.2207, eta_win_std: 0.0394
eta_lose_average: 0.0277, eta_lose_min: -0.0947, eta_lose_max: 0.1758, eta_lose_std: 0.0392
p_win_average: 1.4863, p_win_min: 1.1484, p_win_max: 1.6484, p_win_std: 0.0553
p_lose_average: 1.5116, p_lose_min: 1.2969, p_lose_max: 1.6953, p_lose_std: 0.0536

eval_z_samples_size: 1000
eval_loss: 0.4319, accuracy: 0.8086
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.8113, true_2_rate: 0.8057
log joint likelihood: tensor(-909.9785156250) joint log likelihood: tensor(-1219.0722656250)
r_win_average: 0.4164, r_win_min: -2.5000, r_win_max: 2.5312, r_win_std: 0.8255
r_lose_average: -0.9794, r_lose_min: -4.1875, r_lose_max: 1.8750, r_lose_std: 1.1458
eta_win_average: -0.0216, eta_win_min: -0.0967, eta_win_max: 0.0903, eta_win_std: 0.0170
eta_lose_average: -0.0290, eta_lose_min: -0.0859, eta_lose_max: 0.0444, eta_lose_std: 0.0137
p_win_average: 0.0886, p_win_min: 0.0459, p_win_max: 0.1865, p_win_std: 0.0180
p_lose_average: 0.0843, p_lose_min: 0.0234, p_lose_max: 0.1416, p_lose_std: 0.0165

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.3928, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8113, true_2_rate: 0.8462
log joint likelihood: tensor(-755.4184570312) joint log likelihood: tensor(-1125.3671875000)
r_win_average: -0.2631, r_win_min: -4.6250, r_win_max: 2.2812, r_win_std: 1.2962
r_lose_average: -2.6125, r_lose_min: -8.2500, r_lose_max: 1.5078, r_lose_std: 1.8381
eta_win_average: 0.2480, eta_win_min: -0.0007, eta_win_max: 0.6367, eta_win_std: 0.0887
eta_lose_average: 0.1474, eta_lose_min: -0.2070, eta_lose_max: 0.5352, eta_lose_std: 0.1014
p_win_average: -0.1112, p_win_min: -0.4863, p_win_max: 0.2002, p_win_std: 0.0751
p_lose_average: -0.1067, p_lose_min: -0.3477, p_lose_max: 0.1201, p_lose_std: 0.0724

eval_z_samples_size: 1000
eval_loss: 0.3901, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8075, true_2_rate: 0.8381
log joint likelihood: tensor(-740.8593750000) joint log likelihood: tensor(-1127.9711914062)
r_win_average: -0.2266, r_win_min: -4.5000, r_win_max: 2.2656, r_win_std: 1.2617
r_lose_average: -2.4988, r_lose_min: -7.7188, r_lose_max: 1.5312, r_lose_std: 1.7584
eta_win_average: -0.1552, eta_win_min: -0.2021, eta_win_max: -0.0859, eta_win_std: 0.0203
eta_lose_average: -0.1761, eta_lose_min: -0.2412, eta_lose_max: -0.0928, eta_lose_std: 0.0202
p_win_average: 0.3278, p_win_min: 0.2432, p_win_max: 0.3965, p_win_std: 0.0202
p_lose_average: 0.3310, p_lose_min: 0.2402, p_lose_max: 0.4121, p_lose_std: 0.0210

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.4092, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.8113, true_2_rate: 0.8340
log joint likelihood: tensor(-876.4257812500) joint log likelihood: tensor(-1149.6308593750)
r_win_average: -0.8319, r_win_min: -3.6094, r_win_max: 0.9492, r_win_std: 0.8175
r_lose_average: -2.2376, r_lose_min: -5.1562, r_lose_max: 0.3145, r_lose_std: 1.0576
eta_win_average: -0.4284, eta_win_min: -0.6094, eta_win_max: -0.1777, eta_win_std: 0.0473
eta_lose_average: -0.3862, eta_lose_min: -0.6289, eta_lose_max: -0.1191, eta_lose_std: 0.0582
p_win_average: -0.5396, p_win_min: -0.6992, p_win_max: -0.2031, p_win_std: 0.0741
p_lose_average: -0.5465, p_lose_min: -0.7344, p_lose_max: -0.2559, p_lose_std: 0.0632

eval_z_samples_size: 1000
eval_loss: 0.4080, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8038, true_2_rate: 0.8340
log joint likelihood: tensor(-870.9570312500) joint log likelihood: tensor(-1143.0234375000)
r_win_average: 0.1455, r_win_min: -2.6875, r_win_max: 1.8984, r_win_std: 0.8165
r_lose_average: -1.2869, r_lose_min: -4.4062, r_lose_max: 1.2188, r_lose_std: 1.0816
eta_win_average: -0.1817, eta_win_min: -0.2236, eta_win_max: -0.1123, eta_win_std: 0.0113
eta_lose_average: -0.1738, eta_lose_min: -0.2090, eta_lose_max: -0.1270, eta_lose_std: 0.0131
p_win_average: 0.1902, p_win_min: 0.1279, p_win_max: 0.3066, p_win_std: 0.0213
p_lose_average: 0.1928, p_lose_min: 0.1426, p_lose_max: 0.2559, p_lose_std: 0.0202

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.3730, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4766, prediction_2_rate: 0.5234
true_1_rate: 0.7887, true_2_rate: 0.8583
log joint likelihood: tensor(-753.2753906250) joint log likelihood: tensor(-1062.7958984375)
r_win_average: -0.3921, r_win_min: -4.8438, r_win_max: 2.2344, r_win_std: 1.1652
r_lose_average: -2.4302, r_lose_min: -6.4688, r_lose_max: 1.1016, r_lose_std: 1.4931
eta_win_average: -0.1541, eta_win_min: -0.3105, eta_win_max: 0.0104, eta_win_std: 0.0417
eta_lose_average: -0.1703, eta_lose_min: -0.3125, eta_lose_max: -0.0396, eta_lose_std: 0.0373
p_win_average: 0.1784, p_win_min: 0.0388, p_win_max: 0.4102, p_win_std: 0.0428
p_lose_average: 0.2032, p_lose_min: 0.1021, p_lose_max: 0.3340, p_lose_std: 0.0447

eval_z_samples_size: 1000
eval_loss: 0.3733, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4766, prediction_2_rate: 0.5234
true_1_rate: 0.7925, true_2_rate: 0.8623
log joint likelihood: tensor(-752.5263671875) joint log likelihood: tensor(-1068.7841796875)
r_win_average: -0.5002, r_win_min: -5.0000, r_win_max: 2.0781, r_win_std: 1.1799
r_lose_average: -2.5732, r_lose_min: -6.6562, r_lose_max: 1.0078, r_lose_std: 1.5216
eta_win_average: -0.0528, eta_win_min: -0.0942, eta_win_max: 0.0093, eta_win_std: 0.0147
eta_lose_average: -0.0668, eta_lose_min: -0.1235, eta_lose_max: -0.0063, eta_lose_std: 0.0155
p_win_average: -0.0311, p_win_min: -0.0981, p_win_max: 0.0327, p_win_std: 0.0186
p_lose_average: -0.0426, p_lose_min: -0.1099, p_lose_max: 0.0121, p_lose_std: 0.0206

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.3726, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8075, true_2_rate: 0.8543
log joint likelihood: tensor(-738.7919921875) joint log likelihood: tensor(-1065.3051757812)
r_win_average: -0.9784, r_win_min: -6.0938, r_win_max: 1.9375, r_win_std: 1.3288
r_lose_average: -3.3119, r_lose_min: -7.9375, r_lose_max: 0.8555, r_lose_std: 1.7241
eta_win_average: -0.4844, eta_win_min: -0.7852, eta_win_max: -0.0830, eta_win_std: 0.0787
eta_lose_average: -0.5640, eta_lose_min: -0.8125, eta_lose_max: -0.2324, eta_lose_std: 0.0734
p_win_average: -0.2276, p_win_min: -0.4570, p_win_max: -0.0713, p_win_std: 0.0540
p_lose_average: -0.2041, p_lose_min: -0.4727, p_lose_max: -0.0040, p_lose_std: 0.0558

eval_z_samples_size: 1000
eval_loss: 0.3733, accuracy: 0.8320
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.8113, true_2_rate: 0.8543
log joint likelihood: tensor(-729.6484375000) joint log likelihood: tensor(-1064.2099609375)
r_win_average: -0.1687, r_win_min: -5.0625, r_win_max: 2.6562, r_win_std: 1.2873
r_lose_average: -2.4314, r_lose_min: -6.7500, r_lose_max: 1.5938, r_lose_std: 1.6694
eta_win_average: -0.0030, eta_win_min: -0.0698, eta_win_max: 0.0488, eta_win_std: 0.0160
eta_lose_average: 0.0092, eta_lose_min: -0.0505, eta_lose_max: 0.0645, eta_lose_std: 0.0177
p_win_average: 0.1003, p_win_min: 0.0249, p_win_max: 0.1826, p_win_std: 0.0234
p_lose_average: 0.1035, p_lose_min: 0.0430, p_lose_max: 0.1885, p_lose_std: 0.0228

------------------------------------------------------------------------------------------
[2023-09-25 07:17:23,375] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,476] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,646] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,889] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,944] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,957] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,966] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:38,978] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 07:17:44,488] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:44,488] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 07:17:45,047] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:45,047] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 07:17:45,148] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:45,148] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 07:17:45,151] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:45,151] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 07:17:45,252] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:45,252] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 07:17:45,290] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:45,290] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 07:17:45,290] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 07:17:45,300] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:45,301] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 07:17:45,302] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 07:17:45,302] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 742
cuda seed 742
torch seed 542
cuda seed 542
torch seed 642
cuda seed 642
torch seed 442
cuda seed 442
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
torch seed 242
cuda seed 242
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim128-num_ref_train500-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-25 07:18:26,160] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 07:18:28,611] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 07:18:28,612] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 07:18:28,612] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 07:18:28,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 07:18:28,618] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 07:18:28,618] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 07:18:28,618] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 07:18:28,618] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 07:18:28,618] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 07:18:28,618] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-25 07:18:39,136] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 07:18:39,136] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 07:18:39,137] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.44 GB, percent = 6.2%
[2023-09-25 07:18:39,241] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 07:18:39,242] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 07:18:39,242] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.44 GB, percent = 6.2%
[2023-09-25 07:18:39,242] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 07:18:39,333] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 07:18:39,333] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 07:18:39,333] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.44 GB, percent = 6.2%
[2023-09-25 07:18:39,335] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 07:18:39,335] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 07:18:39,335] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 07:18:39,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 07:18:39,335] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f00444fc2d0>
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 07:18:39,336] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 07:18:39,337] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 07:18:39,337] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.0767, accuracy: 0.5117
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.5094, true_2_rate: 0.5142
log joint likelihood: tensor(-647.5312500000) joint log likelihood: tensor(-5955.0625000000)
r_win_average: -2.6057, r_win_min: -6.2812, r_win_max: 2.5781, r_win_std: 1.3715
r_lose_average: -4.4027, r_lose_min: -8.1875, r_lose_max: -0.4668, r_lose_std: 1.3920
eta_win_average: -0.5828, eta_win_min: -1.6016, eta_win_max: 0.6641, eta_win_std: 0.3011
eta_lose_average: -0.6857, eta_lose_min: -1.6328, eta_lose_max: 0.4375, eta_lose_std: 0.3068
p_win_average: -1.3579, p_win_min: -2.5781, p_win_max: 0.6328, p_win_std: 0.5596
p_lose_average: -1.2278, p_lose_min: -2.6094, p_lose_max: 0.2871, p_lose_std: 0.6303

eval_z_samples_size: 1000
eval_loss: 1.2144, accuracy: 0.5098
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5245, true_2_rate: 0.4939
log joint likelihood: tensor(-643.5273437500) joint log likelihood: tensor(-5951.7812500000)
r_win_average: -0.3894, r_win_min: -4.4688, r_win_max: 3.2031, r_win_std: 1.3413
r_lose_average: -2.3354, r_lose_min: -6.5000, r_lose_max: 1.1094, r_lose_std: 1.6202
eta_win_average: -0.1085, eta_win_min: -0.3438, eta_win_max: 0.0688, eta_win_std: 0.0552
eta_lose_average: -0.1007, eta_lose_min: -0.3438, eta_lose_max: 0.0796, eta_lose_std: 0.0586
p_win_average: 0.3430, p_win_min: 0.0267, p_win_max: 0.6406, p_win_std: 0.0979
p_lose_average: 0.2952, p_lose_min: -0.0356, p_lose_max: 0.5586, p_lose_std: 0.1173

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.4973, accuracy: 0.7734
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.7623, true_2_rate: 0.7854
log joint likelihood: tensor(-949.7265625000) joint log likelihood: tensor(-1578.7968750000)
r_win_average: -0.6825, r_win_min: -3.0000, r_win_max: 1.8750, r_win_std: 0.6635
r_lose_average: -1.6464, r_lose_min: -3.7344, r_lose_max: 0.7812, r_lose_std: 0.7688
eta_win_average: -0.6752, eta_win_min: -1.0312, eta_win_max: -0.3047, eta_win_std: 0.0687
eta_lose_average: -0.6239, eta_lose_min: -0.9336, eta_lose_max: -0.1992, eta_lose_std: 0.0801
p_win_average: -0.1398, p_win_min: -0.6758, p_win_max: 0.1855, p_win_std: 0.0705
p_lose_average: -0.1429, p_lose_min: -0.4844, p_lose_max: 0.1738, p_lose_std: 0.0611

eval_z_samples_size: 1000
eval_loss: 0.4934, accuracy: 0.7793
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.7698, true_2_rate: 0.7895
log joint likelihood: tensor(-939.9609375000) joint log likelihood: tensor(-1569.1953125000)
r_win_average: 0.0469, r_win_min: -2.4688, r_win_max: 2.2969, r_win_std: 0.6746
r_lose_average: -0.9658, r_lose_min: -3.2188, r_lose_max: 1.5156, r_lose_std: 0.8084
eta_win_average: -0.0453, eta_win_min: -0.1543, eta_win_max: 0.0757, eta_win_std: 0.0184
eta_lose_average: -0.0420, eta_lose_min: -0.1348, eta_lose_max: 0.0608, eta_lose_std: 0.0174
p_win_average: -0.0413, p_win_min: -0.1064, p_win_max: 0.0825, p_win_std: 0.0219
p_lose_average: -0.0433, p_lose_min: -0.1167, p_lose_max: 0.0569, p_lose_std: 0.0196

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4458, accuracy: 0.7910
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8000, true_2_rate: 0.7814
log joint likelihood: tensor(-916.2421875000) joint log likelihood: tensor(-1321.8359375000)
r_win_average: 0.6284, r_win_min: -2.8125, r_win_max: 2.7344, r_win_std: 0.8303
r_lose_average: -0.6732, r_lose_min: -3.3125, r_lose_max: 1.9844, r_lose_std: 1.0674
eta_win_average: 0.2395, eta_win_min: -0.0093, eta_win_max: 0.3984, eta_win_std: 0.0451
eta_lose_average: 0.2179, eta_lose_min: -0.0498, eta_lose_max: 0.3496, eta_lose_std: 0.0447
p_win_average: 0.0907, p_win_min: -0.2617, p_win_max: 0.4238, p_win_std: 0.0778
p_lose_average: 0.0758, p_lose_min: -0.1289, p_lose_max: 0.3438, p_lose_std: 0.0670

eval_z_samples_size: 1000
eval_loss: 0.4485, accuracy: 0.7910
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7962, true_2_rate: 0.7854
log joint likelihood: tensor(-912.8847656250) joint log likelihood: tensor(-1314.6015625000)
r_win_average: 0.6219, r_win_min: -2.7031, r_win_max: 2.6094, r_win_std: 0.8039
r_lose_average: -0.6441, r_lose_min: -3.2031, r_lose_max: 2.0312, r_lose_std: 1.0485
eta_win_average: 0.2138, eta_win_min: 0.1211, eta_win_max: 0.2715, eta_win_std: 0.0202
eta_lose_average: 0.2171, eta_lose_min: 0.1128, eta_lose_max: 0.2578, eta_lose_std: 0.0172
p_win_average: 0.1089, p_win_min: 0.0188, p_win_max: 0.1963, p_win_std: 0.0230
p_lose_average: 0.1063, p_lose_min: -0.0762, p_lose_max: 0.1807, p_lose_std: 0.0231

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.4243, accuracy: 0.8125
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.8113, true_2_rate: 0.8138
log joint likelihood: tensor(-764.9130859375) joint log likelihood: tensor(-1248.0507812500)
r_win_average: 0.1416, r_win_min: -4.1250, r_win_max: 2.4219, r_win_std: 1.1329
r_lose_average: -1.9883, r_lose_min: -6.3125, r_lose_max: 1.5938, r_lose_std: 1.7103
eta_win_average: 0.0445, eta_win_min: -0.0864, eta_win_max: 0.1543, eta_win_std: 0.0315
eta_lose_average: 0.0382, eta_lose_min: -0.0703, eta_lose_max: 0.1729, eta_lose_std: 0.0334
p_win_average: 0.3908, p_win_min: 0.1631, p_win_max: 0.5391, p_win_std: 0.0408
p_lose_average: 0.3765, p_lose_min: 0.2227, p_lose_max: 0.4902, p_lose_std: 0.0405

eval_z_samples_size: 1000
eval_loss: 0.4219, accuracy: 0.8145
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.8038, true_2_rate: 0.8259
log joint likelihood: tensor(-759.5703125000) joint log likelihood: tensor(-1249.4355468750)
r_win_average: -0.3545, r_win_min: -4.4688, r_win_max: 1.9062, r_win_std: 1.1180
r_lose_average: -2.4427, r_lose_min: -6.5625, r_lose_max: 1.2109, r_lose_std: 1.6631
eta_win_average: -0.0570, eta_win_min: -0.1270, eta_win_max: 0.0243, eta_win_std: 0.0182
eta_lose_average: -0.0365, eta_lose_min: -0.1191, eta_lose_max: 0.0564, eta_lose_std: 0.0282
p_win_average: -0.0042, p_win_min: -0.0579, p_win_max: 0.1099, p_win_std: 0.0232
p_lose_average: -0.0033, p_lose_min: -0.0825, p_lose_max: 0.1216, p_lose_std: 0.0254

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.4316, accuracy: 0.8125
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.8151, true_2_rate: 0.8097
log joint likelihood: tensor(-904.7773437500) joint log likelihood: tensor(-1250.0585937500)
r_win_average: 0.8442, r_win_min: -1.9922, r_win_max: 2.7188, r_win_std: 0.8238
r_lose_average: -0.5722, r_lose_min: -4.3125, r_lose_max: 2.0312, r_lose_std: 1.1957
eta_win_average: 0.4010, eta_win_min: 0.2080, eta_win_max: 0.6094, eta_win_std: 0.0554
eta_lose_average: 0.3618, eta_lose_min: 0.0874, eta_lose_max: 0.5156, eta_lose_std: 0.0700
p_win_average: 0.0735, p_win_min: -0.1514, p_win_max: 0.2832, p_win_std: 0.0415
p_lose_average: 0.0778, p_lose_min: -0.0801, p_lose_max: 0.2256, p_lose_std: 0.0349

eval_z_samples_size: 1000
eval_loss: 0.4319, accuracy: 0.8105
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.8113, true_2_rate: 0.8097
log joint likelihood: tensor(-903.3574218750) joint log likelihood: tensor(-1252.4960937500)
r_win_average: 0.7965, r_win_min: -1.9453, r_win_max: 2.6406, r_win_std: 0.8137
r_lose_average: -0.5969, r_lose_min: -4.1562, r_lose_max: 2.0625, r_lose_std: 1.1619
eta_win_average: 0.1367, eta_win_min: 0.0430, eta_win_max: 0.2500, eta_win_std: 0.0259
eta_lose_average: 0.1137, eta_lose_min: -0.0092, eta_lose_max: 0.1992, eta_lose_std: 0.0343
p_win_average: 0.2895, p_win_min: 0.1074, p_win_max: 0.3945, p_win_std: 0.0281
p_lose_average: 0.3021, p_lose_min: 0.1885, p_lose_max: 0.3711, p_lose_std: 0.0235

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.3877, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8038, true_2_rate: 0.8421
log joint likelihood: tensor(-736.0039062500) joint log likelihood: tensor(-1175.1708984375)
r_win_average: -0.5349, r_win_min: -4.7812, r_win_max: 2.0156, r_win_std: 1.2535
r_lose_average: -2.8027, r_lose_min: -7.7812, r_lose_max: 1.2656, r_lose_std: 1.7686
eta_win_average: -0.1809, eta_win_min: -0.2988, eta_win_max: 0.0293, eta_win_std: 0.0314
eta_lose_average: -0.2126, eta_lose_min: -0.3066, eta_lose_max: -0.1006, eta_lose_std: 0.0306
p_win_average: -0.0081, p_win_min: -0.1426, p_win_max: 0.0645, p_win_std: 0.0254
p_lose_average: -0.0087, p_lose_min: -0.1621, p_lose_max: 0.1118, p_lose_std: 0.0290

eval_z_samples_size: 1000
eval_loss: 0.3887, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8038, true_2_rate: 0.8381
log joint likelihood: tensor(-730.2065429688) joint log likelihood: tensor(-1168.6513671875)
r_win_average: -0.2911, r_win_min: -4.5000, r_win_max: 2.2500, r_win_std: 1.2552
r_lose_average: -2.5563, r_lose_min: -7.5000, r_lose_max: 1.5000, r_lose_std: 1.7665
eta_win_average: -0.1245, eta_win_min: -0.1904, eta_win_max: -0.0111, eta_win_std: 0.0264
eta_lose_average: -0.1488, eta_lose_min: -0.2236, eta_lose_max: -0.0481, eta_lose_std: 0.0230
p_win_average: 0.1793, p_win_min: 0.1240, p_win_max: 0.2373, p_win_std: 0.0185
p_lose_average: 0.1739, p_lose_min: 0.1187, p_lose_max: 0.2363, p_lose_std: 0.0203

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.4070, accuracy: 0.8125
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.8075, true_2_rate: 0.8178
log joint likelihood: tensor(-863.6386718750) joint log likelihood: tensor(-1195.4492187500)
r_win_average: -1.1407, r_win_min: -3.9219, r_win_max: 0.7539, r_win_std: 0.8480
r_lose_average: -2.6017, r_lose_min: -5.7188, r_lose_max: -0.0674, r_lose_std: 1.1185
eta_win_average: -0.1568, eta_win_min: -0.3867, eta_win_max: 0.0645, eta_win_std: 0.0451
eta_lose_average: -0.1197, eta_lose_min: -0.2773, eta_lose_max: 0.0459, eta_lose_std: 0.0516
p_win_average: -1.0884, p_win_min: -1.3906, p_win_max: -0.7461, p_win_std: 0.0796
p_lose_average: -1.1407, p_lose_min: -1.3828, p_lose_max: -0.7617, p_lose_std: 0.0683

eval_z_samples_size: 1000
eval_loss: 0.4077, accuracy: 0.8125
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.8038, true_2_rate: 0.8219
log joint likelihood: tensor(-855.0058593750) joint log likelihood: tensor(-1190.6523437500)
r_win_average: 0.2424, r_win_min: -2.4844, r_win_max: 2.0000, r_win_std: 0.8327
r_lose_average: -1.2155, r_lose_min: -4.2812, r_lose_max: 1.3359, r_lose_std: 1.1130
eta_win_average: -0.1017, eta_win_min: -0.1318, eta_win_max: -0.0442, eta_win_std: 0.0125
eta_lose_average: -0.1111, eta_lose_min: -0.1494, eta_lose_max: -0.0635, eta_lose_std: 0.0124
p_win_average: 0.2393, p_win_min: 0.1631, p_win_max: 0.2949, p_win_std: 0.0154
p_lose_average: 0.2371, p_lose_min: 0.1758, p_lose_max: 0.2930, p_lose_std: 0.0164

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.3740, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4688, prediction_2_rate: 0.5312
true_1_rate: 0.7887, true_2_rate: 0.8745
log joint likelihood: tensor(-727.0820312500) joint log likelihood: tensor(-1113.7207031250)
r_win_average: 0.3761, r_win_min: -3.8906, r_win_max: 3.0156, r_win_std: 1.1443
r_lose_average: -1.6674, r_lose_min: -6.1875, r_lose_max: 1.8281, r_lose_std: 1.5299
eta_win_average: 0.4873, eta_win_min: 0.1631, eta_win_max: 0.7344, eta_win_std: 0.0879
eta_lose_average: 0.5025, eta_lose_min: 0.1660, eta_lose_max: 0.6602, eta_lose_std: 0.0730
p_win_average: 0.3335, p_win_min: -0.1079, p_win_max: 0.4941, p_win_std: 0.0748
p_lose_average: 0.3335, p_lose_min: 0.0830, p_lose_max: 0.5234, p_lose_std: 0.0650

eval_z_samples_size: 1000
eval_loss: 0.3721, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4688, prediction_2_rate: 0.5312
true_1_rate: 0.7849, true_2_rate: 0.8704
log joint likelihood: tensor(-728.6552734375) joint log likelihood: tensor(-1118.0585937500)
r_win_average: -0.6911, r_win_min: -4.9375, r_win_max: 1.8750, r_win_std: 1.1883
r_lose_average: -2.7856, r_lose_min: -7.1562, r_lose_max: 0.8750, r_lose_std: 1.5394
eta_win_average: -0.1602, eta_win_min: -0.2178, eta_win_max: -0.0356, eta_win_std: 0.0199
eta_lose_average: -0.1754, eta_lose_min: -0.2246, eta_lose_max: -0.1045, eta_lose_std: 0.0156
p_win_average: -0.0883, p_win_min: -0.1504, p_win_max: -0.0221, p_win_std: 0.0199
p_lose_average: -0.1052, p_lose_min: -0.1689, p_lose_max: -0.0356, p_lose_std: 0.0207

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.3723, accuracy: 0.8320
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4824, prediction_2_rate: 0.5176
true_1_rate: 0.8038, true_2_rate: 0.8623
log joint likelihood: tensor(-710.2670898438) joint log likelihood: tensor(-1109.8051757812)
r_win_average: 1.0607, r_win_min: -3.8281, r_win_max: 3.8438, r_win_std: 1.3225
r_lose_average: -1.3060, r_lose_min: -6.0938, r_lose_max: 2.8125, r_lose_std: 1.7664
eta_win_average: 0.0097, eta_win_min: -0.2236, eta_win_max: 0.2217, eta_win_std: 0.0634
eta_lose_average: -0.0796, eta_lose_min: -0.3535, eta_lose_max: 0.1777, eta_lose_std: 0.0900
p_win_average: 1.3191, p_win_min: 0.9375, p_win_max: 1.5000, p_win_std: 0.0711
p_lose_average: 1.3375, p_lose_min: 0.9766, p_lose_max: 1.5703, p_lose_std: 0.0693

eval_z_samples_size: 1000
eval_loss: 0.3721, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8000, true_2_rate: 0.8462
log joint likelihood: tensor(-706.9531250000) joint log likelihood: tensor(-1112.4003906250)
r_win_average: -0.8334, r_win_min: -5.5938, r_win_max: 1.9766, r_win_std: 1.3186
r_lose_average: -3.1468, r_lose_min: -7.5938, r_lose_max: 0.9688, r_lose_std: 1.7172
eta_win_average: -0.1925, eta_win_min: -0.2559, eta_win_max: -0.1182, eta_win_std: 0.0183
eta_lose_average: -0.1739, eta_lose_min: -0.2354, eta_lose_max: -0.0659, eta_lose_std: 0.0253
p_win_average: -0.3734, p_win_min: -0.4922, p_win_max: -0.2051, p_win_std: 0.0360
p_lose_average: -0.4080, p_lose_min: -0.5117, p_lose_max: -0.2637, p_lose_std: 0.0380

------------------------------------------------------------------------------------------
[2023-09-25 08:45:51,676] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:05,947] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:06,930] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:07,076] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:07,206] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:07,280] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:07,310] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:07,314] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:07,315] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:46:11,778] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:11,778] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:46:13,114] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:13,114] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:46:13,354] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:13,354] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:46:13,387] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:13,387] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:46:13,449] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:13,449] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:46:13,449] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 08:46:13,470] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:13,470] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:46:13,496] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:13,497] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:46:13,567] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:46:13,567] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 342
cuda seed 342
torch seed 142
cuda seed 142
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim256-num_ref_train10-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 442
cuda seed 442
torch seed 542
cuda seed 542
torch seed 242
cuda seed 242
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-25 08:46:52,202] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 08:46:56,820] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 08:46:56,821] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 08:46:56,821] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 08:46:56,826] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 08:46:56,826] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 08:46:56,826] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 08:46:56,826] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 08:46:56,826] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 08:46:56,826] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 08:46:56,826] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
[2023-09-25 08:47:08,652] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 08:47:08,653] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.02 GB         Max_CA 8 GB 
[2023-09-25 08:47:08,653] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.44 GB, percent = 6.2%
[2023-09-25 08:47:08,753] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 08:47:08,754] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 16.0 GB         Max_CA 16 GB 
[2023-09-25 08:47:08,754] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.44 GB, percent = 6.2%
[2023-09-25 08:47:08,754] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 08:47:08,843] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 08:47:08,843] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 16.0 GB         Max_CA 16 GB 
[2023-09-25 08:47:08,843] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.44 GB, percent = 6.2%
[2023-09-25 08:47:08,844] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 08:47:08,845] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 08:47:08,845] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 08:47:08,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 08:47:08,845] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 08:47:08,845] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 08:47:08,845] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 08:47:08,845] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 08:47:08,845] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 08:47:08,845] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8fa32e7bd0>
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 08:47:08,846] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 08:47:08,846] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.1978, accuracy: 0.5137
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.5396, true_2_rate: 0.4858
log joint likelihood: tensor(-648.4804687500) joint log likelihood: tensor(-5998.6406250000)
r_win_average: -1.6865, r_win_min: -5.7188, r_win_max: 2.7500, r_win_std: 1.3921
r_lose_average: -3.6377, r_lose_min: -7.8750, r_lose_max: -0.2188, r_lose_std: 1.6014
eta_win_average: -0.7765, eta_win_min: -1.4688, eta_win_max: 0.0356, eta_win_std: 0.2322
eta_lose_average: -0.8373, eta_lose_min: -1.5078, eta_lose_max: -0.1318, eta_lose_std: 0.2167
p_win_average: -0.2715, p_win_min: -0.7734, p_win_max: 0.4062, p_win_std: 0.1887
p_lose_average: -0.2855, p_lose_min: -0.8281, p_lose_max: 0.4551, p_lose_std: 0.1952

eval_z_samples_size: 1000
eval_loss: 1.1987, accuracy: 0.5020
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.5094, true_2_rate: 0.4939
log joint likelihood: tensor(-634.3554687500) joint log likelihood: tensor(-5958.8750000000)
r_win_average: -0.4194, r_win_min: -4.4688, r_win_max: 3.3438, r_win_std: 1.3440
r_lose_average: -2.3486, r_lose_min: -6.5000, r_lose_max: 1.2578, r_lose_std: 1.5918
eta_win_average: 0.0341, eta_win_min: -0.2314, eta_win_max: 0.3477, eta_win_std: 0.0928
eta_lose_average: 0.0322, eta_lose_min: -0.2119, eta_lose_max: 0.3418, eta_lose_std: 0.0915
p_win_average: 0.1717, p_win_min: -0.1436, p_win_max: 0.4160, p_win_std: 0.0910
p_lose_average: 0.1481, p_lose_min: -0.1011, p_lose_max: 0.5664, p_lose_std: 0.0975

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.5054, accuracy: 0.7734
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.7660, true_2_rate: 0.7814
log joint likelihood: tensor(-905.8320312500) joint log likelihood: tensor(-1635.0312500000)
r_win_average: 0.5163, r_win_min: -2.1094, r_win_max: 2.6562, r_win_std: 0.6783
r_lose_average: -0.5269, r_lose_min: -2.9531, r_lose_max: 1.7188, r_lose_std: 0.8325
eta_win_average: 0.1085, eta_win_min: -0.3672, eta_win_max: 0.4297, eta_win_std: 0.1157
eta_lose_average: 0.1519, eta_lose_min: -0.6367, eta_lose_max: 0.4727, eta_lose_std: 0.1222
p_win_average: 0.1408, p_win_min: -0.4688, p_win_max: 0.5547, p_win_std: 0.0949
p_lose_average: 0.1290, p_lose_min: -0.3926, p_lose_max: 0.4395, p_lose_std: 0.0886

eval_z_samples_size: 1000
eval_loss: 0.4944, accuracy: 0.7852
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.7811, true_2_rate: 0.7895
log joint likelihood: tensor(-897.5312500000) joint log likelihood: tensor(-1625.3281250000)
r_win_average: 0.1825, r_win_min: -2.5000, r_win_max: 2.8906, r_win_std: 0.7321
r_lose_average: -0.9143, r_lose_min: -3.4375, r_lose_max: 1.7031, r_lose_std: 0.8741
eta_win_average: -0.1436, eta_win_min: -0.2949, eta_win_max: 0.1582, eta_win_std: 0.0301
eta_lose_average: -0.1491, eta_lose_min: -0.2598, eta_lose_max: -0.0237, eta_lose_std: 0.0252
p_win_average: 0.0554, p_win_min: -0.0238, p_win_max: 0.1885, p_win_std: 0.0283
p_lose_average: 0.0461, p_lose_min: -0.0654, p_lose_max: 0.1533, p_lose_std: 0.0240

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4497, accuracy: 0.7949
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8038, true_2_rate: 0.7854
log joint likelihood: tensor(-903.8007812500) joint log likelihood: tensor(-1309.4765625000)
r_win_average: 0.1465, r_win_min: -2.9375, r_win_max: 2.5781, r_win_std: 0.8065
r_lose_average: -1.1126, r_lose_min: -3.6875, r_lose_max: 1.5156, r_lose_std: 1.0350
eta_win_average: -0.4487, eta_win_min: -0.6719, eta_win_max: -0.1118, eta_win_std: 0.0863
eta_lose_average: -0.4976, eta_lose_min: -0.7266, eta_lose_max: -0.0130, eta_lose_std: 0.0803
p_win_average: -0.0226, p_win_min: -0.3945, p_win_max: 0.1543, p_win_std: 0.0606
p_lose_average: -0.0050, p_lose_min: -0.3145, p_lose_max: 0.1543, p_lose_std: 0.0539

eval_z_samples_size: 1000
eval_loss: 0.4504, accuracy: 0.7930
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.8038, true_2_rate: 0.7814
log joint likelihood: tensor(-908.1367187500) joint log likelihood: tensor(-1308.8593750000)
r_win_average: 0.4538, r_win_min: -2.6094, r_win_max: 2.7188, r_win_std: 0.7853
r_lose_average: -0.7801, r_lose_min: -3.2500, r_lose_max: 1.7969, r_lose_std: 1.0123
eta_win_average: -0.0855, eta_win_min: -0.1582, eta_win_max: 0.0452, eta_win_std: 0.0196
eta_lose_average: -0.0815, eta_lose_min: -0.1582, eta_lose_max: 0.0527, eta_lose_std: 0.0182
p_win_average: -0.0803, p_win_min: -0.1553, p_win_max: 0.0339, p_win_std: 0.0208
p_lose_average: -0.0869, p_lose_min: -0.1436, p_lose_max: 0.0228, p_lose_std: 0.0198

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.4219, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.8038, true_2_rate: 0.8097
log joint likelihood: tensor(-799.2509765625) joint log likelihood: tensor(-1256.7109375000)
r_win_average: 0.1989, r_win_min: -3.4688, r_win_max: 2.7500, r_win_std: 1.1196
r_lose_average: -1.8448, r_lose_min: -6.3125, r_lose_max: 1.5234, r_lose_std: 1.6490
eta_win_average: 0.2380, eta_win_min: 0.0552, eta_win_max: 0.4023, eta_win_std: 0.0437
eta_lose_average: 0.2517, eta_lose_min: 0.0513, eta_lose_max: 0.4219, eta_lose_std: 0.0472
p_win_average: 0.0208, p_win_min: -0.1147, p_win_max: 0.1416, p_win_std: 0.0372
p_lose_average: -0.0077, p_lose_min: -0.2080, p_lose_max: 0.1089, p_lose_std: 0.0418

eval_z_samples_size: 1000
eval_loss: 0.4216, accuracy: 0.8047
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.8075, true_2_rate: 0.8016
log joint likelihood: tensor(-778.5751953125) joint log likelihood: tensor(-1251.3974609375)
r_win_average: -0.3649, r_win_min: -4.0938, r_win_max: 2.0469, r_win_std: 1.1174
r_lose_average: -2.4046, r_lose_min: -6.8438, r_lose_max: 1.0391, r_lose_std: 1.6486
eta_win_average: -0.1384, eta_win_min: -0.1934, eta_win_max: -0.0386, eta_win_std: 0.0190
eta_lose_average: -0.1499, eta_lose_min: -0.2061, eta_lose_max: -0.0562, eta_lose_std: 0.0177
p_win_average: -0.1667, p_win_min: -0.2383, p_win_max: -0.0928, p_win_std: 0.0181
p_lose_average: -0.1657, p_lose_min: -0.2256, p_lose_max: -0.0859, p_lose_std: 0.0195

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.4363, accuracy: 0.8086
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.8113, true_2_rate: 0.8057
log joint likelihood: tensor(-907.9453125000) joint log likelihood: tensor(-1238.9609375000)
r_win_average: 0.5158, r_win_min: -2.1250, r_win_max: 2.9062, r_win_std: 0.8416
r_lose_average: -0.8648, r_lose_min: -4.1875, r_lose_max: 2.0469, r_lose_std: 1.1384
eta_win_average: 0.0456, eta_win_min: -0.1279, eta_win_max: 0.4375, eta_win_std: 0.0607
eta_lose_average: 0.0257, eta_lose_min: -0.1455, eta_lose_max: 0.2832, eta_lose_std: 0.0531
p_win_average: 0.2133, p_win_min: -0.0067, p_win_max: 0.4395, p_win_std: 0.0512
p_lose_average: 0.2176, p_lose_min: 0.0791, p_lose_max: 0.3711, p_lose_std: 0.0487

eval_z_samples_size: 1000
eval_loss: 0.4338, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8151, true_2_rate: 0.7976
log joint likelihood: tensor(-904.0742187500) joint log likelihood: tensor(-1239.5136718750)
r_win_average: 0.2698, r_win_min: -2.3438, r_win_max: 2.4531, r_win_std: 0.8042
r_lose_average: -1.0801, r_lose_min: -4.5000, r_lose_max: 1.6328, r_lose_std: 1.1138
eta_win_average: 0.0552, eta_win_min: -0.0576, eta_win_max: 0.1069, eta_win_std: 0.0155
eta_lose_average: 0.0638, eta_lose_min: 0.0122, eta_lose_max: 0.1982, eta_lose_std: 0.0149
p_win_average: -0.0429, p_win_min: -0.1279, p_win_max: 0.0344, p_win_std: 0.0201
p_lose_average: -0.0349, p_lose_min: -0.1060, p_lose_max: 0.0496, p_lose_std: 0.0164

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.3938, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.8226, true_2_rate: 0.8300
log joint likelihood: tensor(-730.7045898438) joint log likelihood: tensor(-1136.5661621094)
r_win_average: 0.5440, r_win_min: -3.6406, r_win_max: 3.3125, r_win_std: 1.3469
r_lose_average: -1.8777, r_lose_min: -7.6875, r_lose_max: 2.4688, r_lose_std: 1.9243
eta_win_average: 0.5089, eta_win_min: 0.2148, eta_win_max: 0.7344, eta_win_std: 0.0588
eta_lose_average: 0.5190, eta_lose_min: 0.1680, eta_lose_max: 0.6953, eta_lose_std: 0.0645
p_win_average: 0.5273, p_win_min: 0.0742, p_win_max: 0.7227, p_win_std: 0.0647
p_lose_average: 0.5041, p_lose_min: 0.2676, p_lose_max: 0.6797, p_lose_std: 0.0655

eval_z_samples_size: 1000
eval_loss: 0.3911, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.8264, true_2_rate: 0.8300
log joint likelihood: tensor(-717.1877441406) joint log likelihood: tensor(-1143.1943359375)
r_win_average: -0.3891, r_win_min: -4.6562, r_win_max: 2.2031, r_win_std: 1.3409
r_lose_average: -2.8115, r_lose_min: -8.5625, r_lose_max: 1.3281, r_lose_std: 1.9194
eta_win_average: -0.0573, eta_win_min: -0.1133, eta_win_max: 0.0435, eta_win_std: 0.0216
eta_lose_average: -0.0674, eta_lose_min: -0.1396, eta_lose_max: 0.0088, eta_lose_std: 0.0229
p_win_average: 0.1598, p_win_min: 0.0601, p_win_max: 0.2773, p_win_std: 0.0230
p_lose_average: 0.1569, p_lose_min: 0.0688, p_lose_max: 0.2217, p_lose_std: 0.0244

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.4080, accuracy: 0.8164
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.8113, true_2_rate: 0.8219
log joint likelihood: tensor(-855.5058593750) joint log likelihood: tensor(-1173.5507812500)
r_win_average: -0.4303, r_win_min: -3.1250, r_win_max: 1.3672, r_win_std: 0.8325
r_lose_average: -1.8728, r_lose_min: -4.9688, r_lose_max: 0.7109, r_lose_std: 1.1208
eta_win_average: -0.1167, eta_win_min: -0.2617, eta_win_max: 0.0165, eta_win_std: 0.0443
eta_lose_average: -0.0946, eta_lose_min: -0.2559, eta_lose_max: 0.1152, eta_lose_std: 0.0418
p_win_average: -0.4376, p_win_min: -0.7031, p_win_max: -0.2930, p_win_std: 0.0378
p_lose_average: -0.4428, p_lose_min: -0.5859, p_lose_max: -0.2275, p_lose_std: 0.0355

eval_z_samples_size: 1000
eval_loss: 0.4075, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.8151, true_2_rate: 0.8219
log joint likelihood: tensor(-846.6406250000) joint log likelihood: tensor(-1173.6289062500)
r_win_average: -0.0972, r_win_min: -2.8594, r_win_max: 1.6797, r_win_std: 0.8439
r_lose_average: -1.5578, r_lose_min: -4.5938, r_lose_max: 1.0391, r_lose_std: 1.1299
eta_win_average: -0.0701, eta_win_min: -0.1221, eta_win_max: 0.0344, eta_win_std: 0.0173
eta_lose_average: -0.0670, eta_lose_min: -0.1118, eta_lose_max: 0.0110, eta_lose_std: 0.0170
p_win_average: -0.1511, p_win_min: -0.2031, p_win_max: -0.0918, p_win_std: 0.0152
p_lose_average: -0.1559, p_lose_min: -0.2295, p_lose_max: -0.0801, p_lose_std: 0.0157

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.3718, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.7962, true_2_rate: 0.8462
log joint likelihood: tensor(-712.3500976562) joint log likelihood: tensor(-1098.6713867188)
r_win_average: -0.2347, r_win_min: -4.9688, r_win_max: 2.5938, r_win_std: 1.2538
r_lose_average: -2.4419, r_lose_min: -7.0938, r_lose_max: 1.2969, r_lose_std: 1.6486
eta_win_average: 0.1118, eta_win_min: -0.0189, eta_win_max: 0.2383, eta_win_std: 0.0417
eta_lose_average: 0.1371, eta_lose_min: -0.0156, eta_lose_max: 0.2676, eta_lose_std: 0.0395
p_win_average: 0.0470, p_win_min: -0.0649, p_win_max: 0.2285, p_win_std: 0.0334
p_lose_average: 0.0226, p_lose_min: -0.0991, p_lose_max: 0.1553, p_lose_std: 0.0378

eval_z_samples_size: 1000
eval_loss: 0.3713, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.7962, true_2_rate: 0.8421
log joint likelihood: tensor(-701.8750000000) joint log likelihood: tensor(-1099.6137695312)
r_win_average: -0.2832, r_win_min: -5.0625, r_win_max: 2.4688, r_win_std: 1.2500
r_lose_average: -2.4854, r_lose_min: -7.0625, r_lose_max: 1.2656, r_lose_std: 1.6476
eta_win_average: 0.0337, eta_win_min: -0.0364, eta_win_max: 0.1367, eta_win_std: 0.0173
eta_lose_average: 0.0330, eta_lose_min: -0.0439, eta_lose_max: 0.0854, eta_lose_std: 0.0185
p_win_average: 0.0764, p_win_min: 0.0141, p_win_max: 0.1582, p_win_std: 0.0228
p_lose_average: 0.0824, p_lose_min: -0.0034, p_lose_max: 0.1846, p_lose_std: 0.0232

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.3745, accuracy: 0.8379
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8226, true_2_rate: 0.8543
log joint likelihood: tensor(-700.8422851562) joint log likelihood: tensor(-1085.4404296875)
r_win_average: 0.1267, r_win_min: -4.7500, r_win_max: 2.9219, r_win_std: 1.2871
r_lose_average: -2.1280, r_lose_min: -6.9062, r_lose_max: 1.7188, r_lose_std: 1.7113
eta_win_average: -0.1530, eta_win_min: -0.3809, eta_win_max: 0.1992, eta_win_std: 0.0847
eta_lose_average: -0.2005, eta_lose_min: -0.4238, eta_lose_max: 0.0942, eta_lose_std: 0.0736
p_win_average: 0.2483, p_win_min: -0.1328, p_win_max: 0.4180, p_win_std: 0.0590
p_lose_average: 0.2859, p_lose_min: 0.0243, p_lose_max: 0.4922, p_lose_std: 0.0615

eval_z_samples_size: 1000
eval_loss: 0.3718, accuracy: 0.8359
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8189, true_2_rate: 0.8543
log joint likelihood: tensor(-702.0424804688) joint log likelihood: tensor(-1087.2070312500)
r_win_average: 0.2141, r_win_min: -4.6562, r_win_max: 3.0625, r_win_std: 1.2791
r_lose_average: -2.0431, r_lose_min: -6.7500, r_lose_max: 1.8516, r_lose_std: 1.7128
eta_win_average: 0.0721, eta_win_min: -0.0014, eta_win_max: 0.1494, eta_win_std: 0.0187
eta_lose_average: 0.0838, eta_lose_min: -0.0059, eta_lose_max: 0.1621, eta_lose_std: 0.0205
p_win_average: 0.1105, p_win_min: 0.0349, p_win_max: 0.2090, p_win_std: 0.0292
p_lose_average: 0.0865, p_lose_min: 0.0019, p_lose_max: 0.2012, p_lose_std: 0.0312

------------------------------------------------------------------------------------------
[2023-09-25 10:14:09,997] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:24,931] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:25,175] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:25,329] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:25,379] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:25,478] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:25,524] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:25,532] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:25,546] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:14:30,823] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:30,823] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,344] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:31,344] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,458] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:31,458] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,558] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:31,558] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,631] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:31,631] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,667] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:31,667] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,668] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:31,668] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,697] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:14:31,697] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:14:31,697] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking_new_final/se_cooking_preference-ref_size10-enn_dim256-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
torch seed 242
cuda seed 242
torch seed 442
cuda seed 442
torch seed 542
cuda seed 542
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-25 10:15:10,190] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 10:15:14,796] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 10:15:14,796] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 10:15:14,796] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 10:15:14,802] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 10:15:14,802] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 10:15:14,802] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 10:15:14,802] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 10:15:14,802] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 10:15:14,802] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 10:15:14,802] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (111298, False)] 
[2023-09-25 10:15:28,008] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 10:15:28,009] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.02 GB         Max_CA 8 GB 
[2023-09-25 10:15:28,009] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.45 GB, percent = 6.2%
[2023-09-25 10:15:28,109] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 10:15:28,109] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 16.0 GB         Max_CA 16 GB 
[2023-09-25 10:15:28,110] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.45 GB, percent = 6.2%
[2023-09-25 10:15:28,110] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 10:15:28,199] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 10:15:28,199] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 16.0 GB         Max_CA 16 GB 
[2023-09-25 10:15:28,199] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 62.45 GB, percent = 6.2%
[2023-09-25 10:15:28,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 10:15:28,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 10:15:28,200] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 10:15:28,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 10:15:28,201] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f68ed223f10>
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 10:15:28,201] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 10:15:28,202] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 10:15:28,202] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 1.1978, accuracy: 0.5137
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.5396, true_2_rate: 0.4858
log joint likelihood: tensor(-648.4804687500) joint log likelihood: tensor(-5998.6406250000)
r_win_average: -1.6865, r_win_min: -5.7188, r_win_max: 2.7500, r_win_std: 1.3921
r_lose_average: -3.6377, r_lose_min: -7.8750, r_lose_max: -0.2188, r_lose_std: 1.6014
eta_win_average: -0.7765, eta_win_min: -1.4688, eta_win_max: 0.0356, eta_win_std: 0.2322
eta_lose_average: -0.8373, eta_lose_min: -1.5078, eta_lose_max: -0.1318, eta_lose_std: 0.2167
p_win_average: -0.2715, p_win_min: -0.7734, p_win_max: 0.4062, p_win_std: 0.1887
p_lose_average: -0.2855, p_lose_min: -0.8281, p_lose_max: 0.4551, p_lose_std: 0.1952

eval_z_samples_size: 1000
eval_loss: 1.1987, accuracy: 0.5020
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.5094, true_2_rate: 0.4939
log joint likelihood: tensor(-634.3554687500) joint log likelihood: tensor(-5958.8750000000)
r_win_average: -0.4194, r_win_min: -4.4688, r_win_max: 3.3438, r_win_std: 1.3440
r_lose_average: -2.3486, r_lose_min: -6.5000, r_lose_max: 1.2578, r_lose_std: 1.5918
eta_win_average: 0.0341, eta_win_min: -0.2314, eta_win_max: 0.3477, eta_win_std: 0.0928
eta_lose_average: 0.0322, eta_lose_min: -0.2119, eta_lose_max: 0.3418, eta_lose_std: 0.0915
p_win_average: 0.1717, p_win_min: -0.1436, p_win_max: 0.4160, p_win_std: 0.0910
p_lose_average: 0.1481, p_lose_min: -0.1011, p_lose_max: 0.5664, p_lose_std: 0.0975

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.4885, accuracy: 0.7930
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.7962, true_2_rate: 0.7895
log joint likelihood: tensor(-902.1328125000) joint log likelihood: tensor(-1677.3984375000)
r_win_average: 0.9311, r_win_min: -1.8984, r_win_max: 3.7188, r_win_std: 0.7526
r_lose_average: -0.1678, r_lose_min: -2.6406, r_lose_max: 2.3281, r_lose_std: 0.8654
eta_win_average: -0.1470, eta_win_min: -0.4453, eta_win_max: 0.4512, eta_win_std: 0.0896
eta_lose_average: -0.1697, eta_lose_min: -0.4180, eta_lose_max: 0.2852, eta_lose_std: 0.0595
p_win_average: 0.9981, p_win_min: 0.6211, p_win_max: 1.6719, p_win_std: 0.1141
p_lose_average: 0.9585, p_lose_min: 0.3418, p_lose_max: 1.4531, p_lose_std: 0.0847

eval_z_samples_size: 1000
eval_loss: 0.4866, accuracy: 0.7891
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.7811, true_2_rate: 0.7976
log joint likelihood: tensor(-897.0039062500) joint log likelihood: tensor(-1678.4921875000)
r_win_average: -0.0724, r_win_min: -2.7969, r_win_max: 2.2969, r_win_std: 0.6809
r_lose_average: -1.1046, r_lose_min: -3.4688, r_lose_max: 1.3906, r_lose_std: 0.8317
eta_win_average: -0.1099, eta_win_min: -0.3066, eta_win_max: 0.0608, eta_win_std: 0.0485
eta_lose_average: -0.0938, eta_lose_min: -0.2852, eta_lose_max: 0.1953, eta_lose_std: 0.0395
p_win_average: -0.0475, p_win_min: -0.1533, p_win_max: 0.2090, p_win_std: 0.0266
p_lose_average: -0.0495, p_lose_min: -0.1523, p_lose_max: 0.0381, p_lose_std: 0.0234

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.4539, accuracy: 0.7988
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8075, true_2_rate: 0.7895
log joint likelihood: tensor(-883.0195312500) joint log likelihood: tensor(-1434.0781250000)
r_win_average: 0.6233, r_win_min: -2.4688, r_win_max: 2.8281, r_win_std: 0.7865
r_lose_average: -0.6063, r_lose_min: -3.3594, r_lose_max: 2.0625, r_lose_std: 1.0308
eta_win_average: 0.1992, eta_win_min: -0.0117, eta_win_max: 0.4688, eta_win_std: 0.0611
eta_lose_average: 0.1595, eta_lose_min: -0.6367, eta_lose_max: 0.4277, eta_lose_std: 0.0694
p_win_average: 0.1247, p_win_min: -0.0476, p_win_max: 0.3145, p_win_std: 0.0410
p_lose_average: 0.1208, p_lose_min: -0.0142, p_lose_max: 0.2637, p_lose_std: 0.0381

eval_z_samples_size: 1000
eval_loss: 0.4531, accuracy: 0.7930
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.8000, true_2_rate: 0.7854
log joint likelihood: tensor(-879.2695312500) joint log likelihood: tensor(-1426.4101562500)
r_win_average: 0.1278, r_win_min: -2.8750, r_win_max: 2.1719, r_win_std: 0.7457
r_lose_average: -1.0486, r_lose_min: -3.5781, r_lose_max: 1.3516, r_lose_std: 0.9808
eta_win_average: -0.1534, eta_win_min: -0.2275, eta_win_max: -0.0381, eta_win_std: 0.0189
eta_lose_average: -0.1479, eta_lose_min: -0.2080, eta_lose_max: -0.0166, eta_lose_std: 0.0186
p_win_average: -0.0195, p_win_min: -0.1348, p_win_max: 0.0347, p_win_std: 0.0242
p_lose_average: -0.0132, p_lose_min: -0.1025, p_lose_max: 0.0403, p_lose_std: 0.0200

------------------------------------------------------------------------------------------
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 100, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 500, 'num_ref_train': 500, 'eval_z_size_list': '100,1000', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
