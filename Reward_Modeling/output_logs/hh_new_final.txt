[2023-09-24 23:59:02,446] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:07,370] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,280] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,531] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,718] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,779] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,933] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,948] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,960] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:18,969] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-24 23:59:22,761] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:22,761] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:59:22,766] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:22,766] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:59:23,078] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:23,078] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:59:23,253] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:23,253] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:59:23,285] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:23,285] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:59:23,288] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:23,288] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:59:23,291] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:23,291] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-24 23:59:23,291] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-24 23:59:23,296] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-24 23:59:23,296] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 742
cuda seed 742
torch seed 142
cuda seed 142
torch seed 242
cuda seed 242
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/hh_new_final/anthropic_hh-ref_size10-enn_dim64-num_ref_train10-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 442
cuda seed 442
torch seed 542
cuda seed 542
torch seed 642
cuda seed 642
torch seed 342
cuda seed 342
training dataset size: 2544
eval dataset size: 8
joint eval dataset size: 256
Total steps:  2544
Warmup steps:  76
[2023-09-24 23:59:57,316] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 00:00:00,533] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 00:00:00,534] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 00:00:00,535] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 00:00:00,540] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 00:00:00,540] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 00:00:00,542] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 00:00:00,543] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 00:00:00,544] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 00:00:00,544] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 00:00:00,544] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-25 00:00:13,458] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 00:00:13,458] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 00:00:13,459] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 46.65 GB, percent = 4.6%
[2023-09-25 00:00:13,568] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 00:00:13,568] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 00:00:13,570] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 46.65 GB, percent = 4.6%
[2023-09-25 00:00:13,571] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 00:00:13,671] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 00:00:13,672] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 00:00:13,673] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 46.65 GB, percent = 4.6%
[2023-09-25 00:00:13,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 00:00:13,675] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 00:00:13,677] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 00:00:13,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 00:00:13,681] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 00:00:13,681] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 00:00:13,682] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 00:00:13,682] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 00:00:13,683] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 00:00:13,683] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 00:00:13,684] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 00:00:13,684] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 00:00:13,686] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 00:00:13,686] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 00:00:13,686] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f74ac20b450>
[2023-09-25 00:00:13,687] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 00:00:13,687] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 00:00:13,688] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 00:00:13,688] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 00:00:13,688] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 00:00:13,689] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 00:00:13,689] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 00:00:13,689] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 00:00:13,690] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 00:00:13,690] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 00:00:13,690] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 00:00:13,690] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 00:00:13,690] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 00:00:13,690] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 00:00:13,691] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 00:00:13,691] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 00:00:13,692] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 00:00:13,692] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 00:00:13,693] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 00:00:13,693] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 00:00:13,694] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 00:00:13,694] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 00:00:13,696] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 00:00:13,696] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 00:00:13,696] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 00:00:13,697] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 00:00:13,697] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 00:00:13,697] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 00:00:13,698] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 00:00:13,698] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 00:00:13,699] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 00:00:13,699] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 00:00:13,700] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 00:00:13,700] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 00:00:13,700] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 00:00:13,700] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 00:00:13,701] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 00:00:13,702] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 00:00:13,702] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 00:00:13,704] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 00:00:13,704] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 00:00:13,704] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 00:00:13,704] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 00:00:13,705] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 00:00:13,705] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 00:00:13,705] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 00:00:13,705] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 00:00:13,705] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 00:00:13,706] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 00:00:13,706] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 00:00:13,706] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 00:00:13,707] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 00:00:13,707] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 00:00:13,707] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 00:00:13,708] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 00:00:13,708] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 00:00:13,709] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 00:00:13,709] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 00:00:13,709] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 00:00:13,710] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 0.8320, accuracy: 0.5527
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.5538, true_2_rate: 0.5516
log joint likelihood: tensor(-730.0312500000) joint log likelihood: tensor(-4136.8593750000)
r_win_average: -3.1621, r_win_min: -5.8125, r_win_max: 2.8594, r_win_std: 1.3653
r_lose_average: -4.3405, r_lose_min: -6.7500, r_lose_max: 0.1318, r_lose_std: 1.0439
eta_win_average: -0.1926, eta_win_min: -1.0703, eta_win_max: 0.2598, eta_win_std: 0.1589
eta_lose_average: -0.2199, eta_lose_min: -1.1562, eta_lose_max: 0.2617, eta_lose_std: 0.1597
p_win_average: -0.4364, p_win_min: -1.0938, p_win_max: 0.3184, p_win_std: 0.1842
p_lose_average: -0.4467, p_lose_min: -0.8984, p_lose_max: 0.1001, p_lose_std: 0.1704

eval_z_samples_size: 1000
eval_loss: 0.8481, accuracy: 0.5371
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5462, true_2_rate: 0.5278
log joint likelihood: tensor(-707.7812500000) joint log likelihood: tensor(-4127.7890625000)
r_win_average: -2.3291, r_win_min: -4.8125, r_win_max: 3.8594, r_win_std: 1.3352
r_lose_average: -3.4947, r_lose_min: -5.9688, r_lose_max: 0.9727, r_lose_std: 1.0218
eta_win_average: 0.0309, eta_win_min: -0.2930, eta_win_max: 0.2793, eta_win_std: 0.0793
eta_lose_average: 0.0148, eta_lose_min: -0.1572, eta_lose_max: 0.2676, eta_lose_std: 0.0736
p_win_average: 0.1644, p_win_min: -0.1367, p_win_max: 0.3633, p_win_std: 0.0773
p_lose_average: 0.1734, p_lose_min: -0.1299, p_lose_max: 0.3691, p_lose_std: 0.0713

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.6270, accuracy: 0.6445
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.6346, true_2_rate: 0.6548
log joint likelihood: tensor(-1234.0625000000) joint log likelihood: tensor(-1853.8281250000)
r_win_average: 0.0644, r_win_min: -1.8125, r_win_max: 3.8906, r_win_std: 0.7537
r_lose_average: -0.5568, r_lose_min: -2.2656, r_lose_max: 1.7656, r_lose_std: 0.6058
eta_win_average: -0.1757, eta_win_min: -0.5312, eta_win_max: 0.1680, eta_win_std: 0.0481
eta_lose_average: -0.1723, eta_lose_min: -0.4004, eta_lose_max: -0.0206, eta_lose_std: 0.0397
p_win_average: -0.4952, p_win_min: -0.8438, p_win_max: 0.1348, p_win_std: 0.1068
p_lose_average: -0.5326, p_lose_min: -0.7656, p_lose_max: -0.1206, p_lose_std: 0.0799

eval_z_samples_size: 1000
eval_loss: 0.6348, accuracy: 0.6309
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.6462, true_2_rate: 0.6151
log joint likelihood: tensor(-1232.4609375000) joint log likelihood: tensor(-1857.9375000000)
r_win_average: 0.7990, r_win_min: -0.9805, r_win_max: 4.3750, r_win_std: 0.7082
r_lose_average: 0.2168, r_lose_min: -1.4531, r_lose_max: 2.3125, r_lose_std: 0.5734
eta_win_average: -0.0595, eta_win_min: -0.1123, eta_win_max: 0.1455, eta_win_std: 0.0274
eta_lose_average: -0.0615, eta_lose_min: -0.1260, eta_lose_max: 0.0708, eta_lose_std: 0.0194
p_win_average: 0.1226, p_win_min: -0.1338, p_win_max: 0.2158, p_win_std: 0.0383
p_lose_average: 0.1308, p_lose_min: -0.0146, p_lose_max: 0.2412, p_lose_std: 0.0301

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.5991, accuracy: 0.6543
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.6692, true_2_rate: 0.6389
log joint likelihood: tensor(-1267.5625000000) joint log likelihood: tensor(-1659.2578125000)
r_win_average: 0.4724, r_win_min: -2.4688, r_win_max: 2.7656, r_win_std: 0.8075
r_lose_average: -0.2376, r_lose_min: -2.9062, r_lose_max: 1.9453, r_lose_std: 0.8852
eta_win_average: 0.0560, eta_win_min: -0.0542, eta_win_max: 0.1436, eta_win_std: 0.0261
eta_lose_average: 0.0436, eta_lose_min: -0.0444, eta_lose_max: 0.1187, eta_lose_std: 0.0279
p_win_average: 0.0375, p_win_min: -0.0918, p_win_max: 0.2871, p_win_std: 0.0360
p_lose_average: 0.0334, p_lose_min: -0.0933, p_lose_max: 0.1934, p_lose_std: 0.0371

eval_z_samples_size: 1000
eval_loss: 0.5981, accuracy: 0.6582
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.6769, true_2_rate: 0.6389
log joint likelihood: tensor(-1262.7187500000) joint log likelihood: tensor(-1663.2265625000)
r_win_average: 0.3396, r_win_min: -2.4844, r_win_max: 2.4688, r_win_std: 0.7823
r_lose_average: -0.3510, r_lose_min: -2.9688, r_lose_max: 1.7891, r_lose_std: 0.8554
eta_win_average: 0.0118, eta_win_min: -0.0162, eta_win_max: 0.0320, eta_win_std: 0.0065
eta_lose_average: 0.0132, eta_lose_min: -0.0122, eta_lose_max: 0.0288, eta_lose_std: 0.0058
p_win_average: -0.0518, p_win_min: -0.0986, p_win_max: -0.0137, p_win_std: 0.0135
p_lose_average: -0.0487, p_lose_min: -0.0903, p_lose_max: -0.0134, p_lose_std: 0.0137

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.5640, accuracy: 0.6914
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7077, true_2_rate: 0.6746
log joint likelihood: tensor(-1210.7890625000) joint log likelihood: tensor(-1555.8593750000)
r_win_average: -0.3701, r_win_min: -3.7031, r_win_max: 1.8281, r_win_std: 0.9736
r_lose_average: -1.2819, r_lose_min: -4.2188, r_lose_max: 1.5312, r_lose_std: 1.0525
eta_win_average: -0.5030, eta_win_min: -0.6016, eta_win_max: -0.3867, eta_win_std: 0.0287
eta_lose_average: -0.5049, eta_lose_min: -0.5820, eta_lose_max: -0.4082, eta_lose_std: 0.0274
p_win_average: -0.3103, p_win_min: -0.4238, p_win_max: -0.1943, p_win_std: 0.0366
p_lose_average: -0.3118, p_lose_min: -0.4082, p_lose_max: -0.2119, p_lose_std: 0.0361

eval_z_samples_size: 1000
eval_loss: 0.5654, accuracy: 0.6855
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7038, true_2_rate: 0.6667
log joint likelihood: tensor(-1211.5820312500) joint log likelihood: tensor(-1552.4531250000)
r_win_average: 0.5323, r_win_min: -2.8438, r_win_max: 2.7500, r_win_std: 0.9674
r_lose_average: -0.3678, r_lose_min: -3.2656, r_lose_max: 2.4844, r_lose_std: 1.0421
eta_win_average: 0.0297, eta_win_min: -0.0002, eta_win_max: 0.0552, eta_win_std: 0.0075
eta_lose_average: 0.0324, eta_lose_min: -0.0011, eta_lose_max: 0.0554, eta_lose_std: 0.0081
p_win_average: 0.0590, p_win_min: -0.0065, p_win_max: 0.1099, p_win_std: 0.0195
p_lose_average: 0.0653, p_lose_min: 0.0062, p_lose_max: 0.1230, p_lose_std: 0.0180

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.5547, accuracy: 0.6973
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7192, true_2_rate: 0.6746
log joint likelihood: tensor(-1200.4531250000) joint log likelihood: tensor(-1522.9101562500)
r_win_average: -0.0795, r_win_min: -3.6719, r_win_max: 2.8594, r_win_std: 1.2188
r_lose_average: -1.1970, r_lose_min: -4.1250, r_lose_max: 2.3906, r_lose_std: 1.2370
eta_win_average: -0.1184, eta_win_min: -0.2129, eta_win_max: 0.0398, eta_win_std: 0.0351
eta_lose_average: -0.1367, eta_lose_min: -0.2324, eta_lose_max: -0.0242, eta_lose_std: 0.0344
p_win_average: -0.6487, p_win_min: -0.8633, p_win_max: -0.2852, p_win_std: 0.0739
p_lose_average: -0.6871, p_lose_min: -0.8672, p_lose_max: -0.4180, p_lose_std: 0.0688

eval_z_samples_size: 1000
eval_loss: 0.5569, accuracy: 0.6953
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5352, prediction_2_rate: 0.4648
true_1_rate: 0.7269, true_2_rate: 0.6627
log joint likelihood: tensor(-1195.0585937500) joint log likelihood: tensor(-1522.4296875000)
r_win_average: 0.4656, r_win_min: -2.9844, r_win_max: 3.3125, r_win_std: 1.1841
r_lose_average: -0.6016, r_lose_min: -3.4844, r_lose_max: 2.8594, r_lose_std: 1.1939
eta_win_average: -0.1144, eta_win_min: -0.1729, eta_win_max: -0.0525, eta_win_std: 0.0149
eta_lose_average: -0.1195, eta_lose_min: -0.1562, eta_lose_max: -0.0525, eta_lose_std: 0.0140
p_win_average: -0.1092, p_win_min: -0.1621, p_win_max: -0.0361, p_win_std: 0.0158
p_lose_average: -0.1069, p_lose_min: -0.1641, p_lose_max: -0.0586, p_lose_std: 0.0153

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.5530, accuracy: 0.7285
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.7269, true_2_rate: 0.7302
log joint likelihood: tensor(-1216.6054687500) joint log likelihood: tensor(-1486.5976562500)
r_win_average: -0.4069, r_win_min: -3.2656, r_win_max: 2.7969, r_win_std: 0.9469
r_lose_average: -1.2989, r_lose_min: -3.5938, r_lose_max: 1.5703, r_lose_std: 0.9621
eta_win_average: -0.0260, eta_win_min: -0.1514, eta_win_max: 0.0908, eta_win_std: 0.0298
eta_lose_average: -0.0166, eta_lose_min: -0.0854, eta_lose_max: 0.0747, eta_lose_std: 0.0269
p_win_average: -0.2448, p_win_min: -0.5156, p_win_max: -0.0391, p_win_std: 0.0605
p_lose_average: -0.2579, p_lose_min: -0.4180, p_lose_max: -0.0679, p_lose_std: 0.0527

eval_z_samples_size: 1000
eval_loss: 0.5500, accuracy: 0.7207
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.7192, true_2_rate: 0.7222
log joint likelihood: tensor(-1211.7695312500) joint log likelihood: tensor(-1489.2812500000)
r_win_average: -0.2916, r_win_min: -3.2656, r_win_max: 2.8750, r_win_std: 0.9574
r_lose_average: -1.1889, r_lose_min: -3.4844, r_lose_max: 1.6641, r_lose_std: 0.9710
eta_win_average: 0.0390, eta_win_min: 0.0058, eta_win_max: 0.0679, eta_win_std: 0.0088
eta_lose_average: 0.0384, eta_lose_min: 0.0112, eta_lose_max: 0.0718, eta_lose_std: 0.0082
p_win_average: -0.1953, p_win_min: -0.2656, p_win_max: -0.1099, p_win_std: 0.0192
p_lose_average: -0.2021, p_lose_min: -0.2656, p_lose_max: -0.1445, p_lose_std: 0.0180

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.5676, accuracy: 0.6934
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7000, true_2_rate: 0.6865
log joint likelihood: tensor(-1212.5605468750) joint log likelihood: tensor(-1533.0546875000)
r_win_average: 0.1643, r_win_min: -4.4688, r_win_max: 3.2969, r_win_std: 1.1074
r_lose_average: -0.9555, r_lose_min: -4.9375, r_lose_max: 1.8672, r_lose_std: 1.3722
eta_win_average: -0.2628, eta_win_min: -0.3906, eta_win_max: -0.0640, eta_win_std: 0.0395
eta_lose_average: -0.2758, eta_lose_min: -0.4102, eta_lose_max: -0.1250, eta_lose_std: 0.0398
p_win_average: -0.4042, p_win_min: -0.5898, p_win_max: -0.2305, p_win_std: 0.0562
p_lose_average: -0.3948, p_lose_min: -0.6875, p_lose_max: -0.1855, p_lose_std: 0.0603

eval_z_samples_size: 1000
eval_loss: 0.5654, accuracy: 0.7031
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.7077, true_2_rate: 0.6984
log joint likelihood: tensor(-1209.1367187500) joint log likelihood: tensor(-1539.2773437500)
r_win_average: 0.6965, r_win_min: -3.7500, r_win_max: 3.9062, r_win_std: 1.1023
r_lose_average: -0.4150, r_lose_min: -4.3125, r_lose_max: 2.4375, r_lose_std: 1.3602
eta_win_average: 0.0178, eta_win_min: -0.0742, eta_win_max: 0.0574, eta_win_std: 0.0131
eta_lose_average: 0.0234, eta_lose_min: -0.0339, eta_lose_max: 0.0566, eta_lose_std: 0.0128
p_win_average: -0.1534, p_win_min: -0.1953, p_win_max: -0.0894, p_win_std: 0.0155
p_lose_average: -0.1526, p_lose_min: -0.2021, p_lose_max: -0.0942, p_lose_std: 0.0147

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.5469, accuracy: 0.6934
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7154, true_2_rate: 0.6706
log joint likelihood: tensor(-1219.2265625000) joint log likelihood: tensor(-1491.1796875000)
r_win_average: 0.0938, r_win_min: -3.3125, r_win_max: 3.2031, r_win_std: 0.9852
r_lose_average: -0.9101, r_lose_min: -4.0000, r_lose_max: 1.9297, r_lose_std: 1.1010
eta_win_average: 0.0284, eta_win_min: -0.0786, eta_win_max: 0.0908, eta_win_std: 0.0221
eta_lose_average: 0.0325, eta_lose_min: -0.0576, eta_lose_max: 0.1138, eta_lose_std: 0.0210
p_win_average: -0.3090, p_win_min: -0.3887, p_win_max: -0.1592, p_win_std: 0.0344
p_lose_average: -0.3179, p_lose_min: -0.3926, p_lose_max: -0.2100, p_lose_std: 0.0296

eval_z_samples_size: 1000
eval_loss: 0.5479, accuracy: 0.6973
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7192, true_2_rate: 0.6746
log joint likelihood: tensor(-1213.8085937500) joint log likelihood: tensor(-1487.4375000000)
r_win_average: 0.1761, r_win_min: -3.2500, r_win_max: 3.2812, r_win_std: 0.9808
r_lose_average: -0.8221, r_lose_min: -3.8750, r_lose_max: 1.9766, r_lose_std: 1.0914
eta_win_average: -0.0635, eta_win_min: -0.0913, eta_win_max: -0.0369, eta_win_std: 0.0081
eta_lose_average: -0.0630, eta_lose_min: -0.0869, eta_lose_max: -0.0302, eta_lose_std: 0.0080
p_win_average: -0.1349, p_win_min: -0.1738, p_win_max: -0.0801, p_win_std: 0.0137
p_lose_average: -0.1344, p_lose_min: -0.1680, p_lose_max: -0.0889, p_lose_std: 0.0130

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 100
eval_loss: 0.5420, accuracy: 0.6973
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7115, true_2_rate: 0.6825
log joint likelihood: tensor(-1196.9199218750) joint log likelihood: tensor(-1464.5957031250)
r_win_average: 0.1818, r_win_min: -3.8594, r_win_max: 3.7656, r_win_std: 1.2268
r_lose_average: -1.0005, r_lose_min: -4.4688, r_lose_max: 2.2969, r_lose_std: 1.2756
eta_win_average: 0.3602, eta_win_min: 0.1963, eta_win_max: 0.5273, eta_win_std: 0.0428
eta_lose_average: 0.3698, eta_lose_min: 0.1982, eta_lose_max: 0.5469, eta_lose_std: 0.0382
p_win_average: -0.2458, p_win_min: -0.4258, p_win_max: -0.0153, p_win_std: 0.0592
p_lose_average: -0.2668, p_lose_min: -0.4102, p_lose_max: -0.0393, p_lose_std: 0.0506

eval_z_samples_size: 1000
eval_loss: 0.5425, accuracy: 0.6953
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7115, true_2_rate: 0.6786
log joint likelihood: tensor(-1193.4375000000) joint log likelihood: tensor(-1465.5156250000)
r_win_average: -0.2674, r_win_min: -4.3125, r_win_max: 3.2656, r_win_std: 1.2070
r_lose_average: -1.4328, r_lose_min: -4.9062, r_lose_max: 1.7734, r_lose_std: 1.2553
eta_win_average: -0.0965, eta_win_min: -0.1357, eta_win_max: -0.0654, eta_win_std: 0.0100
eta_lose_average: -0.0935, eta_lose_min: -0.1357, eta_lose_max: -0.0698, eta_lose_std: 0.0092
p_win_average: -0.2390, p_win_min: -0.3145, p_win_max: -0.1689, p_win_std: 0.0250
p_lose_average: -0.2351, p_lose_min: -0.3164, p_lose_max: -0.1533, p_lose_std: 0.0230

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 100
eval_loss: 0.5378, accuracy: 0.7070
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7231, true_2_rate: 0.6905
log joint likelihood: tensor(-1197.6289062500) joint log likelihood: tensor(-1449.3183593750)
r_win_average: -1.0540, r_win_min: -5.4062, r_win_max: 2.4219, r_win_std: 1.1294
r_lose_average: -2.1875, r_lose_min: -5.8750, r_lose_max: 0.9805, r_lose_std: 1.2792
eta_win_average: -0.3380, eta_win_min: -0.4668, eta_win_max: -0.1865, eta_win_std: 0.0382
eta_lose_average: -0.3477, eta_lose_min: -0.4746, eta_lose_max: -0.2119, eta_lose_std: 0.0358
p_win_average: -0.7086, p_win_min: -0.8945, p_win_max: -0.3223, p_win_std: 0.0740
p_lose_average: -0.7462, p_lose_min: -0.9531, p_lose_max: -0.4746, p_lose_std: 0.0663

eval_z_samples_size: 1000
eval_loss: 0.5396, accuracy: 0.7109
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7269, true_2_rate: 0.6944
log joint likelihood: tensor(-1194.7031250000) joint log likelihood: tensor(-1452.9218750000)
r_win_average: -0.0316, r_win_min: -4.0938, r_win_max: 3.1406, r_win_std: 1.0855
r_lose_average: -1.1116, r_lose_min: -4.6875, r_lose_max: 1.8594, r_lose_std: 1.2313
eta_win_average: 0.0155, eta_win_min: -0.0344, eta_win_max: 0.0488, eta_win_std: 0.0094
eta_lose_average: 0.0165, eta_lose_min: -0.0057, eta_lose_max: 0.0447, eta_lose_std: 0.0088
p_win_average: -0.0403, p_win_min: -0.0801, p_win_max: 0.0103, p_win_std: 0.0138
p_lose_average: -0.0342, p_lose_min: -0.0820, p_lose_max: 0.0271, p_lose_std: 0.0137

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 100
eval_loss: 0.5408, accuracy: 0.7109
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7269, true_2_rate: 0.6944
log joint likelihood: tensor(-1187.5507812500) joint log likelihood: tensor(-1455.7011718750)
r_win_average: 0.1762, r_win_min: -4.0000, r_win_max: 3.2500, r_win_std: 1.1772
r_lose_average: -0.9864, r_lose_min: -5.3438, r_lose_max: 2.0781, r_lose_std: 1.3887
eta_win_average: 0.1732, eta_win_min: 0.0967, eta_win_max: 0.2461, eta_win_std: 0.0247
eta_lose_average: 0.1857, eta_lose_min: 0.0952, eta_lose_max: 0.2988, eta_lose_std: 0.0267
p_win_average: -0.1953, p_win_min: -0.2988, p_win_max: -0.1001, p_win_std: 0.0262
p_lose_average: -0.2048, p_lose_min: -0.4023, p_lose_max: -0.1328, p_lose_std: 0.0291

eval_z_samples_size: 1000
eval_loss: 0.5388, accuracy: 0.7168
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.7269, true_2_rate: 0.7063
log joint likelihood: tensor(-1178.0371093750) joint log likelihood: tensor(-1454.6875000000)
r_win_average: -0.0238, r_win_min: -4.1875, r_win_max: 3.0625, r_win_std: 1.1849
r_lose_average: -1.1904, r_lose_min: -5.4688, r_lose_max: 1.9141, r_lose_std: 1.3938
eta_win_average: -0.0326, eta_win_min: -0.1436, eta_win_max: 0.0122, eta_win_std: 0.0156
eta_lose_average: -0.0283, eta_lose_min: -0.0767, eta_lose_max: 0.0170, eta_lose_std: 0.0148
p_win_average: -0.1891, p_win_min: -0.2432, p_win_max: -0.0952, p_win_std: 0.0184
p_lose_average: -0.1950, p_lose_min: -0.2559, p_lose_max: -0.1357, p_lose_std: 0.0166

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.5374, accuracy: 0.7148
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7269, true_2_rate: 0.7024
log joint likelihood: tensor(-1181.7988281250) joint log likelihood: tensor(-1457.6367187500)
r_win_average: 0.1728, r_win_min: -3.9375, r_win_max: 3.2500, r_win_std: 1.2131
r_lose_average: -1.0301, r_lose_min: -5.4688, r_lose_max: 2.0469, r_lose_std: 1.4229
eta_win_average: 0.0218, eta_win_min: -0.0571, eta_win_max: 0.1426, eta_win_std: 0.0210
eta_lose_average: 0.0118, eta_lose_min: -0.1011, eta_lose_max: 0.0762, eta_lose_std: 0.0237
p_win_average: -0.0845, p_win_min: -0.1611, p_win_max: 0.0747, p_win_std: 0.0266
p_lose_average: -0.0895, p_lose_min: -0.1855, p_lose_max: -0.0047, p_lose_std: 0.0281

eval_z_samples_size: 1000
eval_loss: 0.5388, accuracy: 0.7129
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7269, true_2_rate: 0.6984
log joint likelihood: tensor(-1176.8144531250) joint log likelihood: tensor(-1456.3691406250)
r_win_average: 0.0877, r_win_min: -3.9219, r_win_max: 3.0938, r_win_std: 1.1838
r_lose_average: -1.0861, r_lose_min: -5.4062, r_lose_max: 1.9688, r_lose_std: 1.3866
eta_win_average: -0.1717, eta_win_min: -0.2197, eta_win_max: -0.1279, eta_win_std: 0.0113
eta_lose_average: -0.1743, eta_lose_min: -0.2100, eta_lose_max: -0.1348, eta_lose_std: 0.0098
p_win_average: 0.0237, p_win_min: -0.0593, p_win_max: 0.1123, p_win_std: 0.0289
p_lose_average: 0.0409, p_lose_min: -0.0481, p_lose_max: 0.1245, p_lose_std: 0.0289

------------------------------------------------------------------------------------------
[2023-09-25 02:00:22,615] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,500] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,593] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,780] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,841] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,853] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,859] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,889] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:36,907] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 02:00:41,528] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:41,528] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:41,663] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:41,663] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:42,045] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:42,046] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:42,052] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:42,053] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:42,102] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:42,102] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:42,147] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:42,147] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:42,163] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:42,163] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:42,167] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 02:00:42,167] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 02:00:42,167] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
torch seed 642
cuda seed 642
torch seed 242
cuda seed 242
torch seed 342
cuda seed 342
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/hh_new_final/anthropic_hh-ref_size10-enn_dim64-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 742
cuda seed 742
torch seed 142
cuda seed 142
torch seed torch seed542 
442
cuda seed 542
cuda seed 442
training dataset size: 2544
eval dataset size: 8
joint eval dataset size: 256
Total steps:  2544
Warmup steps:  76
[2023-09-25 02:01:15,580] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 02:01:18,407] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 02:01:18,408] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 02:01:18,409] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 02:01:18,414] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 02:01:18,414] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 02:01:18,415] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 02:01:18,416] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 02:01:18,417] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 02:01:18,417] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 02:01:18,417] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-25 02:01:31,318] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 02:01:31,318] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 02:01:31,319] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 46.63 GB, percent = 4.6%
[2023-09-25 02:01:31,437] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 02:01:31,438] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 02:01:31,439] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 46.63 GB, percent = 4.6%
[2023-09-25 02:01:31,440] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 02:01:31,541] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 02:01:31,542] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 02:01:31,543] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 46.63 GB, percent = 4.6%
[2023-09-25 02:01:31,545] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 02:01:31,545] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 02:01:31,546] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 02:01:31,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 02:01:31,547] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 02:01:31,548] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 02:01:31,549] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 02:01:31,549] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 02:01:31,549] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 02:01:31,550] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 02:01:31,551] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 02:01:31,551] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 02:01:31,551] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 02:01:31,551] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8671fbb450>
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 02:01:31,552] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 02:01:31,553] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 02:01:31,553] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 0.8320, accuracy: 0.5527
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.5538, true_2_rate: 0.5516
log joint likelihood: tensor(-730.0312500000) joint log likelihood: tensor(-4136.8593750000)
r_win_average: -3.1621, r_win_min: -5.8125, r_win_max: 2.8594, r_win_std: 1.3653
r_lose_average: -4.3405, r_lose_min: -6.7500, r_lose_max: 0.1318, r_lose_std: 1.0439
eta_win_average: -0.1926, eta_win_min: -1.0703, eta_win_max: 0.2598, eta_win_std: 0.1589
eta_lose_average: -0.2199, eta_lose_min: -1.1562, eta_lose_max: 0.2617, eta_lose_std: 0.1597
p_win_average: -0.4364, p_win_min: -1.0938, p_win_max: 0.3184, p_win_std: 0.1842
p_lose_average: -0.4467, p_lose_min: -0.8984, p_lose_max: 0.1001, p_lose_std: 0.1704

eval_z_samples_size: 1000
eval_loss: 0.8481, accuracy: 0.5371
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5462, true_2_rate: 0.5278
log joint likelihood: tensor(-707.7812500000) joint log likelihood: tensor(-4127.7890625000)
r_win_average: -2.3291, r_win_min: -4.8125, r_win_max: 3.8594, r_win_std: 1.3352
r_lose_average: -3.4947, r_lose_min: -5.9688, r_lose_max: 0.9727, r_lose_std: 1.0218
eta_win_average: 0.0309, eta_win_min: -0.2930, eta_win_max: 0.2793, eta_win_std: 0.0793
eta_lose_average: 0.0148, eta_lose_min: -0.1572, eta_lose_max: 0.2676, eta_lose_std: 0.0736
p_win_average: 0.1644, p_win_min: -0.1367, p_win_max: 0.3633, p_win_std: 0.0773
p_lose_average: 0.1734, p_lose_min: -0.1299, p_lose_max: 0.3691, p_lose_std: 0.0713

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.6270, accuracy: 0.6465
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.6654, true_2_rate: 0.6270
log joint likelihood: tensor(-1299.0390625000) joint log likelihood: tensor(-1799.4218750000)
r_win_average: 1.0668, r_win_min: -0.6992, r_win_max: 4.8125, r_win_std: 0.6198
r_lose_average: 0.5639, r_lose_min: -0.9023, r_lose_max: 1.8984, r_lose_std: 0.4917
eta_win_average: 0.0575, eta_win_min: -0.1973, eta_win_max: 0.2734, eta_win_std: 0.0455
eta_lose_average: 0.0509, eta_lose_min: -0.2598, eta_lose_max: 0.1621, eta_lose_std: 0.0380
p_win_average: -0.1135, p_win_min: -0.4258, p_win_max: 0.2168, p_win_std: 0.0503
p_lose_average: -0.1236, p_lose_min: -0.2871, p_lose_max: 0.0288, p_lose_std: 0.0355

eval_z_samples_size: 1000
eval_loss: 0.6323, accuracy: 0.6445
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.6538, true_2_rate: 0.6349
log joint likelihood: tensor(-1300.1718750000) joint log likelihood: tensor(-1801.5468750000)
r_win_average: 1.0714, r_win_min: -0.5586, r_win_max: 4.2812, r_win_std: 0.5989
r_lose_average: 0.5873, r_lose_min: -0.7734, r_lose_max: 1.9453, r_lose_std: 0.4637
eta_win_average: 0.0197, eta_win_min: -0.1660, eta_win_max: 0.0811, eta_win_std: 0.0208
eta_lose_average: 0.0228, eta_lose_min: -0.0703, eta_lose_max: 0.0762, eta_lose_std: 0.0168
p_win_average: -0.0724, p_win_min: -0.1270, p_win_max: 0.0854, p_win_std: 0.0211
p_lose_average: -0.0710, p_lose_min: -0.1299, p_lose_max: 0.0057, p_lose_std: 0.0193

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.6060, accuracy: 0.6621
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5371, prediction_2_rate: 0.4629
true_1_rate: 0.6962, true_2_rate: 0.6270
log joint likelihood: tensor(-1243.3359375000) joint log likelihood: tensor(-1690.6406250000)
r_win_average: 1.3500, r_win_min: -1.8203, r_win_max: 4.5000, r_win_std: 0.9300
r_lose_average: 0.5525, r_lose_min: -2.1875, r_lose_max: 3.2188, r_lose_std: 0.9337
eta_win_average: 0.0352, eta_win_min: -0.0732, eta_win_max: 0.1953, eta_win_std: 0.0334
eta_lose_average: 0.0402, eta_lose_min: -0.0664, eta_lose_max: 0.1494, eta_lose_std: 0.0317
p_win_average: -0.1762, p_win_min: -0.3516, p_win_max: 0.0422, p_win_std: 0.0398
p_lose_average: -0.1912, p_lose_min: -0.3379, p_lose_max: -0.0615, p_lose_std: 0.0376

eval_z_samples_size: 1000
eval_loss: 0.6084, accuracy: 0.6484
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5352, prediction_2_rate: 0.4648
true_1_rate: 0.6808, true_2_rate: 0.6151
log joint likelihood: tensor(-1237.9296875000) joint log likelihood: tensor(-1695.6250000000)
r_win_average: 1.7249, r_win_min: -1.4062, r_win_max: 4.8125, r_win_std: 0.9287
r_lose_average: 0.9313, r_lose_min: -1.7656, r_lose_max: 3.5469, r_lose_std: 0.9266
eta_win_average: 0.0524, eta_win_min: -0.0002, eta_win_max: 0.1079, eta_win_std: 0.0151
eta_lose_average: 0.0466, eta_lose_min: -0.0114, eta_lose_max: 0.1006, eta_lose_std: 0.0161
p_win_average: 0.1813, p_win_min: 0.0698, p_win_max: 0.2891, p_win_std: 0.0303
p_lose_average: 0.1813, p_lose_min: 0.0679, p_lose_max: 0.2617, p_lose_std: 0.0327

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.5720, accuracy: 0.6875
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5352, prediction_2_rate: 0.4648
true_1_rate: 0.7192, true_2_rate: 0.6548
log joint likelihood: tensor(-1244.3359375000) joint log likelihood: tensor(-1563.5937500000)
r_win_average: 0.5226, r_win_min: -2.3594, r_win_max: 2.5156, r_win_std: 0.8731
r_lose_average: -0.2787, r_lose_min: -2.6562, r_lose_max: 2.2500, r_lose_std: 0.9259
eta_win_average: -0.1096, eta_win_min: -0.3613, eta_win_max: 0.0228, eta_win_std: 0.0469
eta_lose_average: -0.0993, eta_lose_min: -0.2432, eta_lose_max: 0.0212, eta_lose_std: 0.0407
p_win_average: -0.0077, p_win_min: -0.1001, p_win_max: 0.2148, p_win_std: 0.0410
p_lose_average: 0.0008, p_lose_min: -0.1138, p_lose_max: 0.1147, p_lose_std: 0.0387

eval_z_samples_size: 1000
eval_loss: 0.5703, accuracy: 0.6855
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7077, true_2_rate: 0.6627
log joint likelihood: tensor(-1240.3359375000) joint log likelihood: tensor(-1562.5234375000)
r_win_average: 0.7649, r_win_min: -2.2500, r_win_max: 2.7969, r_win_std: 0.9003
r_lose_average: -0.0597, r_lose_min: -2.5156, r_lose_max: 2.4844, r_lose_std: 0.9586
eta_win_average: 0.1496, eta_win_min: 0.1167, eta_win_max: 0.2100, eta_win_std: 0.0111
eta_lose_average: 0.1457, eta_lose_min: 0.1113, eta_lose_max: 0.1816, eta_lose_std: 0.0101
p_win_average: -0.0252, p_win_min: -0.0630, p_win_max: 0.0084, p_win_std: 0.0103
p_lose_average: -0.0246, p_lose_min: -0.0542, p_lose_max: 0.0126, p_lose_std: 0.0097

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.5615, accuracy: 0.7051
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7192, true_2_rate: 0.6905
log joint likelihood: tensor(-1198.8437500000) joint log likelihood: tensor(-1534.6914062500)
r_win_average: 1.5007, r_win_min: -1.9766, r_win_max: 4.3125, r_win_std: 1.1616
r_lose_average: 0.4798, r_lose_min: -2.2188, r_lose_max: 3.7812, r_lose_std: 1.1545
eta_win_average: 0.2080, eta_win_min: 0.0669, eta_win_max: 0.3789, eta_win_std: 0.0406
eta_lose_average: 0.2102, eta_lose_min: 0.0967, eta_lose_max: 0.3125, eta_lose_std: 0.0380
p_win_average: 0.2839, p_win_min: -0.0249, p_win_max: 0.4375, p_win_std: 0.0642
p_lose_average: 0.3046, p_lose_min: 0.0728, p_lose_max: 0.4629, p_lose_std: 0.0602

eval_z_samples_size: 1000
eval_loss: 0.5605, accuracy: 0.7051
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7192, true_2_rate: 0.6905
log joint likelihood: tensor(-1196.5507812500) joint log likelihood: tensor(-1529.7968750000)
r_win_average: 1.3252, r_win_min: -2.2500, r_win_max: 4.1562, r_win_std: 1.1818
r_lose_average: 0.2839, r_lose_min: -2.5000, r_lose_max: 3.6406, r_lose_std: 1.1813
eta_win_average: 0.0912, eta_win_min: -0.0189, eta_win_max: 0.1396, eta_win_std: 0.0194
eta_lose_average: 0.0866, eta_lose_min: 0.0052, eta_lose_max: 0.1279, eta_lose_std: 0.0167
p_win_average: 0.2254, p_win_min: 0.1279, p_win_max: 0.2949, p_win_std: 0.0274
p_lose_average: 0.2327, p_lose_min: 0.1484, p_lose_max: 0.3027, p_lose_std: 0.0264

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.5510, accuracy: 0.7207
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.7077, true_2_rate: 0.7341
log joint likelihood: tensor(-1208.8593750000) joint log likelihood: tensor(-1502.2187500000)
r_win_average: -0.8740, r_win_min: -3.8594, r_win_max: 2.3906, r_win_std: 0.9502
r_lose_average: -1.7637, r_lose_min: -4.4062, r_lose_max: 0.8828, r_lose_std: 0.9871
eta_win_average: -0.2823, eta_win_min: -0.3965, eta_win_max: -0.0625, eta_win_std: 0.0408
eta_lose_average: -0.2778, eta_lose_min: -0.3691, eta_lose_max: -0.0625, eta_lose_std: 0.0362
p_win_average: -0.8606, p_win_min: -1.0547, p_win_max: -0.5391, p_win_std: 0.0744
p_lose_average: -0.8842, p_lose_min: -1.1641, p_lose_max: -0.6758, p_lose_std: 0.0665

eval_z_samples_size: 1000
eval_loss: 0.5527, accuracy: 0.7129
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.7154, true_2_rate: 0.7103
log joint likelihood: tensor(-1206.5117187500) joint log likelihood: tensor(-1504.8828125000)
r_win_average: 0.1196, r_win_min: -2.7344, r_win_max: 3.2656, r_win_std: 0.9322
r_lose_average: -0.7499, r_lose_min: -3.2188, r_lose_max: 1.8750, r_lose_std: 0.9665
eta_win_average: -0.0705, eta_win_min: -0.1191, eta_win_max: 0.0079, eta_win_std: 0.0160
eta_lose_average: -0.0733, eta_lose_min: -0.1079, eta_lose_max: -0.0161, eta_lose_std: 0.0135
p_win_average: -0.0799, p_win_min: -0.1611, p_win_max: -0.0225, p_win_std: 0.0223
p_lose_average: -0.0736, p_lose_min: -0.1436, p_lose_max: -0.0068, p_lose_std: 0.0203

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.5562, accuracy: 0.7031
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.7115, true_2_rate: 0.6944
log joint likelihood: tensor(-1179.9921875000) joint log likelihood: tensor(-1518.6015625000)
r_win_average: 0.4477, r_win_min: -3.8281, r_win_max: 3.6719, r_win_std: 1.1467
r_lose_average: -0.6985, r_lose_min: -4.8125, r_lose_max: 2.2344, r_lose_std: 1.3830
eta_win_average: -0.4055, eta_win_min: -0.4980, eta_win_max: -0.2559, eta_win_std: 0.0326
eta_lose_average: -0.4112, eta_lose_min: -0.5039, eta_lose_max: -0.2930, eta_lose_std: 0.0329
p_win_average: -0.1276, p_win_min: -0.2832, p_win_max: -0.0032, p_win_std: 0.0399
p_lose_average: -0.1199, p_lose_min: -0.2373, p_lose_max: 0.0085, p_lose_std: 0.0384

eval_z_samples_size: 1000
eval_loss: 0.5569, accuracy: 0.7012
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7077, true_2_rate: 0.6944
log joint likelihood: tensor(-1175.8789062500) joint log likelihood: tensor(-1527.4414062500)
r_win_average: 0.8170, r_win_min: -3.4375, r_win_max: 4.0000, r_win_std: 1.1389
r_lose_average: -0.3218, r_lose_min: -4.3438, r_lose_max: 2.5625, r_lose_std: 1.3659
eta_win_average: -0.1641, eta_win_min: -0.1826, eta_win_max: -0.1245, eta_win_std: 0.0072
eta_lose_average: -0.1635, eta_lose_min: -0.1836, eta_lose_max: -0.1318, eta_lose_std: 0.0067
p_win_average: 0.0002, p_win_min: -0.0564, p_win_max: 0.0618, p_win_std: 0.0200
p_lose_average: 0.0095, p_lose_min: -0.0603, p_lose_max: 0.0684, p_lose_std: 0.0209

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.5442, accuracy: 0.7070
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7231, true_2_rate: 0.6905
log joint likelihood: tensor(-1218.9804687500) joint log likelihood: tensor(-1490.5781250000)
r_win_average: -0.7498, r_win_min: -4.0000, r_win_max: 2.8125, r_win_std: 0.9956
r_lose_average: -1.7601, r_lose_min: -4.8125, r_lose_max: 1.0781, r_lose_std: 1.0993
eta_win_average: -0.8771, eta_win_min: -0.9727, eta_win_max: -0.6914, eta_win_std: 0.0333
eta_lose_average: -0.8762, eta_lose_min: -0.9531, eta_lose_max: -0.7344, eta_lose_std: 0.0314
p_win_average: -0.6130, p_win_min: -0.7578, p_win_max: -0.4082, p_win_std: 0.0495
p_lose_average: -0.6163, p_lose_min: -0.7773, p_lose_max: -0.4766, p_lose_std: 0.0434

eval_z_samples_size: 1000
eval_loss: 0.5466, accuracy: 0.6973
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5293, prediction_2_rate: 0.4707
true_1_rate: 0.7231, true_2_rate: 0.6706
log joint likelihood: tensor(-1216.9453125000) joint log likelihood: tensor(-1486.8164062500)
r_win_average: 0.9227, r_win_min: -2.2812, r_win_max: 4.3750, r_win_std: 0.9952
r_lose_average: -0.0808, r_lose_min: -3.1094, r_lose_max: 2.7188, r_lose_std: 1.0838
eta_win_average: 0.1366, eta_win_min: 0.0747, eta_win_max: 0.1719, eta_win_std: 0.0096
eta_lose_average: 0.1351, eta_lose_min: 0.0967, eta_lose_max: 0.1719, eta_lose_std: 0.0087
p_win_average: 0.0453, p_win_min: -0.0106, p_win_max: 0.0889, p_win_std: 0.0146
p_lose_average: 0.0519, p_lose_min: -0.0040, p_lose_max: 0.0981, p_lose_std: 0.0147

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 100
eval_loss: 0.5437, accuracy: 0.6992
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.7269, true_2_rate: 0.6706
log joint likelihood: tensor(-1203.3027343750) joint log likelihood: tensor(-1470.3183593750)
r_win_average: 0.0916, r_win_min: -3.8906, r_win_max: 3.8438, r_win_std: 1.1916
r_lose_average: -1.0470, r_lose_min: -4.4062, r_lose_max: 2.2656, r_lose_std: 1.2369
eta_win_average: -0.3333, eta_win_min: -0.4648, eta_win_max: -0.2373, eta_win_std: 0.0313
eta_lose_average: -0.3156, eta_lose_min: -0.4043, eta_lose_max: -0.2432, eta_lose_std: 0.0308
p_win_average: -0.0529, p_win_min: -0.2070, p_win_max: 0.0820, p_win_std: 0.0339
p_lose_average: -0.0427, p_lose_min: -0.1631, p_lose_max: 0.0771, p_lose_std: 0.0317

eval_z_samples_size: 1000
eval_loss: 0.5435, accuracy: 0.6992
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.7269, true_2_rate: 0.6706
log joint likelihood: tensor(-1198.9042968750) joint log likelihood: tensor(-1467.0214843750)
r_win_average: 0.6031, r_win_min: -3.4219, r_win_max: 4.3750, r_win_std: 1.2105
r_lose_average: -0.5533, r_lose_min: -3.9219, r_lose_max: 2.7500, r_lose_std: 1.2566
eta_win_average: 0.0511, eta_win_min: 0.0066, eta_win_max: 0.0874, eta_win_std: 0.0121
eta_lose_average: 0.0525, eta_lose_min: 0.0186, eta_lose_max: 0.0874, eta_lose_std: 0.0118
p_win_average: 0.0743, p_win_min: 0.0073, p_win_max: 0.1348, p_win_std: 0.0192
p_lose_average: 0.0832, p_lose_min: 0.0035, p_lose_max: 0.1377, p_lose_std: 0.0192

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 100
eval_loss: 0.5391, accuracy: 0.7090
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5293, prediction_2_rate: 0.4707
true_1_rate: 0.7346, true_2_rate: 0.6825
log joint likelihood: tensor(-1189.5937500000) joint log likelihood: tensor(-1448.9765625000)
r_win_average: 1.3114, r_win_min: -2.8906, r_win_max: 5.0000, r_win_std: 1.1117
r_lose_average: 0.2166, r_lose_min: -3.4531, r_lose_max: 3.4062, r_lose_std: 1.2560
eta_win_average: 0.9002, eta_win_min: 0.7812, eta_win_max: 0.9922, eta_win_std: 0.0324
eta_lose_average: 0.8933, eta_lose_min: 0.7812, eta_lose_max: 1.0547, eta_lose_std: 0.0318
p_win_average: 0.1283, p_win_min: 0.0287, p_win_max: 0.2451, p_win_std: 0.0323
p_lose_average: 0.1175, p_lose_min: 0.0156, p_lose_max: 0.1934, p_lose_std: 0.0306

eval_z_samples_size: 1000
eval_loss: 0.5383, accuracy: 0.7090
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7308, true_2_rate: 0.6865
log joint likelihood: tensor(-1190.2929687500) joint log likelihood: tensor(-1452.9296875000)
r_win_average: 0.1604, r_win_min: -3.9375, r_win_max: 3.6875, r_win_std: 1.0846
r_lose_average: -0.9066, r_lose_min: -4.4688, r_lose_max: 2.1875, r_lose_std: 1.2280
eta_win_average: -0.0563, eta_win_min: -0.0771, eta_win_max: -0.0153, eta_win_std: 0.0094
eta_lose_average: -0.0550, eta_lose_min: -0.0801, eta_lose_max: -0.0122, eta_lose_std: 0.0088
p_win_average: -0.0664, p_win_min: -0.1377, p_win_max: -0.0188, p_win_std: 0.0182
p_lose_average: -0.0573, p_lose_min: -0.1260, p_lose_max: -0.0096, p_lose_std: 0.0174

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 100
eval_loss: 0.5381, accuracy: 0.7168
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7308, true_2_rate: 0.7024
log joint likelihood: tensor(-1189.0019531250) joint log likelihood: tensor(-1464.4296875000)
r_win_average: 0.6508, r_win_min: -3.7344, r_win_max: 4.5000, r_win_std: 1.2181
r_lose_average: -0.5495, r_lose_min: -4.9688, r_lose_max: 2.6562, r_lose_std: 1.4290
eta_win_average: 0.6324, eta_win_min: 0.5273, eta_win_max: 0.8281, eta_win_std: 0.0405
eta_lose_average: 0.6238, eta_lose_min: 0.4805, eta_lose_max: 0.7656, eta_lose_std: 0.0383
p_win_average: -0.4687, p_win_min: -0.6758, p_win_max: -0.1494, p_win_std: 0.0570
p_lose_average: -0.4909, p_lose_min: -0.7617, p_lose_max: -0.2949, p_lose_std: 0.0593

eval_z_samples_size: 1000
eval_loss: 0.5408, accuracy: 0.7188
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7385, true_2_rate: 0.6984
log joint likelihood: tensor(-1179.8476562500) joint log likelihood: tensor(-1459.2519531250)
r_win_average: 0.6587, r_win_min: -3.4844, r_win_max: 4.2188, r_win_std: 1.1726
r_lose_average: -0.4916, r_lose_min: -4.6250, r_lose_max: 2.6406, r_lose_std: 1.3667
eta_win_average: -0.0108, eta_win_min: -0.0540, eta_win_max: 0.0209, eta_win_std: 0.0102
eta_lose_average: -0.0084, eta_lose_min: -0.0393, eta_lose_max: 0.0161, eta_lose_std: 0.0099
p_win_average: 0.1824, p_win_min: 0.0664, p_win_max: 0.2598, p_win_std: 0.0299
p_lose_average: 0.1991, p_lose_min: 0.1074, p_lose_max: 0.3047, p_lose_std: 0.0300

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.5388, accuracy: 0.7188
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.7231, true_2_rate: 0.7143
log joint likelihood: tensor(-1187.8007812500) joint log likelihood: tensor(-1462.1796875000)
r_win_average: 0.6906, r_win_min: -3.3438, r_win_max: 4.2500, r_win_std: 1.1527
r_lose_average: -0.4614, r_lose_min: -4.5938, r_lose_max: 2.4844, r_lose_std: 1.3453
eta_win_average: 0.2441, eta_win_min: 0.1377, eta_win_max: 0.3066, eta_win_std: 0.0222
eta_lose_average: 0.2508, eta_lose_min: 0.1797, eta_lose_max: 0.3164, eta_lose_std: 0.0202
p_win_average: -0.1296, p_win_min: -0.2383, p_win_max: -0.0156, p_win_std: 0.0277
p_lose_average: -0.1283, p_lose_min: -0.2275, p_lose_max: -0.0110, p_lose_std: 0.0281

eval_z_samples_size: 1000
eval_loss: 0.5376, accuracy: 0.7168
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7231, true_2_rate: 0.7103
log joint likelihood: tensor(-1180.4101562500) joint log likelihood: tensor(-1458.3183593750)
r_win_average: 0.5929, r_win_min: -3.4219, r_win_max: 4.2188, r_win_std: 1.1586
r_lose_average: -0.5592, r_lose_min: -4.6250, r_lose_max: 2.4844, r_lose_std: 1.3474
eta_win_average: -0.0930, eta_win_min: -0.1562, eta_win_max: -0.0544, eta_win_std: 0.0160
eta_lose_average: -0.0916, eta_lose_min: -0.1650, eta_lose_max: -0.0447, eta_lose_std: 0.0153
p_win_average: 0.1097, p_win_min: 0.0260, p_win_max: 0.1592, p_win_std: 0.0173
p_lose_average: 0.1160, p_lose_min: 0.0664, p_lose_max: 0.2139, p_lose_std: 0.0191

------------------------------------------------------------------------------------------
[2023-09-25 04:01:56,105] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:06,667] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:07,154] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:07,235] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:07,384] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:07,470] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:07,477] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:07,481] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:07,502] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 04:02:10,687] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:10,687] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,211] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:11,211] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,352] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:11,353] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,609] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:11,609] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,628] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:11,628] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,638] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:11,639] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,652] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:11,653] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,670] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 04:02:11,670] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 04:02:11,670] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/hh_new_final/anthropic_hh-ref_size10-enn_dim64-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 342
cuda seed 342
torch seed 142
cuda seed 142
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
torch seed 242
cuda seed 242
torch seed 542
cuda seed 542
torch seed 442
cuda seed 442
training dataset size: 2544
eval dataset size: 8
joint eval dataset size: 256
Total steps:  2544
Warmup steps:  76
[2023-09-25 04:02:45,378] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 04:02:47,732] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 04:02:47,733] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 04:02:47,734] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 04:02:47,739] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 04:02:47,739] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 04:02:47,739] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 04:02:47,740] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 04:02:47,740] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 04:02:47,741] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 04:02:47,741] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-25 04:03:00,666] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 04:03:00,667] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 04:03:00,668] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 45.27 GB, percent = 4.5%
[2023-09-25 04:03:00,781] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 04:03:00,781] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 04:03:00,782] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 45.27 GB, percent = 4.5%
[2023-09-25 04:03:00,783] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 04:03:00,887] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 04:03:00,887] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 04:03:00,888] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 45.27 GB, percent = 4.5%
[2023-09-25 04:03:00,890] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 04:03:00,890] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 04:03:00,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 04:03:00,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 04:03:00,893] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 04:03:00,893] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 04:03:00,894] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 04:03:00,894] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 04:03:00,894] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 04:03:00,895] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 04:03:00,896] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 04:03:00,896] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 04:03:00,897] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 04:03:00,897] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 04:03:00,897] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f16552a0690>
[2023-09-25 04:03:00,898] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 04:03:00,898] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 04:03:00,899] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 04:03:00,899] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 04:03:00,899] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 04:03:00,899] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 04:03:00,900] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 04:03:00,900] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 04:03:00,901] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 04:03:00,901] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 04:03:00,902] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 04:03:00,903] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 04:03:00,903] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 04:03:00,904] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 04:03:00,904] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 04:03:00,905] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 04:03:00,905] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 04:03:00,905] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 04:03:00,906] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 04:03:00,906] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 04:03:00,907] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 04:03:00,908] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 04:03:00,908] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 04:03:00,908] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 04:03:00,909] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 04:03:00,909] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 04:03:00,909] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 04:03:00,910] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 04:03:00,911] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 04:03:00,912] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 04:03:00,913] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 04:03:00,913] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 04:03:00,914] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 04:03:00,914] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 04:03:00,915] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 04:03:00,915] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 04:03:00,916] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 04:03:00,916] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 04:03:00,917] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 04:03:00,917] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 04:03:00,918] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 04:03:00,919] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 04:03:00,920] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 04:03:00,921] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 04:03:00,922] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 04:03:00,922] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 04:03:00,922] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 04:03:00,923] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 04:03:00,923] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 04:03:00,923] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 04:03:00,924] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 04:03:00,924] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 04:03:00,925] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 04:03:00,925] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 04:03:00,926] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 04:03:00,926] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 04:03:00,927] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 04:03:00,928] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 04:03:00,929] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 04:03:00,929] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 0.8320, accuracy: 0.5527
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.5538, true_2_rate: 0.5516
log joint likelihood: tensor(-730.0312500000) joint log likelihood: tensor(-4136.8593750000)
r_win_average: -3.1621, r_win_min: -5.8125, r_win_max: 2.8594, r_win_std: 1.3653
r_lose_average: -4.3405, r_lose_min: -6.7500, r_lose_max: 0.1318, r_lose_std: 1.0439
eta_win_average: -0.1926, eta_win_min: -1.0703, eta_win_max: 0.2598, eta_win_std: 0.1589
eta_lose_average: -0.2199, eta_lose_min: -1.1562, eta_lose_max: 0.2617, eta_lose_std: 0.1597
p_win_average: -0.4364, p_win_min: -1.0938, p_win_max: 0.3184, p_win_std: 0.1842
p_lose_average: -0.4467, p_lose_min: -0.8984, p_lose_max: 0.1001, p_lose_std: 0.1704

eval_z_samples_size: 1000
eval_loss: 0.8481, accuracy: 0.5371
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5462, true_2_rate: 0.5278
log joint likelihood: tensor(-707.7812500000) joint log likelihood: tensor(-4127.7890625000)
r_win_average: -2.3291, r_win_min: -4.8125, r_win_max: 3.8594, r_win_std: 1.3352
r_lose_average: -3.4947, r_lose_min: -5.9688, r_lose_max: 0.9727, r_lose_std: 1.0218
eta_win_average: 0.0309, eta_win_min: -0.2930, eta_win_max: 0.2793, eta_win_std: 0.0793
eta_lose_average: 0.0148, eta_lose_min: -0.1572, eta_lose_max: 0.2676, eta_lose_std: 0.0736
p_win_average: 0.1644, p_win_min: -0.1367, p_win_max: 0.3633, p_win_std: 0.0773
p_lose_average: 0.1734, p_lose_min: -0.1299, p_lose_max: 0.3691, p_lose_std: 0.0713

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.6284, accuracy: 0.6348
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.6500, true_2_rate: 0.6190
log joint likelihood: tensor(-1244.3437500000) joint log likelihood: tensor(-1830.7812500000)
r_win_average: 0.7142, r_win_min: -1.0938, r_win_max: 4.1250, r_win_std: 0.6598
r_lose_average: 0.1778, r_lose_min: -1.2969, r_lose_max: 1.5625, r_lose_std: 0.5087
eta_win_average: -0.2228, eta_win_min: -0.4785, eta_win_max: -0.0108, eta_win_std: 0.0469
eta_lose_average: -0.2359, eta_lose_min: -0.4707, eta_lose_max: -0.1162, eta_lose_std: 0.0434
p_win_average: -0.0897, p_win_min: -0.1680, p_win_max: 0.3359, p_win_std: 0.0431
p_lose_average: -0.0921, p_lose_min: -0.1592, p_lose_max: 0.0231, p_lose_std: 0.0265

eval_z_samples_size: 1000
eval_loss: 0.6313, accuracy: 0.6309
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.6462, true_2_rate: 0.6151
log joint likelihood: tensor(-1241.2109375000) joint log likelihood: tensor(-1836.0468750000)
r_win_average: 1.2561, r_win_min: -0.4102, r_win_max: 4.5625, r_win_std: 0.6388
r_lose_average: 0.7420, r_lose_min: -0.6484, r_lose_max: 2.0938, r_lose_std: 0.4975
eta_win_average: 0.0524, eta_win_min: -0.0009, eta_win_max: 0.1973, eta_win_std: 0.0191
eta_lose_average: 0.0526, eta_lose_min: 0.0066, eta_lose_max: 0.1631, eta_lose_std: 0.0152
p_win_average: 0.1763, p_win_min: -0.0133, p_win_max: 0.2656, p_win_std: 0.0353
p_lose_average: 0.1844, p_lose_min: 0.0801, p_lose_max: 0.2715, p_lose_std: 0.0295

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.6108, accuracy: 0.6504
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5332, prediction_2_rate: 0.4668
true_1_rate: 0.6808, true_2_rate: 0.6190
log joint likelihood: tensor(-1225.2968750000) joint log likelihood: tensor(-1708.4140625000)
r_win_average: 2.4079, r_win_min: -0.6719, r_win_max: 5.4375, r_win_std: 0.9267
r_lose_average: 1.6199, r_lose_min: -1.1797, r_lose_max: 4.2188, r_lose_std: 0.9226
eta_win_average: 0.5107, eta_win_min: 0.2910, eta_win_max: 0.6367, eta_win_std: 0.0407
eta_lose_average: 0.5112, eta_lose_min: 0.3633, eta_lose_max: 0.6328, eta_lose_std: 0.0395
p_win_average: 0.5266, p_win_min: 0.3359, p_win_max: 0.6797, p_win_std: 0.0482
p_lose_average: 0.5321, p_lose_min: 0.3457, p_lose_max: 0.6641, p_lose_std: 0.0528

eval_z_samples_size: 1000
eval_loss: 0.6055, accuracy: 0.6543
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5371, prediction_2_rate: 0.4629
true_1_rate: 0.6885, true_2_rate: 0.6190
log joint likelihood: tensor(-1223.4843750000) joint log likelihood: tensor(-1715.2578125000)
r_win_average: 1.2633, r_win_min: -1.8672, r_win_max: 4.2812, r_win_std: 0.9237
r_lose_average: 0.4721, r_lose_min: -2.2344, r_lose_max: 3.0938, r_lose_std: 0.9294
eta_win_average: -0.0844, eta_win_min: -0.1167, eta_win_max: -0.0581, eta_win_std: 0.0086
eta_lose_average: -0.0832, eta_lose_min: -0.1147, eta_lose_max: -0.0486, eta_lose_std: 0.0090
p_win_average: -0.0241, p_win_min: -0.0718, p_win_max: 0.0144, p_win_std: 0.0150
p_lose_average: -0.0206, p_lose_min: -0.0762, p_lose_max: 0.0245, p_lose_std: 0.0160

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.5669, accuracy: 0.6934
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7115, true_2_rate: 0.6746
log joint likelihood: tensor(-1228.1562500000) joint log likelihood: tensor(-1569.0703125000)
r_win_average: 0.8855, r_win_min: -2.2969, r_win_max: 2.9062, r_win_std: 0.8897
r_lose_average: 0.0583, r_lose_min: -2.4844, r_lose_max: 2.6250, r_lose_std: 0.9524
eta_win_average: 0.3585, eta_win_min: 0.2539, eta_win_max: 0.5078, eta_win_std: 0.0325
eta_lose_average: 0.3626, eta_lose_min: 0.2480, eta_lose_max: 0.5078, eta_lose_std: 0.0290
p_win_average: -0.1667, p_win_min: -0.2676, p_win_max: 0.0369, p_win_std: 0.0436
p_lose_average: -0.1810, p_lose_min: -0.2793, p_lose_max: -0.0195, p_lose_std: 0.0364

eval_z_samples_size: 1000
eval_loss: 0.5713, accuracy: 0.6895
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7115, true_2_rate: 0.6667
log joint likelihood: tensor(-1225.4843750000) joint log likelihood: tensor(-1566.9609375000)
r_win_average: 1.1084, r_win_min: -1.9453, r_win_max: 3.0938, r_win_std: 0.8765
r_lose_average: 0.2985, r_lose_min: -2.2031, r_lose_max: 2.8125, r_lose_std: 0.9388
eta_win_average: 0.2142, eta_win_min: 0.1436, eta_win_max: 0.2754, eta_win_std: 0.0170
eta_lose_average: 0.2169, eta_lose_min: 0.1641, eta_lose_max: 0.2656, eta_lose_std: 0.0160
p_win_average: 0.1999, p_win_min: 0.1167, p_win_max: 0.2539, p_win_std: 0.0225
p_lose_average: 0.2057, p_lose_min: 0.1182, p_lose_max: 0.2637, p_lose_std: 0.0204

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.5544, accuracy: 0.7129
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5332, prediction_2_rate: 0.4668
true_1_rate: 0.7423, true_2_rate: 0.6825
log joint likelihood: tensor(-1185.2187500000) joint log likelihood: tensor(-1543.2734375000)
r_win_average: 0.2111, r_win_min: -3.3594, r_win_max: 3.2344, r_win_std: 1.1966
r_lose_average: -0.8644, r_lose_min: -3.7188, r_lose_max: 2.7969, r_lose_std: 1.2132
eta_win_average: -0.4266, eta_win_min: -0.5117, eta_win_max: -0.2715, eta_win_std: 0.0287
eta_lose_average: -0.4358, eta_lose_min: -0.5078, eta_lose_max: -0.3594, eta_lose_std: 0.0243
p_win_average: -0.3710, p_win_min: -0.5469, p_win_max: -0.1553, p_win_std: 0.0603
p_lose_average: -0.3730, p_lose_min: -0.5078, p_lose_max: -0.1895, p_lose_std: 0.0538

eval_z_samples_size: 1000
eval_loss: 0.5569, accuracy: 0.7148
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7346, true_2_rate: 0.6944
log joint likelihood: tensor(-1178.9570312500) joint log likelihood: tensor(-1543.2109375000)
r_win_average: 0.9201, r_win_min: -2.6250, r_win_max: 3.8594, r_win_std: 1.1977
r_lose_average: -0.1437, r_lose_min: -2.9688, r_lose_max: 3.3750, r_lose_std: 1.2026
eta_win_average: 0.0530, eta_win_min: 0.0236, eta_win_max: 0.0933, eta_win_std: 0.0100
eta_lose_average: 0.0549, eta_lose_min: 0.0304, eta_lose_max: 0.0820, eta_lose_std: 0.0091
p_win_average: -0.1421, p_win_min: -0.1895, p_win_max: -0.0437, p_win_std: 0.0172
p_lose_average: -0.1425, p_lose_min: -0.1875, p_lose_max: -0.0791, p_lose_std: 0.0143

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.5547, accuracy: 0.7051
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.7077, true_2_rate: 0.7024
log joint likelihood: tensor(-1201.9375000000) joint log likelihood: tensor(-1506.0234375000)
r_win_average: 0.8503, r_win_min: -1.9375, r_win_max: 3.7656, r_win_std: 0.9073
r_lose_average: 0.0106, r_lose_min: -2.2812, r_lose_max: 2.5625, r_lose_std: 0.9327
eta_win_average: 0.1585, eta_win_min: -0.0342, eta_win_max: 0.3672, eta_win_std: 0.0447
eta_lose_average: 0.1687, eta_lose_min: -0.0581, eta_lose_max: 0.2734, eta_lose_std: 0.0401
p_win_average: 0.6381, p_win_min: 0.3418, p_win_max: 0.8203, p_win_std: 0.0765
p_lose_average: 0.6579, p_lose_min: 0.4062, p_lose_max: 0.8516, p_lose_std: 0.0639

eval_z_samples_size: 1000
eval_loss: 0.5515, accuracy: 0.7129
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7192, true_2_rate: 0.7063
log joint likelihood: tensor(-1202.2109375000) joint log likelihood: tensor(-1511.0703125000)
r_win_average: -0.1042, r_win_min: -3.0469, r_win_max: 3.0469, r_win_std: 0.9340
r_lose_average: -0.9743, r_lose_min: -3.2969, r_lose_max: 1.5781, r_lose_std: 0.9599
eta_win_average: 0.0333, eta_win_min: -0.0073, eta_win_max: 0.0605, eta_win_std: 0.0089
eta_lose_average: 0.0345, eta_lose_min: 0.0079, eta_lose_max: 0.0618, eta_lose_std: 0.0080
p_win_average: -0.1920, p_win_min: -0.2500, p_win_max: -0.1104, p_win_std: 0.0151
p_lose_average: -0.1919, p_lose_min: -0.2598, p_lose_max: -0.1357, p_lose_std: 0.0142

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.5532, accuracy: 0.7012
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7077, true_2_rate: 0.6944
log joint likelihood: tensor(-1176.6113281250) joint log likelihood: tensor(-1523.5468750000)
r_win_average: 0.0802, r_win_min: -4.7500, r_win_max: 3.5469, r_win_std: 1.1974
r_lose_average: -1.1140, r_lose_min: -5.5625, r_lose_max: 1.8906, r_lose_std: 1.4597
eta_win_average: -0.3485, eta_win_min: -0.5547, eta_win_max: -0.0713, eta_win_std: 0.0602
eta_lose_average: -0.3808, eta_lose_min: -0.6055, eta_lose_max: -0.2031, eta_lose_std: 0.0607
p_win_average: -0.4457, p_win_min: -0.6523, p_win_max: -0.1650, p_win_std: 0.0563
p_lose_average: -0.4616, p_lose_min: -0.7109, p_lose_max: -0.1836, p_lose_std: 0.0632

eval_z_samples_size: 1000
eval_loss: 0.5566, accuracy: 0.7012
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7154, true_2_rate: 0.6865
log joint likelihood: tensor(-1173.2968750000) joint log likelihood: tensor(-1529.7382812500)
r_win_average: 0.8658, r_win_min: -3.5781, r_win_max: 3.9688, r_win_std: 1.1298
r_lose_average: -0.2642, r_lose_min: -4.3750, r_lose_max: 2.6250, r_lose_std: 1.3742
eta_win_average: -0.1428, eta_win_min: -0.1768, eta_win_max: -0.1084, eta_win_std: 0.0099
eta_lose_average: -0.1428, eta_lose_min: -0.1748, eta_lose_max: -0.1152, eta_lose_std: 0.0100
p_win_average: 0.1333, p_win_min: 0.0233, p_win_max: 0.2354, p_win_std: 0.0333
p_lose_average: 0.1514, p_lose_min: 0.0508, p_lose_max: 0.2402, p_lose_std: 0.0339

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.5396, accuracy: 0.6992
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7154, true_2_rate: 0.6825
log joint likelihood: tensor(-1213.0312500000) joint log likelihood: tensor(-1487.2031250000)
r_win_average: 0.1885, r_win_min: -3.3750, r_win_max: 3.8906, r_win_std: 1.0456
r_lose_average: -0.8640, r_lose_min: -4.0938, r_lose_max: 1.9766, r_lose_std: 1.1502
eta_win_average: 0.1070, eta_win_min: 0.0143, eta_win_max: 0.2520, eta_win_std: 0.0296
eta_lose_average: 0.0941, eta_lose_min: -0.0243, eta_lose_max: 0.2080, eta_lose_std: 0.0284
p_win_average: -0.5549, p_win_min: -0.7500, p_win_max: -0.1748, p_win_std: 0.0740
p_lose_average: -0.5875, p_lose_min: -0.7656, p_lose_max: -0.3418, p_lose_std: 0.0654

eval_z_samples_size: 1000
eval_loss: 0.5452, accuracy: 0.6953
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5352, prediction_2_rate: 0.4648
true_1_rate: 0.7269, true_2_rate: 0.6627
log joint likelihood: tensor(-1208.7890625000) joint log likelihood: tensor(-1483.5976562500)
r_win_average: 0.8013, r_win_min: -2.4688, r_win_max: 4.2500, r_win_std: 0.9891
r_lose_average: -0.1906, r_lose_min: -3.2031, r_lose_max: 2.5938, r_lose_std: 1.0863
eta_win_average: -0.0789, eta_win_min: -0.1182, eta_win_max: -0.0508, eta_win_std: 0.0092
eta_lose_average: -0.0777, eta_lose_min: -0.1157, eta_lose_max: -0.0464, eta_lose_std: 0.0088
p_win_average: 0.2426, p_win_min: 0.1162, p_win_max: 0.3184, p_win_std: 0.0331
p_lose_average: 0.2589, p_lose_min: 0.1650, p_lose_max: 0.3691, p_lose_std: 0.0323

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 100
eval_loss: 0.5405, accuracy: 0.6992
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.7231, true_2_rate: 0.6746
log joint likelihood: tensor(-1200.1308593750) joint log likelihood: tensor(-1463.9648437500)
r_win_average: -0.2125, r_win_min: -4.3125, r_win_max: 3.7344, r_win_std: 1.2474
r_lose_average: -1.3928, r_lose_min: -4.7812, r_lose_max: 1.8438, r_lose_std: 1.2861
eta_win_average: -0.3592, eta_win_min: -0.5117, eta_win_max: -0.1748, eta_win_std: 0.0373
eta_lose_average: -0.3619, eta_lose_min: -0.4805, eta_lose_max: -0.2383, eta_lose_std: 0.0347
p_win_average: -0.2195, p_win_min: -0.3496, p_win_max: -0.0131, p_win_std: 0.0430
p_lose_average: -0.2310, p_lose_min: -0.3516, p_lose_max: 0.0032, p_lose_std: 0.0407

eval_z_samples_size: 1000
eval_loss: 0.5403, accuracy: 0.6953
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.7231, true_2_rate: 0.6667
log joint likelihood: tensor(-1196.9082031250) joint log likelihood: tensor(-1463.6113281250)
r_win_average: 0.6565, r_win_min: -3.3750, r_win_max: 4.4688, r_win_std: 1.2174
r_lose_average: -0.4999, r_lose_min: -3.8281, r_lose_max: 2.6406, r_lose_std: 1.2649
eta_win_average: 0.2257, eta_win_min: 0.1562, eta_win_max: 0.2773, eta_win_std: 0.0139
eta_lose_average: 0.2304, eta_lose_min: 0.1953, eta_lose_max: 0.2715, eta_lose_std: 0.0119
p_win_average: 0.0648, p_win_min: 0.0064, p_win_max: 0.1108, p_win_std: 0.0144
p_lose_average: 0.0698, p_lose_min: 0.0315, p_lose_max: 0.1045, p_lose_std: 0.0140

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 100
eval_loss: 0.5369, accuracy: 0.7012
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5293, prediction_2_rate: 0.4707
true_1_rate: 0.7269, true_2_rate: 0.6746
log joint likelihood: tensor(-1193.9472656250) joint log likelihood: tensor(-1452.3691406250)
r_win_average: -0.7738, r_win_min: -4.6562, r_win_max: 2.7344, r_win_std: 1.0789
r_lose_average: -1.8463, r_lose_min: -5.4062, r_lose_max: 1.0000, r_lose_std: 1.2222
eta_win_average: -0.3088, eta_win_min: -0.4746, eta_win_max: -0.1582, eta_win_std: 0.0482
eta_lose_average: -0.3030, eta_lose_min: -0.4824, eta_lose_max: -0.1328, eta_lose_std: 0.0487
p_win_average: -0.6893, p_win_min: -0.8477, p_win_max: -0.5117, p_win_std: 0.0484
p_lose_average: -0.7002, p_lose_min: -0.8672, p_lose_max: -0.5625, p_lose_std: 0.0470

eval_z_samples_size: 1000
eval_loss: 0.5378, accuracy: 0.7012
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5293, prediction_2_rate: 0.4707
true_1_rate: 0.7269, true_2_rate: 0.6746
log joint likelihood: tensor(-1196.2500000000) joint log likelihood: tensor(-1455.3964843750)
r_win_average: 0.2750, r_win_min: -3.6406, r_win_max: 3.7500, r_win_std: 1.0658
r_lose_average: -0.7792, r_lose_min: -4.4062, r_lose_max: 2.1094, r_lose_std: 1.2126
eta_win_average: 0.0600, eta_win_min: 0.0011, eta_win_max: 0.0938, eta_win_std: 0.0126
eta_lose_average: 0.0654, eta_lose_min: 0.0130, eta_lose_max: 0.1060, eta_lose_std: 0.0118
p_win_average: -0.0097, p_win_min: -0.0869, p_win_max: 0.0466, p_win_std: 0.0192
p_lose_average: -0.0012, p_lose_min: -0.0713, p_lose_max: 0.0491, p_lose_std: 0.0184

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 100
eval_loss: 0.5444, accuracy: 0.7070
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.7308, true_2_rate: 0.6825
log joint likelihood: tensor(-1190.9238281250) joint log likelihood: tensor(-1466.6914062500)
r_win_average: 1.2430, r_win_min: -2.7500, r_win_max: 4.7500, r_win_std: 1.1244
r_lose_average: 0.1309, r_lose_min: -4.0312, r_lose_max: 2.9219, r_lose_std: 1.3130
eta_win_average: 0.1637, eta_win_min: -0.0918, eta_win_max: 0.3379, eta_win_std: 0.0495
eta_lose_average: 0.1877, eta_lose_min: 0.0593, eta_lose_max: 0.3477, eta_lose_std: 0.0471
p_win_average: 0.6376, p_win_min: 0.2480, p_win_max: 0.8711, p_win_std: 0.0849
p_lose_average: 0.6697, p_lose_min: 0.3984, p_lose_max: 0.9648, p_lose_std: 0.0803

eval_z_samples_size: 1000
eval_loss: 0.5408, accuracy: 0.7148
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7346, true_2_rate: 0.6944
log joint likelihood: tensor(-1180.9863281250) joint log likelihood: tensor(-1463.3671875000)
r_win_average: 0.4389, r_win_min: -3.7188, r_win_max: 4.1250, r_win_std: 1.1698
r_lose_average: -0.7177, r_lose_min: -5.0938, r_lose_max: 2.2656, r_lose_std: 1.3709
eta_win_average: -0.0411, eta_win_min: -0.0913, eta_win_max: -0.0070, eta_win_std: 0.0126
eta_lose_average: -0.0368, eta_lose_min: -0.0977, eta_lose_max: 0.0125, eta_lose_std: 0.0128
p_win_average: 0.0373, p_win_min: -0.0732, p_win_max: 0.1240, p_win_std: 0.0244
p_lose_average: 0.0465, p_lose_min: -0.0474, p_lose_max: 0.1050, p_lose_std: 0.0224

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.5371, accuracy: 0.7148
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7346, true_2_rate: 0.6944
log joint likelihood: tensor(-1184.6132812500) joint log likelihood: tensor(-1466.6093750000)
r_win_average: 0.0570, r_win_min: -4.2188, r_win_max: 3.7656, r_win_std: 1.1693
r_lose_average: -1.1172, r_lose_min: -5.5312, r_lose_max: 1.8594, r_lose_std: 1.3735
eta_win_average: -0.1930, eta_win_min: -0.3477, eta_win_max: -0.1064, eta_win_std: 0.0318
eta_lose_average: -0.1888, eta_lose_min: -0.2949, eta_lose_max: -0.0752, eta_lose_std: 0.0334
p_win_average: -0.2428, p_win_min: -0.3477, p_win_max: -0.1582, p_win_std: 0.0270
p_lose_average: -0.2402, p_lose_min: -0.3242, p_lose_max: -0.1260, p_lose_std: 0.0252

eval_z_samples_size: 1000
eval_loss: 0.5386, accuracy: 0.7188
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7385, true_2_rate: 0.6984
log joint likelihood: tensor(-1176.9511718750) joint log likelihood: tensor(-1463.4433593750)
r_win_average: 0.4945, r_win_min: -3.6875, r_win_max: 4.1875, r_win_std: 1.1649
r_lose_average: -0.6709, r_lose_min: -5.0312, r_lose_max: 2.2812, r_lose_std: 1.3673
eta_win_average: -0.0130, eta_win_min: -0.0571, eta_win_max: 0.0284, eta_win_std: 0.0121
eta_lose_average: -0.0075, eta_lose_min: -0.0469, eta_lose_max: 0.0374, eta_lose_std: 0.0114
p_win_average: 0.0149, p_win_min: -0.0796, p_win_max: 0.0762, p_win_std: 0.0193
p_lose_average: 0.0248, p_lose_min: -0.0649, p_lose_max: 0.0830, p_lose_std: 0.0198

------------------------------------------------------------------------------------------
[2023-09-25 06:03:53,572] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:04,149] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:04,719] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:04,919] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:05,066] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:05,084] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:05,096] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:05,107] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:05,118] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 06:04:08,250] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:08,250] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 06:04:08,920] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:08,920] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 06:04:09,152] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:09,152] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 06:04:09,171] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:09,172] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 06:04:09,254] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:09,254] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 06:04:09,255] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 06:04:09,296] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:09,297] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 06:04:09,297] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:09,297] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 06:04:09,407] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 06:04:09,407] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 242
cuda seed 242
torch seed 442
cuda seed 442
torch seed 642
cuda seed 642
torch seed 342
cuda seed 342
torch seed 142
cuda seed 142
torch seed 542
cuda seed 542
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/hh_new_final/anthropic_hh-ref_size10-enn_dim128-num_ref_train10-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 742
cuda seed 742
training dataset size: 2544
eval dataset size: 8
joint eval dataset size: 256
Total steps:  2544
Warmup steps:  76
[2023-09-25 06:04:43,081] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 06:04:45,310] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 06:04:45,311] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 06:04:45,311] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 06:04:45,316] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 06:04:45,316] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 06:04:45,316] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 06:04:45,316] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 06:04:45,316] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 06:04:45,316] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 06:04:45,316] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-25 06:04:58,401] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 06:04:58,402] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 06:04:58,402] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.91 GB, percent = 4.5%
[2023-09-25 06:04:58,509] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 06:04:58,510] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 06:04:58,510] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.91 GB, percent = 4.5%
[2023-09-25 06:04:58,510] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 06:04:58,607] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 06:04:58,608] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 06:04:58,608] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.91 GB, percent = 4.5%
[2023-09-25 06:04:58,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 06:04:58,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 06:04:58,609] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 06:04:58,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 06:04:58,610] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe071686190>
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 06:04:58,610] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 06:04:58,611] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 06:04:58,611] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 0.8950, accuracy: 0.5117
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.4727, prediction_2_rate: 0.5273
true_1_rate: 0.4846, true_2_rate: 0.5397
log joint likelihood: tensor(-704.2656250000) joint log likelihood: tensor(-4410.2968750000)
r_win_average: -3.5955, r_win_min: -6.5938, r_win_max: 3.0781, r_win_std: 1.3612
r_lose_average: -4.8513, r_lose_min: -7.9375, r_lose_max: 0.1729, r_lose_std: 1.0958
eta_win_average: -0.3047, eta_win_min: -1.2109, eta_win_max: 0.5312, eta_win_std: 0.2485
eta_lose_average: -0.3696, eta_lose_min: -1.0938, eta_lose_max: 0.5195, eta_lose_std: 0.2485
p_win_average: -0.7265, p_win_min: -2.2031, p_win_max: 0.2578, p_win_std: 0.3656
p_lose_average: -0.8393, p_lose_min: -2.3125, p_lose_max: 0.2578, p_lose_std: 0.3368

eval_z_samples_size: 1000
eval_loss: 0.8394, accuracy: 0.5410
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.5462, true_2_rate: 0.5357
log joint likelihood: tensor(-692.1289062500) joint log likelihood: tensor(-4425.2968750000)
r_win_average: -2.4017, r_win_min: -4.8750, r_win_max: 3.9219, r_win_std: 1.3531
r_lose_average: -3.5684, r_lose_min: -5.9688, r_lose_max: 1.1641, r_lose_std: 1.0175
eta_win_average: -0.1005, eta_win_min: -0.2695, eta_win_max: 0.1260, eta_win_std: 0.0540
eta_lose_average: -0.1052, eta_lose_min: -0.2412, eta_lose_max: 0.0610, eta_lose_std: 0.0505
p_win_average: 0.2238, p_win_min: 0.0168, p_win_max: 0.4609, p_win_std: 0.0710
p_lose_average: 0.2191, p_lose_min: 0.0168, p_lose_max: 0.4688, p_lose_std: 0.0624

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.6440, accuracy: 0.6230
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.6308, true_2_rate: 0.6151
log joint likelihood: tensor(-1281.4921875000) joint log likelihood: tensor(-1859.9843750000)
r_win_average: 0.9943, r_win_min: -0.3652, r_win_max: 4.7500, r_win_std: 0.6097
r_lose_average: 0.4947, r_lose_min: -1.1797, r_lose_max: 2.0469, r_lose_std: 0.4481
eta_win_average: -0.4317, eta_win_min: -0.8555, eta_win_max: -0.0879, eta_win_std: 0.0720
eta_lose_average: -0.4415, eta_lose_min: -0.8398, eta_lose_max: -0.2109, eta_lose_std: 0.0570
p_win_average: -0.2503, p_win_min: -0.8711, p_win_max: -0.0248, p_win_std: 0.0544
p_lose_average: -0.2534, p_lose_min: -0.5742, p_lose_max: -0.1021, p_lose_std: 0.0421

eval_z_samples_size: 1000
eval_loss: 0.6436, accuracy: 0.6309
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.6269, true_2_rate: 0.6349
log joint likelihood: tensor(-1274.9062500000) joint log likelihood: tensor(-1864.0625000000)
r_win_average: 1.5255, r_win_min: 0.1768, r_win_max: 5.2500, r_win_std: 0.6086
r_lose_average: 1.0285, r_lose_min: -0.6875, r_lose_max: 2.4844, r_lose_std: 0.4315
eta_win_average: -0.0757, eta_win_min: -0.1689, eta_win_max: 0.0962, eta_win_std: 0.0243
eta_lose_average: -0.0802, eta_lose_min: -0.1426, eta_lose_max: 0.0544, eta_lose_std: 0.0181
p_win_average: -0.0768, p_win_min: -0.1953, p_win_max: 0.0012, p_win_std: 0.0207
p_lose_average: -0.0793, p_lose_min: -0.1641, p_lose_max: 0.0126, p_lose_std: 0.0181

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.6123, accuracy: 0.6406
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.6385, true_2_rate: 0.6429
log joint likelihood: tensor(-1257.1093750000) joint log likelihood: tensor(-1682.3750000000)
r_win_average: 1.4852, r_win_min: -0.9219, r_win_max: 4.1562, r_win_std: 0.8237
r_lose_average: 0.7938, r_lose_min: -1.1484, r_lose_max: 3.0625, r_lose_std: 0.7852
eta_win_average: -0.2881, eta_win_min: -0.4297, eta_win_max: -0.1680, eta_win_std: 0.0398
eta_lose_average: -0.2996, eta_lose_min: -0.4824, eta_lose_max: -0.1641, eta_lose_std: 0.0386
p_win_average: 0.1664, p_win_min: 0.0698, p_win_max: 0.3457, p_win_std: 0.0409
p_lose_average: 0.1614, p_lose_min: 0.0425, p_lose_max: 0.3105, p_lose_std: 0.0368

eval_z_samples_size: 1000
eval_loss: 0.6089, accuracy: 0.6504
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.6538, true_2_rate: 0.6468
log joint likelihood: tensor(-1252.5937500000) joint log likelihood: tensor(-1688.6562500000)
r_win_average: 1.3734, r_win_min: -1.0469, r_win_max: 3.9531, r_win_std: 0.7936
r_lose_average: 0.7000, r_lose_min: -1.2109, r_lose_max: 2.9375, r_lose_std: 0.7623
eta_win_average: -0.0385, eta_win_min: -0.0967, eta_win_max: 0.0135, eta_win_std: 0.0150
eta_lose_average: -0.0381, eta_lose_min: -0.1045, eta_lose_max: 0.0045, eta_lose_std: 0.0138
p_win_average: -0.1961, p_win_min: -0.2695, p_win_max: -0.0806, p_win_std: 0.0234
p_lose_average: -0.1932, p_lose_min: -0.3027, p_lose_max: -0.1221, p_lose_std: 0.0222

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.5806, accuracy: 0.6719
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.6846, true_2_rate: 0.6587
log joint likelihood: tensor(-1213.0781250000) joint log likelihood: tensor(-1594.3828125000)
r_win_average: 1.9152, r_win_min: -0.9688, r_win_max: 4.0312, r_win_std: 0.9152
r_lose_average: 1.0858, r_lose_min: -1.2344, r_lose_max: 3.9844, r_lose_std: 0.9377
eta_win_average: 0.2161, eta_win_min: 0.0889, eta_win_max: 0.3848, eta_win_std: 0.0418
eta_lose_average: 0.2151, eta_lose_min: 0.0659, eta_lose_max: 0.3594, eta_lose_std: 0.0389
p_win_average: 0.2530, p_win_min: 0.0986, p_win_max: 0.3750, p_win_std: 0.0447
p_lose_average: 0.2378, p_lose_min: 0.0972, p_lose_max: 0.3926, p_lose_std: 0.0420

eval_z_samples_size: 1000
eval_loss: 0.5793, accuracy: 0.6738
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.6923, true_2_rate: 0.6548
log joint likelihood: tensor(-1213.1875000000) joint log likelihood: tensor(-1599.8750000000)
r_win_average: 1.5501, r_win_min: -1.3281, r_win_max: 3.6406, r_win_std: 0.8872
r_lose_average: 0.7434, r_lose_min: -1.6094, r_lose_max: 3.4531, r_lose_std: 0.9203
eta_win_average: -0.2344, eta_win_min: -0.3027, eta_win_max: -0.1777, eta_win_std: 0.0166
eta_lose_average: -0.2296, eta_lose_min: -0.2988, eta_lose_max: -0.1768, eta_lose_std: 0.0160
p_win_average: 0.3372, p_win_min: 0.2676, p_win_max: 0.4316, p_win_std: 0.0237
p_lose_average: 0.3407, p_lose_min: 0.2793, p_lose_max: 0.4277, p_lose_std: 0.0208

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.5693, accuracy: 0.6875
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.7115, true_2_rate: 0.6627
log joint likelihood: tensor(-1148.1054687500) joint log likelihood: tensor(-1564.6367187500)
r_win_average: 1.6887, r_win_min: -1.7422, r_win_max: 4.7812, r_win_std: 1.2932
r_lose_average: 0.5899, r_lose_min: -2.1562, r_lose_max: 4.4062, r_lose_std: 1.2623
eta_win_average: 0.1781, eta_win_min: 0.0007, eta_win_max: 0.3008, eta_win_std: 0.0497
eta_lose_average: 0.1807, eta_lose_min: 0.0042, eta_lose_max: 0.3379, eta_lose_std: 0.0462
p_win_average: -0.1704, p_win_min: -0.3594, p_win_max: 0.0486, p_win_std: 0.0506
p_lose_average: -0.1785, p_lose_min: -0.3652, p_lose_max: -0.0304, p_lose_std: 0.0519

eval_z_samples_size: 1000
eval_loss: 0.5630, accuracy: 0.6914
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5352, prediction_2_rate: 0.4648
true_1_rate: 0.7231, true_2_rate: 0.6587
log joint likelihood: tensor(-1143.6171875000) joint log likelihood: tensor(-1567.8867187500)
r_win_average: 1.6909, r_win_min: -1.6797, r_win_max: 4.7812, r_win_std: 1.2784
r_lose_average: 0.5905, r_lose_min: -2.2188, r_lose_max: 4.3750, r_lose_std: 1.2576
eta_win_average: -0.1880, eta_win_min: -0.2266, eta_win_max: -0.1309, eta_win_std: 0.0123
eta_lose_average: -0.1864, eta_lose_min: -0.2197, eta_lose_max: -0.1406, eta_lose_std: 0.0105
p_win_average: 0.1955, p_win_min: 0.1465, p_win_max: 0.2578, p_win_std: 0.0184
p_lose_average: 0.1916, p_lose_min: 0.1426, p_lose_max: 0.2559, p_lose_std: 0.0174

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.5574, accuracy: 0.7148
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7269, true_2_rate: 0.7024
log joint likelihood: tensor(-1191.1132812500) joint log likelihood: tensor(-1528.1796875000)
r_win_average: 1.7253, r_win_min: -1.0938, r_win_max: 4.9375, r_win_std: 0.9472
r_lose_average: 0.8629, r_lose_min: -1.6719, r_lose_max: 3.4531, r_lose_std: 0.9416
eta_win_average: 0.2097, eta_win_min: -0.0737, eta_win_max: 0.5938, eta_win_std: 0.0816
eta_lose_average: 0.2221, eta_lose_min: -0.0688, eta_lose_max: 0.3965, eta_lose_std: 0.0700
p_win_average: 0.6768, p_win_min: 0.4551, p_win_max: 0.9375, p_win_std: 0.0498
p_lose_average: 0.6691, p_lose_min: 0.5000, p_lose_max: 0.7891, p_lose_std: 0.0448

eval_z_samples_size: 1000
eval_loss: 0.5579, accuracy: 0.7109
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.7192, true_2_rate: 0.7024
log joint likelihood: tensor(-1185.8515625000) joint log likelihood: tensor(-1530.0156250000)
r_win_average: 0.6985, r_win_min: -2.3438, r_win_max: 4.1250, r_win_std: 0.9678
r_lose_average: -0.1764, r_lose_min: -2.8438, r_lose_max: 2.4062, r_lose_std: 0.9615
eta_win_average: -0.0201, eta_win_min: -0.0442, eta_win_max: 0.0168, eta_win_std: 0.0083
eta_lose_average: -0.0222, eta_lose_min: -0.0464, eta_lose_max: 0.0021, eta_lose_std: 0.0079
p_win_average: -0.1214, p_win_min: -0.1719, p_win_max: -0.0500, p_win_std: 0.0189
p_lose_average: -0.1246, p_lose_min: -0.1982, p_lose_max: -0.0569, p_lose_std: 0.0180

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.5583, accuracy: 0.7070
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.7077, true_2_rate: 0.7063
log joint likelihood: tensor(-1141.9179687500) joint log likelihood: tensor(-1537.3964843750)
r_win_average: 1.7683, r_win_min: -2.9688, r_win_max: 5.8438, r_win_std: 1.3282
r_lose_average: 0.5289, r_lose_min: -3.7500, r_lose_max: 4.0312, r_lose_std: 1.4809
eta_win_average: 0.5506, eta_win_min: 0.3516, eta_win_max: 0.9180, eta_win_std: 0.0647
eta_lose_average: 0.5316, eta_lose_min: 0.3301, eta_lose_max: 0.7773, eta_lose_std: 0.0569
p_win_average: -0.4984, p_win_min: -0.8438, p_win_max: -0.1157, p_win_std: 0.0752
p_lose_average: -0.5213, p_lose_min: -0.7266, p_lose_max: -0.2207, p_lose_std: 0.0689

eval_z_samples_size: 1000
eval_loss: 0.5552, accuracy: 0.7012
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.7115, true_2_rate: 0.6905
log joint likelihood: tensor(-1128.7812500000) joint log likelihood: tensor(-1535.5136718750)
r_win_average: 1.6778, r_win_min: -2.9844, r_win_max: 5.6250, r_win_std: 1.2979
r_lose_average: 0.4673, r_lose_min: -3.7500, r_lose_max: 3.9844, r_lose_std: 1.4531
eta_win_average: 0.0766, eta_win_min: 0.0160, eta_win_max: 0.1562, eta_win_std: 0.0185
eta_lose_average: 0.0740, eta_lose_min: 0.0247, eta_lose_max: 0.1455, eta_lose_std: 0.0171
p_win_average: -0.1158, p_win_min: -0.2021, p_win_max: -0.0408, p_win_std: 0.0241
p_lose_average: -0.1242, p_lose_min: -0.2031, p_lose_max: -0.0186, p_lose_std: 0.0232

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.5479, accuracy: 0.6914
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7038, true_2_rate: 0.6786
log joint likelihood: tensor(-1198.0703125000) joint log likelihood: tensor(-1495.9296875000)
r_win_average: 0.8346, r_win_min: -2.4062, r_win_max: 4.7812, r_win_std: 1.0199
r_lose_average: -0.1492, r_lose_min: -3.0469, r_lose_max: 2.5781, r_lose_std: 1.0508
eta_win_average: -0.2329, eta_win_min: -0.4688, eta_win_max: -0.0698, eta_win_std: 0.0463
eta_lose_average: -0.2322, eta_lose_min: -0.3750, eta_lose_max: -0.0583, eta_lose_std: 0.0428
p_win_average: -0.1589, p_win_min: -0.3281, p_win_max: -0.0190, p_win_std: 0.0408
p_lose_average: -0.1619, p_lose_min: -0.3281, p_lose_max: -0.0284, p_lose_std: 0.0365

eval_z_samples_size: 1000
eval_loss: 0.5483, accuracy: 0.6914
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7038, true_2_rate: 0.6786
log joint likelihood: tensor(-1196.6015625000) joint log likelihood: tensor(-1491.3476562500)
r_win_average: 0.7047, r_win_min: -2.5312, r_win_max: 4.8750, r_win_std: 1.0164
r_lose_average: -0.2751, r_lose_min: -3.1562, r_lose_max: 2.4844, r_lose_std: 1.0451
eta_win_average: -0.3192, eta_win_min: -0.3594, eta_win_max: -0.2734, eta_win_std: 0.0099
eta_lose_average: -0.3179, eta_lose_min: -0.3594, eta_lose_max: -0.2930, eta_lose_std: 0.0097
p_win_average: -0.2027, p_win_min: -0.2734, p_win_max: -0.0957, p_win_std: 0.0213
p_lose_average: -0.2018, p_lose_min: -0.2598, p_lose_max: -0.1289, p_lose_std: 0.0197

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 100
eval_loss: 0.5454, accuracy: 0.6953
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7115, true_2_rate: 0.6786
log joint likelihood: tensor(-1191.2128906250) joint log likelihood: tensor(-1485.7382812500)
r_win_average: 0.8514, r_win_min: -3.2969, r_win_max: 5.4375, r_win_std: 1.2619
r_lose_average: -0.3041, r_lose_min: -3.6406, r_lose_max: 3.0469, r_lose_std: 1.2492
eta_win_average: -0.1719, eta_win_min: -0.3242, eta_win_max: -0.0918, eta_win_std: 0.0253
eta_lose_average: -0.1746, eta_lose_min: -0.2676, eta_lose_max: -0.0859, eta_lose_std: 0.0231
p_win_average: 0.0151, p_win_min: -0.1270, p_win_max: 0.1562, p_win_std: 0.0329
p_lose_average: 0.0124, p_lose_min: -0.0972, p_lose_max: 0.1206, p_lose_std: 0.0318

eval_z_samples_size: 1000
eval_loss: 0.5459, accuracy: 0.6953
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7077, true_2_rate: 0.6825
log joint likelihood: tensor(-1185.5605468750) joint log likelihood: tensor(-1482.5312500000)
r_win_average: 0.8255, r_win_min: -3.3281, r_win_max: 5.3438, r_win_std: 1.2578
r_lose_average: -0.3257, r_lose_min: -3.6406, r_lose_max: 2.9375, r_lose_std: 1.2478
eta_win_average: -0.1701, eta_win_min: -0.1953, eta_win_max: -0.1387, eta_win_std: 0.0077
eta_lose_average: -0.1686, eta_lose_min: -0.1934, eta_lose_max: -0.1270, eta_lose_std: 0.0074
p_win_average: -0.0122, p_win_min: -0.0537, p_win_max: 0.0500, p_win_std: 0.0145
p_lose_average: -0.0147, p_lose_min: -0.0537, p_lose_max: 0.0306, p_lose_std: 0.0129

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 100
eval_loss: 0.5408, accuracy: 0.7129
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7308, true_2_rate: 0.6944
log joint likelihood: tensor(-1186.5937500000) joint log likelihood: tensor(-1457.9062500000)
r_win_average: 0.5327, r_win_min: -3.6719, r_win_max: 4.7812, r_win_std: 1.1135
r_lose_average: -0.5157, r_lose_min: -4.0938, r_lose_max: 2.2031, r_lose_std: 1.1966
eta_win_average: 0.4538, eta_win_min: 0.3398, eta_win_max: 0.5898, eta_win_std: 0.0309
eta_lose_average: 0.4458, eta_lose_min: 0.2734, eta_lose_max: 0.5703, eta_lose_std: 0.0275
p_win_average: -0.7908, p_win_min: -0.9336, p_win_max: -0.6875, p_win_std: 0.0392
p_lose_average: -0.8012, p_lose_min: -0.9336, p_lose_max: -0.5430, p_lose_std: 0.0403

eval_z_samples_size: 1000
eval_loss: 0.5398, accuracy: 0.7090
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7269, true_2_rate: 0.6905
log joint likelihood: tensor(-1183.4648437500) joint log likelihood: tensor(-1459.1679687500)
r_win_average: 0.4999, r_win_min: -3.5938, r_win_max: 4.5938, r_win_std: 1.0883
r_lose_average: -0.5229, r_lose_min: -3.9375, r_lose_max: 2.1562, r_lose_std: 1.1671
eta_win_average: -0.3719, eta_win_min: -0.4395, eta_win_max: -0.2793, eta_win_std: 0.0168
eta_lose_average: -0.3656, eta_lose_min: -0.4395, eta_lose_max: -0.2891, eta_lose_std: 0.0175
p_win_average: 0.0025, p_win_min: -0.0566, p_win_max: 0.1152, p_win_std: 0.0218
p_lose_average: 0.0033, p_lose_min: -0.0500, p_lose_max: 0.0820, p_lose_std: 0.0199

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 100
eval_loss: 0.5378, accuracy: 0.7227
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7346, true_2_rate: 0.7103
log joint likelihood: tensor(-1170.1601562500) joint log likelihood: tensor(-1464.5136718750)
r_win_average: 1.3868, r_win_min: -2.9375, r_win_max: 5.7188, r_win_std: 1.1720
r_lose_average: 0.2673, r_lose_min: -3.6250, r_lose_max: 3.2031, r_lose_std: 1.3060
eta_win_average: -0.6589, eta_win_min: -0.7539, eta_win_max: -0.4863, eta_win_std: 0.0324
eta_lose_average: -0.6592, eta_lose_min: -0.7539, eta_lose_max: -0.5547, eta_lose_std: 0.0281
p_win_average: 1.0052, p_win_min: 0.9023, p_win_max: 1.1328, p_win_std: 0.0366
p_lose_average: 1.0072, p_lose_min: 0.8867, p_lose_max: 1.1641, p_lose_std: 0.0371

eval_z_samples_size: 1000
eval_loss: 0.5393, accuracy: 0.7207
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7269, true_2_rate: 0.7143
log joint likelihood: tensor(-1160.7792968750) joint log likelihood: tensor(-1463.4257812500)
r_win_average: 0.8573, r_win_min: -3.5000, r_win_max: 5.1250, r_win_std: 1.1768
r_lose_average: -0.2671, r_lose_min: -4.2812, r_lose_max: 2.5781, r_lose_std: 1.3165
eta_win_average: -0.1385, eta_win_min: -0.1904, eta_win_max: -0.0374, eta_win_std: 0.0155
eta_lose_average: -0.1371, eta_lose_min: -0.2354, eta_lose_max: -0.0889, eta_lose_std: 0.0152
p_win_average: -0.0448, p_win_min: -0.1035, p_win_max: 0.0801, p_win_std: 0.0200
p_lose_average: -0.0491, p_lose_min: -0.1094, p_lose_max: 0.1050, p_lose_std: 0.0212

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.5388, accuracy: 0.7188
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7308, true_2_rate: 0.7063
log joint likelihood: tensor(-1165.4257812500) joint log likelihood: tensor(-1464.5195312500)
r_win_average: 0.3532, r_win_min: -3.9375, r_win_max: 4.6562, r_win_std: 1.1638
r_lose_average: -0.7671, r_lose_min: -4.6875, r_lose_max: 1.8984, r_lose_std: 1.2956
eta_win_average: -0.1716, eta_win_min: -0.3516, eta_win_max: -0.0391, eta_win_std: 0.0445
eta_lose_average: -0.1724, eta_lose_min: -0.3184, eta_lose_max: 0.0396, eta_lose_std: 0.0408
p_win_average: -0.5228, p_win_min: -0.6562, p_win_max: -0.3730, p_win_std: 0.0401
p_lose_average: -0.5196, p_lose_min: -0.6602, p_lose_max: -0.3203, p_lose_std: 0.0397

eval_z_samples_size: 1000
eval_loss: 0.5378, accuracy: 0.7148
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.7192, true_2_rate: 0.7103
log joint likelihood: tensor(-1164.5351562500) joint log likelihood: tensor(-1462.8203125000)
r_win_average: 0.7193, r_win_min: -3.6250, r_win_max: 4.7812, r_win_std: 1.1642
r_lose_average: -0.4019, r_lose_min: -4.3125, r_lose_max: 2.3594, r_lose_std: 1.3028
eta_win_average: -0.2496, eta_win_min: -0.3027, eta_win_max: -0.1934, eta_win_std: 0.0121
eta_lose_average: -0.2471, eta_lose_min: -0.2812, eta_lose_max: -0.2080, eta_lose_std: 0.0117
p_win_average: -0.0790, p_win_min: -0.1348, p_win_max: 0.0101, p_win_std: 0.0223
p_lose_average: -0.0797, p_lose_min: -0.1406, p_lose_max: 0.0092, p_lose_std: 0.0214

------------------------------------------------------------------------------------------
[2023-09-25 08:06:30,807] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:40,279] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:41,791] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:41,879] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:41,946] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:41,961] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:41,981] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:41,998] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:42,032] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 08:06:42,870] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:42,870] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:06:45,493] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:45,493] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:06:45,558] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:45,558] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:06:45,625] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:45,625] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:06:45,625] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 08:06:45,687] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:45,687] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:06:45,733] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:45,733] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:06:45,738] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:45,738] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 08:06:45,739] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 08:06:45,739] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 342
cuda seed 342
torch seed 542
cuda seed 542
torch seed 142
cuda seed 142
torch seed 642
cuda seed 642
torch seed 242
cuda seed 242
torch seed 742
cuda seed 742
torch seed 442
cuda seed 442
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/hh_new_final/anthropic_hh-ref_size10-enn_dim128-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
training dataset size: 2544
eval dataset size: 8
joint eval dataset size: 256
Total steps:  2544
Warmup steps:  76
[2023-09-25 08:07:19,748] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 08:07:22,048] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 08:07:22,049] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 08:07:22,049] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 08:07:22,054] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 08:07:22,054] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 08:07:22,054] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 08:07:22,054] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 08:07:22,054] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 08:07:22,054] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 08:07:22,054] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-25 08:07:30,894] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 08:07:30,894] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 08:07:30,894] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.9 GB, percent = 4.5%
[2023-09-25 08:07:31,004] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 08:07:31,005] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 08:07:31,005] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.9 GB, percent = 4.5%
[2023-09-25 08:07:31,005] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 08:07:31,105] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 08:07:31,105] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 08:07:31,105] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.9 GB, percent = 4.5%
[2023-09-25 08:07:31,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 08:07:31,107] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 08:07:31,107] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 08:07:31,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 08:07:31,107] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 08:07:31,107] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 08:07:31,107] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 08:07:31,107] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 08:07:31,107] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd86d7e21d0>
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 08:07:31,108] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 08:07:31,109] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 08:07:31,109] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 0.8950, accuracy: 0.5117
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.4727, prediction_2_rate: 0.5273
true_1_rate: 0.4846, true_2_rate: 0.5397
log joint likelihood: tensor(-704.2656250000) joint log likelihood: tensor(-4410.2968750000)
r_win_average: -3.5955, r_win_min: -6.5938, r_win_max: 3.0781, r_win_std: 1.3612
r_lose_average: -4.8513, r_lose_min: -7.9375, r_lose_max: 0.1729, r_lose_std: 1.0958
eta_win_average: -0.3047, eta_win_min: -1.2109, eta_win_max: 0.5312, eta_win_std: 0.2485
eta_lose_average: -0.3696, eta_lose_min: -1.0938, eta_lose_max: 0.5195, eta_lose_std: 0.2485
p_win_average: -0.7265, p_win_min: -2.2031, p_win_max: 0.2578, p_win_std: 0.3656
p_lose_average: -0.8393, p_lose_min: -2.3125, p_lose_max: 0.2578, p_lose_std: 0.3368

eval_z_samples_size: 1000
eval_loss: 0.8394, accuracy: 0.5410
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.5462, true_2_rate: 0.5357
log joint likelihood: tensor(-692.1289062500) joint log likelihood: tensor(-4425.2968750000)
r_win_average: -2.4017, r_win_min: -4.8750, r_win_max: 3.9219, r_win_std: 1.3531
r_lose_average: -3.5684, r_lose_min: -5.9688, r_lose_max: 1.1641, r_lose_std: 1.0175
eta_win_average: -0.1005, eta_win_min: -0.2695, eta_win_max: 0.1260, eta_win_std: 0.0540
eta_lose_average: -0.1052, eta_lose_min: -0.2412, eta_lose_max: 0.0610, eta_lose_std: 0.0505
p_win_average: 0.2238, p_win_min: 0.0168, p_win_max: 0.4609, p_win_std: 0.0710
p_lose_average: 0.2191, p_lose_min: 0.0168, p_lose_max: 0.4688, p_lose_std: 0.0624

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.6309, accuracy: 0.6387
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.6615, true_2_rate: 0.6151
log joint likelihood: tensor(-1292.1015625000) joint log likelihood: tensor(-1791.4062500000)
r_win_average: 1.3959, r_win_min: -0.0938, r_win_max: 4.3750, r_win_std: 0.5962
r_lose_average: 0.9105, r_lose_min: -0.5586, r_lose_max: 2.2031, r_lose_std: 0.4729
eta_win_average: 0.3170, eta_win_min: -0.0427, eta_win_max: 0.5742, eta_win_std: 0.0632
eta_lose_average: 0.3291, eta_lose_min: 0.1016, eta_lose_max: 0.4883, eta_lose_std: 0.0542
p_win_average: -0.0176, p_win_min: -0.3008, p_win_max: 0.1797, p_win_std: 0.0423
p_lose_average: -0.0203, p_lose_min: -0.3301, p_lose_max: 0.1260, p_lose_std: 0.0401

eval_z_samples_size: 1000
eval_loss: 0.6284, accuracy: 0.6484
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.6577, true_2_rate: 0.6389
log joint likelihood: tensor(-1284.9921875000) joint log likelihood: tensor(-1792.9531250000)
r_win_average: 0.9489, r_win_min: -0.6523, r_win_max: 4.1562, r_win_std: 0.6337
r_lose_average: 0.4452, r_lose_min: -0.7969, r_lose_max: 1.7266, r_lose_std: 0.4914
eta_win_average: -0.0492, eta_win_min: -0.1377, eta_win_max: 0.1143, eta_win_std: 0.0227
eta_lose_average: -0.0496, eta_lose_min: -0.1318, eta_lose_max: 0.0247, eta_lose_std: 0.0194
p_win_average: -0.0998, p_win_min: -0.1660, p_win_max: 0.0962, p_win_std: 0.0278
p_lose_average: -0.1056, p_lose_min: -0.1582, p_lose_max: 0.0217, p_lose_std: 0.0221

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.6035, accuracy: 0.6445
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.6692, true_2_rate: 0.6190
log joint likelihood: tensor(-1236.0312500000) joint log likelihood: tensor(-1689.0156250000)
r_win_average: 1.2887, r_win_min: -1.6875, r_win_max: 4.0000, r_win_std: 0.8764
r_lose_average: 0.5394, r_lose_min: -2.1406, r_lose_max: 3.0625, r_lose_std: 0.8854
eta_win_average: 0.0944, eta_win_min: 0.0182, eta_win_max: 0.2383, eta_win_std: 0.0331
eta_lose_average: 0.0973, eta_lose_min: 0.0067, eta_lose_max: 0.2324, eta_lose_std: 0.0337
p_win_average: -0.1518, p_win_min: -0.3203, p_win_max: -0.0255, p_win_std: 0.0480
p_lose_average: -0.1558, p_lose_min: -0.2871, p_lose_max: 0.0212, p_lose_std: 0.0481

eval_z_samples_size: 1000
eval_loss: 0.6016, accuracy: 0.6445
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.6692, true_2_rate: 0.6190
log joint likelihood: tensor(-1228.4609375000) joint log likelihood: tensor(-1693.7187500000)
r_win_average: 1.1267, r_win_min: -1.8672, r_win_max: 3.9375, r_win_std: 0.8771
r_lose_average: 0.3751, r_lose_min: -2.3281, r_lose_max: 2.8906, r_lose_std: 0.8928
eta_win_average: -0.0695, eta_win_min: -0.1196, eta_win_max: -0.0201, eta_win_std: 0.0155
eta_lose_average: -0.0706, eta_lose_min: -0.1074, eta_lose_max: -0.0291, eta_lose_std: 0.0154
p_win_average: -0.1505, p_win_min: -0.2109, p_win_max: -0.0193, p_win_std: 0.0266
p_lose_average: -0.1519, p_lose_min: -0.2178, p_lose_max: -0.0342, p_lose_std: 0.0271

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 100
eval_loss: 0.5706, accuracy: 0.6816
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7038, true_2_rate: 0.6587
log joint likelihood: tensor(-1206.9531250000) joint log likelihood: tensor(-1582.4453125000)
r_win_average: 1.1835, r_win_min: -1.9922, r_win_max: 3.2188, r_win_std: 0.9092
r_lose_average: 0.3433, r_lose_min: -2.2969, r_lose_max: 3.0000, r_lose_std: 0.9689
eta_win_average: -0.1168, eta_win_min: -0.1758, eta_win_max: -0.0138, eta_win_std: 0.0265
eta_lose_average: -0.1172, eta_lose_min: -0.2070, eta_lose_max: -0.0138, eta_lose_std: 0.0243
p_win_average: 0.3475, p_win_min: 0.1885, p_win_max: 0.4414, p_win_std: 0.0336
p_lose_average: 0.3473, p_lose_min: 0.2354, p_lose_max: 0.4512, p_lose_std: 0.0315

eval_z_samples_size: 1000
eval_loss: 0.5688, accuracy: 0.6855
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7077, true_2_rate: 0.6627
log joint likelihood: tensor(-1202.7734375000) joint log likelihood: tensor(-1582.2031250000)
r_win_average: 0.7449, r_win_min: -2.4688, r_win_max: 2.7969, r_win_std: 0.9119
r_lose_average: -0.1034, r_lose_min: -2.7812, r_lose_max: 2.5000, r_lose_std: 0.9789
eta_win_average: 0.0259, eta_win_min: -0.0109, eta_win_max: 0.0630, eta_win_std: 0.0103
eta_lose_average: 0.0236, eta_lose_min: -0.0062, eta_lose_max: 0.0544, eta_lose_std: 0.0103
p_win_average: -0.2339, p_win_min: -0.2832, p_win_max: -0.1748, p_win_std: 0.0165
p_lose_average: -0.2399, p_lose_min: -0.2793, p_lose_max: -0.1748, p_lose_std: 0.0162

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 100
eval_loss: 0.5618, accuracy: 0.7070
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7269, true_2_rate: 0.6865
log joint likelihood: tensor(-1172.8476562500) joint log likelihood: tensor(-1546.3867187500)
r_win_average: 2.8030, r_win_min: -0.7773, r_win_max: 5.7188, r_win_std: 1.2056
r_lose_average: 1.7445, r_lose_min: -1.1719, r_lose_max: 5.3125, r_lose_std: 1.1962
eta_win_average: 0.0098, eta_win_min: -0.2461, eta_win_max: 0.1934, eta_win_std: 0.0556
eta_lose_average: 0.0069, eta_lose_min: -0.2461, eta_lose_max: 0.1357, eta_lose_std: 0.0519
p_win_average: 1.4350, p_win_min: 1.2891, p_win_max: 1.6172, p_win_std: 0.0516
p_lose_average: 1.4309, p_lose_min: 1.2812, p_lose_max: 1.6641, p_lose_std: 0.0497

eval_z_samples_size: 1000
eval_loss: 0.5596, accuracy: 0.7051
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7231, true_2_rate: 0.6865
log joint likelihood: tensor(-1165.0351562500) joint log likelihood: tensor(-1550.1679687500)
r_win_average: 1.4845, r_win_min: -2.0156, r_win_max: 4.4062, r_win_std: 1.2078
r_lose_average: 0.4301, r_lose_min: -2.4375, r_lose_max: 3.9844, r_lose_std: 1.1975
eta_win_average: 0.0603, eta_win_min: 0.0100, eta_win_max: 0.0981, eta_win_std: 0.0146
eta_lose_average: 0.0594, eta_lose_min: 0.0181, eta_lose_max: 0.1040, eta_lose_std: 0.0144
p_win_average: 0.0654, p_win_min: -0.0056, p_win_max: 0.1377, p_win_std: 0.0217
p_lose_average: 0.0645, p_lose_min: -0.0010, p_lose_max: 0.1416, p_lose_std: 0.0212

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 100
eval_loss: 0.5552, accuracy: 0.7168
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7231, true_2_rate: 0.7103
log joint likelihood: tensor(-1203.2460937500) joint log likelihood: tensor(-1519.8515625000)
r_win_average: 0.6378, r_win_min: -2.2188, r_win_max: 3.8594, r_win_std: 0.9083
r_lose_average: -0.1944, r_lose_min: -2.5625, r_lose_max: 2.4688, r_lose_std: 0.9276
eta_win_average: 0.2770, eta_win_min: 0.0762, eta_win_max: 0.4375, eta_win_std: 0.0511
eta_lose_average: 0.2736, eta_lose_min: 0.0757, eta_lose_max: 0.4082, eta_lose_std: 0.0473
p_win_average: -0.1131, p_win_min: -0.3379, p_win_max: 0.0334, p_win_std: 0.0496
p_lose_average: -0.1150, p_lose_min: -0.3223, p_lose_max: -0.0101, p_lose_std: 0.0456

eval_z_samples_size: 1000
eval_loss: 0.5566, accuracy: 0.7109
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.7115, true_2_rate: 0.7103
log joint likelihood: tensor(-1200.2500000000) joint log likelihood: tensor(-1522.6914062500)
r_win_average: 0.7037, r_win_min: -2.2031, r_win_max: 3.9375, r_win_std: 0.9082
r_lose_average: -0.1257, r_lose_min: -2.4688, r_lose_max: 2.5938, r_lose_std: 0.9301
eta_win_average: -0.0634, eta_win_min: -0.1338, eta_win_max: -0.0299, eta_win_std: 0.0163
eta_lose_average: -0.0627, eta_lose_min: -0.1338, eta_lose_max: -0.0299, eta_lose_std: 0.0149
p_win_average: 0.2917, p_win_min: 0.2441, p_win_max: 0.3398, p_win_std: 0.0143
p_lose_average: 0.2909, p_lose_min: 0.2539, p_lose_max: 0.3516, p_lose_std: 0.0137

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 100
eval_loss: 0.5530, accuracy: 0.6973
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7154, true_2_rate: 0.6786
log joint likelihood: tensor(-1145.3398437500) joint log likelihood: tensor(-1529.5117187500)
r_win_average: -0.1412, r_win_min: -4.5938, r_win_max: 3.2812, r_win_std: 1.1551
r_lose_average: -1.2687, r_lose_min: -5.0938, r_lose_max: 1.6016, r_lose_std: 1.3508
eta_win_average: -0.7591, eta_win_min: -0.9219, eta_win_max: -0.4609, eta_win_std: 0.0553
eta_lose_average: -0.7527, eta_lose_min: -0.8945, eta_lose_max: -0.5820, eta_lose_std: 0.0496
p_win_average: -0.5346, p_win_min: -0.6836, p_win_max: -0.2539, p_win_std: 0.0725
p_lose_average: -0.5250, p_lose_min: -0.7031, p_lose_max: -0.0640, p_lose_std: 0.0717

eval_z_samples_size: 1000
eval_loss: 0.5537, accuracy: 0.6953
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7115, true_2_rate: 0.6786
log joint likelihood: tensor(-1144.6640625000) joint log likelihood: tensor(-1530.6953125000)
r_win_average: 1.0342, r_win_min: -3.4844, r_win_max: 4.4688, r_win_std: 1.1661
r_lose_average: -0.1063, r_lose_min: -3.9844, r_lose_max: 2.7344, r_lose_std: 1.3673
eta_win_average: -0.2896, eta_win_min: -0.3359, eta_win_max: -0.2188, eta_win_std: 0.0140
eta_lose_average: -0.2876, eta_lose_min: -0.3242, eta_lose_max: -0.2344, eta_lose_std: 0.0132
p_win_average: 0.1712, p_win_min: 0.1191, p_win_max: 0.2676, p_win_std: 0.0239
p_lose_average: 0.1725, p_lose_min: 0.1079, p_lose_max: 0.2871, p_lose_std: 0.0241

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 100
eval_loss: 0.5435, accuracy: 0.7012
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.7192, true_2_rate: 0.6825
log joint likelihood: tensor(-1190.1445312500) joint log likelihood: tensor(-1489.4296875000)
r_win_average: 0.3360, r_win_min: -3.0625, r_win_max: 4.1562, r_win_std: 1.0095
r_lose_average: -0.6520, r_lose_min: -3.6719, r_lose_max: 2.0469, r_lose_std: 1.0883
eta_win_average: -0.6270, eta_win_min: -0.7109, eta_win_max: -0.5039, eta_win_std: 0.0318
eta_lose_average: -0.6202, eta_lose_min: -0.7109, eta_lose_max: -0.5039, eta_lose_std: 0.0299
p_win_average: 0.1244, p_win_min: 0.0284, p_win_max: 0.3203, p_win_std: 0.0374
p_lose_average: 0.1350, p_lose_min: 0.0469, p_lose_max: 0.3086, p_lose_std: 0.0358

eval_z_samples_size: 1000
eval_loss: 0.5435, accuracy: 0.6914
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.7154, true_2_rate: 0.6667
log joint likelihood: tensor(-1188.3398437500) joint log likelihood: tensor(-1486.0312500000)
r_win_average: 0.7904, r_win_min: -2.7188, r_win_max: 4.5938, r_win_std: 1.0199
r_lose_average: -0.2172, r_lose_min: -3.2812, r_lose_max: 2.5469, r_lose_std: 1.1067
eta_win_average: -0.0145, eta_win_min: -0.0496, eta_win_max: 0.0156, eta_win_std: 0.0092
eta_lose_average: -0.0147, eta_lose_min: -0.0454, eta_lose_max: 0.0175, eta_lose_std: 0.0088
p_win_average: -0.0342, p_win_min: -0.0776, p_win_max: 0.0197, p_win_std: 0.0139
p_lose_average: -0.0348, p_lose_min: -0.0767, p_lose_max: 0.0292, p_lose_std: 0.0132

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 100
eval_loss: 0.5410, accuracy: 0.7031
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.7308, true_2_rate: 0.6746
log joint likelihood: tensor(-1177.1542968750) joint log likelihood: tensor(-1483.1289062500)
r_win_average: 0.0873, r_win_min: -4.1875, r_win_max: 4.4062, r_win_std: 1.2348
r_lose_average: -1.0708, r_lose_min: -4.4375, r_lose_max: 2.1406, r_lose_std: 1.2721
eta_win_average: -0.1227, eta_win_min: -0.2217, eta_win_max: 0.0623, eta_win_std: 0.0354
eta_lose_average: -0.1259, eta_lose_min: -0.3047, eta_lose_max: 0.0186, eta_lose_std: 0.0328
p_win_average: -0.3947, p_win_min: -0.5195, p_win_max: -0.2539, p_win_std: 0.0376
p_lose_average: -0.3983, p_lose_min: -0.5078, p_lose_max: -0.2109, p_lose_std: 0.0389

eval_z_samples_size: 1000
eval_loss: 0.5420, accuracy: 0.7031
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7231, true_2_rate: 0.6825
log joint likelihood: tensor(-1167.2324218750) joint log likelihood: tensor(-1478.6347656250)
r_win_average: 0.5489, r_win_min: -3.6406, r_win_max: 4.6562, r_win_std: 1.2247
r_lose_average: -0.5989, r_lose_min: -3.8906, r_lose_max: 2.6250, r_lose_std: 1.2589
eta_win_average: -0.0481, eta_win_min: -0.0830, eta_win_max: 0.0027, eta_win_std: 0.0116
eta_lose_average: -0.0435, eta_lose_min: -0.0708, eta_lose_max: -0.0074, eta_lose_std: 0.0104
p_win_average: -0.0079, p_win_min: -0.0564, p_win_max: 0.0540, p_win_std: 0.0156
p_lose_average: -0.0083, p_lose_min: -0.0500, p_lose_max: 0.0547, p_lose_std: 0.0151

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 100
eval_loss: 0.5378, accuracy: 0.7051
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.7269, true_2_rate: 0.6825
log joint likelihood: tensor(-1177.4531250000) joint log likelihood: tensor(-1469.1445312500)
r_win_average: 0.1559, r_win_min: -3.9844, r_win_max: 3.8750, r_win_std: 1.0654
r_lose_average: -0.8892, r_lose_min: -4.4062, r_lose_max: 2.0469, r_lose_std: 1.1939
eta_win_average: -0.4726, eta_win_min: -0.5195, eta_win_max: -0.3555, eta_win_std: 0.0186
eta_lose_average: -0.4761, eta_lose_min: -0.5234, eta_lose_max: -0.4297, eta_lose_std: 0.0169
p_win_average: 0.0927, p_win_min: 0.0067, p_win_max: 0.1953, p_win_std: 0.0248
p_lose_average: 0.0951, p_lose_min: -0.0214, p_lose_max: 0.1621, p_lose_std: 0.0234

eval_z_samples_size: 1000
eval_loss: 0.5383, accuracy: 0.7109
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.7308, true_2_rate: 0.6905
log joint likelihood: tensor(-1170.8398437500) joint log likelihood: tensor(-1464.3906250000)
r_win_average: 0.9243, r_win_min: -3.2500, r_win_max: 4.5625, r_win_std: 1.0736
r_lose_average: -0.1289, r_lose_min: -3.6406, r_lose_max: 2.8281, r_lose_std: 1.2055
eta_win_average: 0.3416, eta_win_min: 0.2637, eta_win_max: 0.3770, eta_win_std: 0.0142
eta_lose_average: 0.3415, eta_lose_min: 0.2871, eta_lose_max: 0.3906, eta_lose_std: 0.0133
p_win_average: 0.0471, p_win_min: -0.0238, p_win_max: 0.1040, p_win_std: 0.0198
p_lose_average: 0.0380, p_lose_min: -0.0562, p_lose_max: 0.0947, p_lose_std: 0.0198

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 100
eval_loss: 0.5417, accuracy: 0.7129
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.7231, true_2_rate: 0.7024
log joint likelihood: tensor(-1149.9160156250) joint log likelihood: tensor(-1471.4335937500)
r_win_average: 1.7418, r_win_min: -2.5156, r_win_max: 5.2500, r_win_std: 1.1615
r_lose_average: 0.5869, r_lose_min: -3.3750, r_lose_max: 3.6250, r_lose_std: 1.3527
eta_win_average: 1.1092, eta_win_min: 0.7852, eta_win_max: 1.2969, eta_win_std: 0.0683
eta_lose_average: 1.1175, eta_lose_min: 0.9023, eta_lose_max: 1.3750, eta_lose_std: 0.0614
p_win_average: -0.1527, p_win_min: -0.3711, p_win_max: -0.0201, p_win_std: 0.0503
p_lose_average: -0.1709, p_lose_min: -0.3867, p_lose_max: -0.0342, p_lose_std: 0.0472

eval_z_samples_size: 1000
eval_loss: 0.5383, accuracy: 0.7129
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7269, true_2_rate: 0.6984
log joint likelihood: tensor(-1149.7265625000) joint log likelihood: tensor(-1473.9218750000)
r_win_average: 0.7829, r_win_min: -3.5938, r_win_max: 4.3438, r_win_std: 1.1721
r_lose_average: -0.3701, r_lose_min: -4.5000, r_lose_max: 2.7031, r_lose_std: 1.3606
eta_win_average: 0.2313, eta_win_min: 0.1719, eta_win_max: 0.2949, eta_win_std: 0.0156
eta_lose_average: 0.2338, eta_lose_min: 0.1680, eta_lose_max: 0.2852, eta_lose_std: 0.0162
p_win_average: -0.2345, p_win_min: -0.3184, p_win_max: -0.1465, p_win_std: 0.0239
p_lose_average: -0.2431, p_lose_min: -0.3262, p_lose_max: -0.1709, p_lose_std: 0.0245

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 100
eval_loss: 0.5361, accuracy: 0.7168
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7308, true_2_rate: 0.7024
log joint likelihood: tensor(-1154.5742187500) joint log likelihood: tensor(-1474.9902343750)
r_win_average: 0.1187, r_win_min: -4.2188, r_win_max: 3.6094, r_win_std: 1.1757
r_lose_average: -1.0507, r_lose_min: -5.3125, r_lose_max: 2.0000, r_lose_std: 1.3625
eta_win_average: -0.4359, eta_win_min: -0.5078, eta_win_max: -0.3281, eta_win_std: 0.0308
eta_lose_average: -0.4434, eta_lose_min: -0.5898, eta_lose_max: -0.3184, eta_lose_std: 0.0297
p_win_average: -0.2679, p_win_min: -0.4062, p_win_max: -0.1064, p_win_std: 0.0375
p_lose_average: -0.2708, p_lose_min: -0.4199, p_lose_max: -0.1299, p_lose_std: 0.0399

eval_z_samples_size: 1000
eval_loss: 0.5356, accuracy: 0.7148
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7269, true_2_rate: 0.7024
log joint likelihood: tensor(-1145.2460937500) joint log likelihood: tensor(-1469.3535156250)
r_win_average: 0.5373, r_win_min: -3.8594, r_win_max: 4.1562, r_win_std: 1.1840
r_lose_average: -0.6375, r_lose_min: -4.7812, r_lose_max: 2.3906, r_lose_std: 1.3690
eta_win_average: 0.0302, eta_win_min: -0.0349, eta_win_max: 0.1250, eta_win_std: 0.0253
eta_lose_average: 0.0251, eta_lose_min: -0.0679, eta_lose_max: 0.1118, eta_lose_std: 0.0230
p_win_average: -0.3155, p_win_min: -0.4199, p_win_max: -0.2402, p_win_std: 0.0254
p_lose_average: -0.3259, p_lose_min: -0.4238, p_lose_max: -0.2402, p_lose_std: 0.0262

------------------------------------------------------------------------------------------
[2023-09-25 10:08:12,885] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:23,771] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:24,015] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:24,039] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:24,099] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:24,194] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:24,243] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:24,248] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:24,262] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 10:08:27,711] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:27,711] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:08:28,149] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:28,149] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:08:28,156] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:28,156] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:08:28,240] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:28,240] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:08:28,306] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:28,306] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:08:28,364] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:28,364] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:08:28,364] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 10:08:28,366] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:28,366] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 10:08:28,374] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 10:08:28,374] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 742
cuda seed 742
torch seed 442
cuda seed 442
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/hh_new_final/anthropic_hh-ref_size10-enn_dim128-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0001-enn_decay0.1-reward_lr0.0001-reward_decay0.5-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 342
cuda seed 342
torch seed 642
cuda seed 642
torch seed 242
cuda seed 242
torch seed 142
cuda seed 142
torch seed 542
cuda seed 542
training dataset size: 2544
eval dataset size: 8
joint eval dataset size: 256
Total steps:  2544
Warmup steps:  76
[2023-09-25 10:09:08,712] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 10:09:11,163] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 10:09:11,164] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 10:09:11,164] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 10:09:11,169] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 10:09:11,169] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 10:09:11,169] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 10:09:11,169] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 10:09:11,169] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 10:09:11,169] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 10:09:11,169] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-25 10:09:24,886] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 10:09:24,886] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 10:09:24,887] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.93 GB, percent = 4.5%
[2023-09-25 10:09:24,989] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 10:09:24,990] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 10:09:24,990] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.93 GB, percent = 4.5%
[2023-09-25 10:09:24,990] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 10:09:25,082] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 10:09:25,083] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 10:09:25,083] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 44.93 GB, percent = 4.5%
[2023-09-25 10:09:25,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 10:09:25,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 10:09:25,084] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 10:09:25,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 1e-05, 1e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 10:09:25,085] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f028fc239d0>
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 10:09:25,085] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 10:09:25,086] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 10:09:25,086] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 100
eval_loss: 0.8950, accuracy: 0.5117
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.4727, prediction_2_rate: 0.5273
true_1_rate: 0.4846, true_2_rate: 0.5397
log joint likelihood: tensor(-704.2656250000) joint log likelihood: tensor(-4410.2968750000)
r_win_average: -3.5955, r_win_min: -6.5938, r_win_max: 3.0781, r_win_std: 1.3612
r_lose_average: -4.8513, r_lose_min: -7.9375, r_lose_max: 0.1729, r_lose_std: 1.0958
eta_win_average: -0.3047, eta_win_min: -1.2109, eta_win_max: 0.5312, eta_win_std: 0.2485
eta_lose_average: -0.3696, eta_lose_min: -1.0938, eta_lose_max: 0.5195, eta_lose_std: 0.2485
p_win_average: -0.7265, p_win_min: -2.2031, p_win_max: 0.2578, p_win_std: 0.3656
p_lose_average: -0.8393, p_lose_min: -2.3125, p_lose_max: 0.2578, p_lose_std: 0.3368

eval_z_samples_size: 1000
eval_loss: 0.8394, accuracy: 0.5410
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.5462, true_2_rate: 0.5357
log joint likelihood: tensor(-692.1289062500) joint log likelihood: tensor(-4425.2968750000)
r_win_average: -2.4017, r_win_min: -4.8750, r_win_max: 3.9219, r_win_std: 1.3531
r_lose_average: -3.5684, r_lose_min: -5.9688, r_lose_max: 1.1641, r_lose_std: 1.0175
eta_win_average: -0.1005, eta_win_min: -0.2695, eta_win_max: 0.1260, eta_win_std: 0.0540
eta_lose_average: -0.1052, eta_lose_min: -0.2412, eta_lose_max: 0.0610, eta_lose_std: 0.0505
p_win_average: 0.2238, p_win_min: 0.0168, p_win_max: 0.4609, p_win_std: 0.0710
p_lose_average: 0.2191, p_lose_min: 0.0168, p_lose_max: 0.4688, p_lose_std: 0.0624

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 100
eval_loss: 0.6294, accuracy: 0.6484
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.6654, true_2_rate: 0.6310
log joint likelihood: tensor(-1280.8281250000) joint log likelihood: tensor(-1805.3906250000)
r_win_average: 0.2178, r_win_min: -1.4141, r_win_max: 3.3594, r_win_std: 0.6171
r_lose_average: -0.3005, r_lose_min: -1.8281, r_lose_max: 1.0469, r_lose_std: 0.5119
eta_win_average: -0.3565, eta_win_min: -0.7070, eta_win_max: 0.0214, eta_win_std: 0.0683
eta_lose_average: -0.3604, eta_lose_min: -0.5820, eta_lose_max: -0.0820, eta_lose_std: 0.0520
p_win_average: -0.4435, p_win_min: -0.9766, p_win_max: -0.0771, p_win_std: 0.0754
p_lose_average: -0.4502, p_lose_min: -0.9766, p_lose_max: -0.2363, p_lose_std: 0.0660

eval_z_samples_size: 1000
eval_loss: 0.6279, accuracy: 0.6367
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.6538, true_2_rate: 0.6190
log joint likelihood: tensor(-1272.5312500000) joint log likelihood: tensor(-1803.4531250000)
r_win_average: 0.6309, r_win_min: -1.0234, r_win_max: 3.9531, r_win_std: 0.6367
r_lose_average: 0.1151, r_lose_min: -1.2344, r_lose_max: 1.4844, r_lose_std: 0.5052
eta_win_average: -0.1419, eta_win_min: -0.2715, eta_win_max: 0.0136, eta_win_std: 0.0255
eta_lose_average: -0.1470, eta_lose_min: -0.2080, eta_lose_max: -0.0630, eta_lose_std: 0.0212
p_win_average: -0.2469, p_win_min: -0.3398, p_win_max: -0.1162, p_win_std: 0.0217
p_lose_average: -0.2463, p_lose_min: -0.3008, p_lose_max: -0.1992, p_lose_std: 0.0187

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 100
eval_loss: 0.6050, accuracy: 0.6562
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.6846, true_2_rate: 0.6270
log joint likelihood: tensor(-1219.4453125000) joint log likelihood: tensor(-1688.2968750000)
r_win_average: 1.7135, r_win_min: -1.2891, r_win_max: 4.5625, r_win_std: 0.8950
r_lose_average: 0.9452, r_lose_min: -1.7891, r_lose_max: 3.5156, r_lose_std: 0.9045
eta_win_average: 0.3155, eta_win_min: 0.1426, eta_win_max: 0.4414, eta_win_std: 0.0369
eta_lose_average: 0.3194, eta_lose_min: 0.1152, eta_lose_max: 0.4082, eta_lose_std: 0.0391
p_win_average: 0.0262, p_win_min: -0.0791, p_win_max: 0.1426, p_win_std: 0.0399
p_lose_average: 0.0193, p_lose_min: -0.1016, p_lose_max: 0.1719, p_lose_std: 0.0420

eval_z_samples_size: 1000
eval_loss: 0.6055, accuracy: 0.6523
label_1_rate: 0.5078, label_2_rate: 0.4922, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.6808, true_2_rate: 0.6230
log joint likelihood: tensor(-1211.6875000000) joint log likelihood: tensor(-1698.7265625000)
r_win_average: 1.2121, r_win_min: -1.8047, r_win_max: 4.0938, r_win_std: 0.8948
r_lose_average: 0.4422, r_lose_min: -2.2812, r_lose_max: 2.9844, r_lose_std: 0.9061
eta_win_average: -0.2252, eta_win_min: -0.2754, eta_win_max: -0.1807, eta_win_std: 0.0125
eta_lose_average: -0.2281, eta_lose_min: -0.2969, eta_lose_max: -0.1846, eta_lose_std: 0.0142
p_win_average: 0.0649, p_win_min: 0.0294, p_win_max: 0.1211, p_win_std: 0.0157
p_lose_average: 0.0646, p_lose_min: 0.0225, p_lose_max: 0.1128, p_lose_std: 0.0155

------------------------------------------------------------------------------------------
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'hh_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-hh', 'dataset_name': 'anthropic_hh', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'hh_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-hh', 'dataset_name': 'anthropic_hh', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'hh_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-hh', 'dataset_name': 'anthropic_hh', 'ref_size': 10, 'num_ref_train': 1000, 'eval_z_size_list': '100,1000', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'hh_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-hh', 'dataset_name': 'anthropic_hh', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'hh_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-hh', 'dataset_name': 'anthropic_hh', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'hh_new_final', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-hh', 'dataset_name': 'anthropic_hh', 'ref_size': 10, 'num_ref_train': 1000, 'eval_z_size_list': '100,1000', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0001, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
