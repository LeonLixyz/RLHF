[2023-09-25 20:17:42,508] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:17:48,540] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:00,893] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:02,382] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:02,641] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:02,791] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:02,811] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:02,853] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:02,869] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:02,883] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-25 20:18:04,922] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:04,923] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 20:18:07,764] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:07,764] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 20:18:08,264] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:08,264] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 20:18:08,516] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:08,516] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 20:18:08,528] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:08,528] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 20:18:08,528] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-25 20:18:08,537] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:08,537] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 20:18:08,542] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:08,543] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-25 20:18:08,556] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-25 20:18:08,556] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 642
cuda seed 642
torch seed 242
cuda seed 242
torch seed 542
cuda seed 542
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size10-enn_dim64-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 342
cuda seed 342
torch seed 742
cuda seed 742
torch seed 442
cuda seed 442
torch seed 142
cuda seed 142
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-25 20:19:13,882] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-25 20:19:16,747] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-25 20:19:16,748] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-25 20:19:16,749] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-25 20:19:16,754] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-25 20:19:16,754] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-25 20:19:16,754] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-25 20:19:16,756] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-25 20:19:16,756] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-25 20:19:16,756] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-25 20:19:16,756] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
[2023-09-25 20:19:30,040] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-25 20:19:30,040] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-25 20:19:30,042] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 66.29 GB, percent = 6.6%
[2023-09-25 20:19:30,240] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-25 20:19:30,240] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 20:19:30,244] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 66.29 GB, percent = 6.6%
[2023-09-25 20:19:30,244] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-25 20:19:30,428] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-25 20:19:30,429] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-25 20:19:30,430] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 66.29 GB, percent = 6.6%
[2023-09-25 20:19:30,432] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-25 20:19:30,432] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-25 20:19:30,433] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-25 20:19:30,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-25 20:19:30,436] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-25 20:19:30,436] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-25 20:19:30,437] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-25 20:19:30,438] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-25 20:19:30,438] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-25 20:19:30,439] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-25 20:19:30,440] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-25 20:19:30,442] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-25 20:19:30,443] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-25 20:19:30,443] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-25 20:19:30,444] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa59fd63510>
[2023-09-25 20:19:30,444] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-25 20:19:30,445] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-25 20:19:30,446] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-25 20:19:30,447] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-25 20:19:30,447] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-25 20:19:30,449] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-25 20:19:30,449] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-25 20:19:30,450] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-25 20:19:30,451] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-25 20:19:30,451] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-25 20:19:30,452] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-25 20:19:30,453] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-25 20:19:30,453] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-25 20:19:30,454] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-25 20:19:30,454] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-25 20:19:30,455] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-25 20:19:30,456] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-25 20:19:30,457] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-25 20:19:30,457] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-25 20:19:30,458] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-25 20:19:30,459] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-25 20:19:30,460] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-25 20:19:30,461] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-25 20:19:30,462] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-25 20:19:30,463] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-25 20:19:30,463] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-25 20:19:30,464] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-25 20:19:30,465] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-25 20:19:30,465] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-25 20:19:30,465] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-25 20:19:30,467] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-25 20:19:30,468] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-25 20:19:30,468] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-25 20:19:30,469] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-25 20:19:30,469] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-25 20:19:30,470] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-25 20:19:30,472] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-25 20:19:30,472] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-25 20:19:30,473] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-25 20:19:30,475] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-25 20:19:30,476] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-25 20:19:30,476] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-25 20:19:30,476] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-25 20:19:30,478] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-25 20:19:30,479] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-25 20:19:30,480] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-25 20:19:30,481] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-25 20:19:30,481] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-25 20:19:30,482] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-25 20:19:30,482] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-25 20:19:30,483] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-25 20:19:30,483] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-25 20:19:30,484] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-25 20:19:30,484] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-25 20:19:30,485] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-25 20:19:30,486] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-25 20:19:30,487] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-25 20:19:30,488] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-25 20:19:30,489] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-25 20:19:30,490] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 500
eval_loss: 0.7618, accuracy: 0.5312
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.6973, prediction_2_rate: 0.3027
true_1_rate: 0.7175, true_2_rate: 0.3251
log joint likelihood: tensor(-902.2265625000) joint log likelihood: tensor(-7961.2500000000)
r_win_average: 0.4214, r_win_min: -2.3906, r_win_max: 5.5625, r_win_std: 0.9131
eta_win_average: 0.0520, eta_win_min: -0.1895, eta_win_max: 0.3770, eta_win_std: 0.0609
p_win_average: 0.0942, p_win_min: -0.2002, p_win_max: 0.2949, p_win_std: 0.0597

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 500
eval_loss: 0.2522, accuracy: 0.9121
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5391, prediction_2_rate: 0.4609
true_1_rate: 0.9294, true_2_rate: 0.8930
log joint likelihood: tensor(-257.4755859375) joint log likelihood: tensor(-4017.2187500000)
r_win_average: -0.1326, r_win_min: -14.4375, r_win_max: 10.8750, r_win_std: 7.8509
eta_win_average: -0.3674, eta_win_min: -0.5781, eta_win_max: 0.0669, eta_win_std: 0.1141
p_win_average: 0.0854, p_win_min: -0.2871, p_win_max: 0.3203, p_win_std: 0.0755

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 500
eval_loss: 0.1846, accuracy: 0.9414
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5449, prediction_2_rate: 0.4551
true_1_rate: 0.9628, true_2_rate: 0.9177
log joint likelihood: tensor(-239.7304687500) joint log likelihood: tensor(-2158.9453125000)
r_win_average: 0.9859, r_win_min: -8.5625, r_win_max: 11.3750, r_win_std: 5.4770
eta_win_average: 0.2957, eta_win_min: -0.1240, eta_win_max: 0.5547, eta_win_std: 0.1074
p_win_average: -0.1503, p_win_min: -0.4863, p_win_max: 0.3164, p_win_std: 0.1079

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 500
eval_loss: 0.2302, accuracy: 0.9258
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4785, prediction_2_rate: 0.5215
true_1_rate: 0.8848, true_2_rate: 0.9712
log joint likelihood: tensor(-251.0605468750) joint log likelihood: tensor(-1113.3579101562)
r_win_average: -1.5941, r_win_min: -11.1250, r_win_max: 8.8125, r_win_std: 5.2036
eta_win_average: -0.0702, eta_win_min: -0.2021, eta_win_max: 0.3047, eta_win_std: 0.0634
p_win_average: 0.1384, p_win_min: -0.1338, p_win_max: 0.3359, p_win_std: 0.0693

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 500
eval_loss: 0.1598, accuracy: 0.9453
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5371, prediction_2_rate: 0.4629
true_1_rate: 0.9591, true_2_rate: 0.9300
log joint likelihood: tensor(-222.5180664062) joint log likelihood: tensor(-1510.7412109375)
r_win_average: -0.0320, r_win_min: -9.3125, r_win_max: 9.3125, r_win_std: 5.3611
eta_win_average: 0.5323, eta_win_min: 0.0723, eta_win_max: 0.6641, eta_win_std: 0.0776
p_win_average: -0.2265, p_win_min: -0.5273, p_win_max: 0.0474, p_win_std: 0.0696

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 500
eval_loss: 0.1427, accuracy: 0.9492
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5410, prediction_2_rate: 0.4590
true_1_rate: 0.9665, true_2_rate: 0.9300
log joint likelihood: tensor(-196.4624023438) joint log likelihood: tensor(-1588.3291015625)
r_win_average: 0.1835, r_win_min: -11.1250, r_win_max: 11.0000, r_win_std: 6.1771
eta_win_average: -0.2775, eta_win_min: -0.3379, eta_win_max: 0.0189, eta_win_std: 0.0557
p_win_average: 0.1705, p_win_min: -0.0664, p_win_max: 0.3359, p_win_std: 0.0583

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 500
eval_loss: 0.1353, accuracy: 0.9570
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5332, prediction_2_rate: 0.4668
true_1_rate: 0.9665, true_2_rate: 0.9465
log joint likelihood: tensor(-191.3859863281) joint log likelihood: tensor(-1065.2402343750)
r_win_average: 0.1049, r_win_min: -10.2500, r_win_max: 10.7500, r_win_std: 5.6449
eta_win_average: -0.1011, eta_win_min: -0.1514, eta_win_max: 0.0299, eta_win_std: 0.0240
p_win_average: 0.0262, p_win_min: -0.2812, p_win_max: 0.2334, p_win_std: 0.0815

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 500
eval_loss: 0.1585, accuracy: 0.9492
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5410, prediction_2_rate: 0.4590
true_1_rate: 0.9665, true_2_rate: 0.9300
log joint likelihood: tensor(-172.0590820312) joint log likelihood: tensor(-1139.4895019531)
r_win_average: 0.9254, r_win_min: -10.3125, r_win_max: 11.2500, r_win_std: 6.2448
eta_win_average: 0.2992, eta_win_min: 0.1436, eta_win_max: 0.3457, eta_win_std: 0.0280
p_win_average: -0.0940, p_win_min: -0.2871, p_win_max: 0.1279, p_win_std: 0.0525

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 500
eval_loss: 0.1381, accuracy: 0.9551
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.9591, true_2_rate: 0.9506
log joint likelihood: tensor(-161.3642578125) joint log likelihood: tensor(-778.1289062500)
r_win_average: -0.2763, r_win_min: -10.6875, r_win_max: 10.1250, r_win_std: 6.1149
eta_win_average: -0.0063, eta_win_min: -0.0503, eta_win_max: 0.0540, eta_win_std: 0.0118
p_win_average: 0.0715, p_win_min: -0.0938, p_win_max: 0.1953, p_win_std: 0.0486

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 500
eval_loss: 0.1362, accuracy: 0.9531
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.9405, true_2_rate: 0.9671
log joint likelihood: tensor(-202.6831054688) joint log likelihood: tensor(-757.4257812500)
r_win_average: -0.3486, r_win_min: -8.5625, r_win_max: 8.0000, r_win_std: 4.7077
eta_win_average: 0.0284, eta_win_min: -0.0339, eta_win_max: 0.1240, eta_win_std: 0.0188
p_win_average: -0.0065, p_win_min: -0.3047, p_win_max: 0.2676, p_win_std: 0.0783

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 500
eval_loss: 0.1527, accuracy: 0.9570
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.9368, true_2_rate: 0.9794
log joint likelihood: tensor(-168.5793457031) joint log likelihood: tensor(-691.4506835938)
r_win_average: -0.7846, r_win_min: -10.6875, r_win_max: 9.8750, r_win_std: 6.0456
eta_win_average: -0.2616, eta_win_min: -0.2988, eta_win_max: -0.0703, eta_win_std: 0.0312
p_win_average: 0.1780, p_win_min: -0.2070, p_win_max: 0.3164, p_win_std: 0.0723

------------------------------------------------------------------------------------------
epoch 1 step 330 evaluation
eval_z_samples_size: 500
eval_loss: 0.1398, accuracy: 0.9570
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.9591, true_2_rate: 0.9547
log joint likelihood: tensor(-162.3034667969) joint log likelihood: tensor(-672.2773437500)
r_win_average: 0.3497, r_win_min: -8.9375, r_win_max: 12.2500, r_win_std: 6.1743
eta_win_average: 0.2515, eta_win_min: -0.0073, eta_win_max: 0.3730, eta_win_std: 0.0478
p_win_average: -0.0384, p_win_min: -0.2969, p_win_max: 0.1279, p_win_std: 0.0678

------------------------------------------------------------------------------------------
epoch 1 step 360 evaluation
eval_z_samples_size: 500
eval_loss: 0.1455, accuracy: 0.9551
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5352, prediction_2_rate: 0.4648
true_1_rate: 0.9665, true_2_rate: 0.9424
log joint likelihood: tensor(-239.1049804688) joint log likelihood: tensor(-1047.0678710938)
r_win_average: 1.4687, r_win_min: -5.7500, r_win_max: 11.0000, r_win_std: 4.5978
eta_win_average: -0.1209, eta_win_min: -0.1738, eta_win_max: -0.0413, eta_win_std: 0.0132
p_win_average: 0.0759, p_win_min: -0.0464, p_win_max: 0.1768, p_win_std: 0.0330

------------------------------------------------------------------------------------------
epoch 1 step 390 evaluation
eval_z_samples_size: 500
eval_loss: 0.1431, accuracy: 0.9590
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.9554, true_2_rate: 0.9630
log joint likelihood: tensor(-187.6397094727) joint log likelihood: tensor(-643.2526245117)
r_win_average: 0.4382, r_win_min: -9.0000, r_win_max: 10.1875, r_win_std: 6.1844
eta_win_average: -0.2435, eta_win_min: -0.3145, eta_win_max: -0.1167, eta_win_std: 0.0318
p_win_average: 0.1491, p_win_min: 0.0214, p_win_max: 0.2656, p_win_std: 0.0395

------------------------------------------------------------------------------------------
epoch 1 step 420 evaluation
eval_z_samples_size: 500
eval_loss: 0.1349, accuracy: 0.9531
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5371, prediction_2_rate: 0.4629
true_1_rate: 0.9665, true_2_rate: 0.9383
log joint likelihood: tensor(-195.6322021484) joint log likelihood: tensor(-617.9248046875)
r_win_average: 1.2310, r_win_min: -7.5625, r_win_max: 9.7500, r_win_std: 5.2455
eta_win_average: -0.0638, eta_win_min: -0.1348, eta_win_max: 0.1484, eta_win_std: 0.0324
p_win_average: 0.1218, p_win_min: -0.2520, p_win_max: 0.2461, p_win_std: 0.0657

------------------------------------------------------------------------------------------
epoch 1 step 450 evaluation
eval_z_samples_size: 500
eval_loss: 0.1309, accuracy: 0.9531
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5293, prediction_2_rate: 0.4707
true_1_rate: 0.9591, true_2_rate: 0.9465
log joint likelihood: tensor(-186.8045654297) joint log likelihood: tensor(-640.3464355469)
r_win_average: 0.7927, r_win_min: -7.8438, r_win_max: 9.0000, r_win_std: 5.3156
eta_win_average: 0.0681, eta_win_min: -0.0364, eta_win_max: 0.0967, eta_win_std: 0.0141
p_win_average: -0.0429, p_win_min: -0.1602, p_win_max: 0.1030, p_win_std: 0.0596

------------------------------------------------------------------------------------------
epoch 1 step 480 evaluation
eval_z_samples_size: 500
eval_loss: 0.1249, accuracy: 0.9570
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.9517, true_2_rate: 0.9630
log joint likelihood: tensor(-161.5942382812) joint log likelihood: tensor(-641.4975585938)
r_win_average: -0.7217, r_win_min: -11.7500, r_win_max: 8.4375, r_win_std: 6.2685
eta_win_average: -0.0159, eta_win_min: -0.0718, eta_win_max: 0.1299, eta_win_std: 0.0304
p_win_average: -0.1475, p_win_min: -0.2617, p_win_max: -0.0503, p_win_std: 0.0323

------------------------------------------------------------------------------------------
epoch 1 step 510 evaluation
eval_z_samples_size: 500
eval_loss: 0.1194, accuracy: 0.9590
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.9628, true_2_rate: 0.9547
log joint likelihood: tensor(-173.1142578125) joint log likelihood: tensor(-577.2268066406)
r_win_average: -0.1984, r_win_min: -10.3125, r_win_max: 8.3750, r_win_std: 5.6052
eta_win_average: 0.0449, eta_win_min: 0.0194, eta_win_max: 0.1152, eta_win_std: 0.0086
p_win_average: -0.1494, p_win_min: -0.3438, p_win_max: 0.0408, p_win_std: 0.0903

------------------------------------------------------------------------------------------
epoch 1 step 540 evaluation
eval_z_samples_size: 500
eval_loss: 0.1170, accuracy: 0.9590
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.9554, true_2_rate: 0.9630
log joint likelihood: tensor(-162.7202758789) joint log likelihood: tensor(-496.3851318359)
r_win_average: -0.6722, r_win_min: -11.4375, r_win_max: 7.5938, r_win_std: 5.9473
eta_win_average: 0.0213, eta_win_min: -0.1123, eta_win_max: 0.0583, eta_win_std: 0.0226
p_win_average: -0.0386, p_win_min: -0.2637, p_win_max: 0.1504, p_win_std: 0.0958

------------------------------------------------------------------------------------------
epoch 1 step 570 evaluation
eval_z_samples_size: 500
eval_loss: 0.1119, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.9554, true_2_rate: 0.9671
log joint likelihood: tensor(-148.4735107422) joint log likelihood: tensor(-445.1127319336)
r_win_average: -0.6980, r_win_min: -12.5625, r_win_max: 8.5625, r_win_std: 6.3735
eta_win_average: 0.0514, eta_win_min: 0.0134, eta_win_max: 0.0664, eta_win_std: 0.0045
p_win_average: -0.0599, p_win_min: -0.1406, p_win_max: 0.0312, p_win_std: 0.0304

------------------------------------------------------------------------------------------
epoch 1 step 600 evaluation
eval_z_samples_size: 500
eval_loss: 0.1173, accuracy: 0.9590
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.9517, true_2_rate: 0.9671
log joint likelihood: tensor(-152.3577270508) joint log likelihood: tensor(-472.7322998047)
r_win_average: -0.2893, r_win_min: -11.6875, r_win_max: 9.2500, r_win_std: 6.3708
eta_win_average: -0.1617, eta_win_min: -0.2178, eta_win_max: -0.0347, eta_win_std: 0.0286
p_win_average: 0.1661, p_win_min: 0.0364, p_win_max: 0.3320, p_win_std: 0.0596

------------------------------------------------------------------------------------------
epoch 1 step 630 evaluation
eval_z_samples_size: 500
eval_loss: 0.1107, accuracy: 0.9629
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.9665, true_2_rate: 0.9588
log joint likelihood: tensor(-171.0991210938) joint log likelihood: tensor(-482.4689941406)
r_win_average: 0.0322, r_win_min: -10.1875, r_win_max: 8.5000, r_win_std: 5.5508
eta_win_average: -0.0557, eta_win_min: -0.0684, eta_win_max: -0.0007, eta_win_std: 0.0067
p_win_average: -0.0066, p_win_min: -0.1240, p_win_max: 0.1011, p_win_std: 0.0436

------------------------------------------------------------------------------------------
epoch 1 step 660 evaluation
eval_z_samples_size: 500
eval_loss: 0.1164, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.9517, true_2_rate: 0.9712
log joint likelihood: tensor(-169.5515136719) joint log likelihood: tensor(-477.1552734375)
r_win_average: -0.7910, r_win_min: -11.6250, r_win_max: 8.4375, r_win_std: 6.0179
eta_win_average: -0.0901, eta_win_min: -0.1079, eta_win_max: -0.0442, eta_win_std: 0.0086
p_win_average: 0.2398, p_win_min: -0.1533, p_win_max: 0.3828, p_win_std: 0.0650

------------------------------------------------------------------------------------------
epoch 1 step 690 evaluation
eval_z_samples_size: 500
eval_loss: 0.1138, accuracy: 0.9629
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.9591, true_2_rate: 0.9671
log joint likelihood: tensor(-159.9525146484) joint log likelihood: tensor(-461.5135498047)
r_win_average: -0.8281, r_win_min: -11.9375, r_win_max: 8.3750, r_win_std: 6.0911
eta_win_average: 0.0087, eta_win_min: -0.0124, eta_win_max: 0.0469, eta_win_std: 0.0106
p_win_average: -0.0746, p_win_min: -0.2168, p_win_max: 0.1299, p_win_std: 0.0748

------------------------------------------------------------------------------------------
epoch 1 step 720 evaluation
eval_z_samples_size: 500
eval_loss: 0.1088, accuracy: 0.9648
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.9628, true_2_rate: 0.9671
log joint likelihood: tensor(-159.2131347656) joint log likelihood: tensor(-469.5036621094)
r_win_average: -0.3119, r_win_min: -10.7500, r_win_max: 8.4375, r_win_std: 5.5736
eta_win_average: 0.0632, eta_win_min: 0.0454, eta_win_max: 0.0864, eta_win_std: 0.0086
p_win_average: -0.0204, p_win_min: -0.1260, p_win_max: 0.0493, p_win_std: 0.0281

------------------------------------------------------------------------------------------
epoch 1 step 750 evaluation
eval_z_samples_size: 500
eval_loss: 0.1109, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.9628, true_2_rate: 0.9588
log joint likelihood: tensor(-156.8482055664) joint log likelihood: tensor(-471.7709350586)
r_win_average: 0.0169, r_win_min: -11.0000, r_win_max: 9.1250, r_win_std: 5.9631
eta_win_average: 0.0120, eta_win_min: -0.0069, eta_win_max: 0.1030, eta_win_std: 0.0140
p_win_average: 0.0538, p_win_min: -0.0762, p_win_max: 0.1777, p_win_std: 0.0337

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 500
eval_loss: 0.1138, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.9554, true_2_rate: 0.9671
log joint likelihood: tensor(-170.7048339844) joint log likelihood: tensor(-472.1336669922)
r_win_average: -0.6278, r_win_min: -11.1875, r_win_max: 8.4375, r_win_std: 5.8291
eta_win_average: -0.0970, eta_win_min: -0.1348, eta_win_max: -0.0320, eta_win_std: 0.0183
p_win_average: 0.1211, p_win_min: -0.1260, p_win_max: 0.2773, p_win_std: 0.0639

------------------------------------------------------------------------------------------
[2023-09-26 03:05:55,357] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:17,448] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:17,592] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:17,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:17,854] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:17,936] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:17,944] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:17,961] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:18,010] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 03:06:25,259] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,259] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 03:06:25,400] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,400] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 03:06:25,624] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,624] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 03:06:25,624] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 03:06:25,774] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,774] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 03:06:25,822] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,822] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 03:06:25,860] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,860] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 03:06:25,867] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,867] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 03:06:25,871] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 03:06:25,871] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 542
cuda seed 542
torch seed 642
cuda seed 642
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size10-enn_dim64-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 242
cuda seed 242
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
torch seed 442
cuda seed 442
torch seed 742
cuda seed 742
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 03:07:32,438] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 03:07:34,644] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 03:07:34,645] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 03:07:34,645] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 03:07:34,650] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 03:07:34,650] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 03:07:34,650] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 03:07:34,650] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 03:07:34,650] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 03:07:34,650] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 03:07:34,650] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26290, False)] 
[2023-09-26 03:07:46,591] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 03:07:46,592] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 03:07:46,594] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.77 GB, percent = 7.8%
[2023-09-26 03:07:46,765] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 03:07:46,766] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 03:07:46,767] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.77 GB, percent = 7.8%
[2023-09-26 03:07:46,769] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 03:07:46,928] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 03:07:46,929] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 03:07:46,930] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 78.77 GB, percent = 7.8%
[2023-09-26 03:07:46,933] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 03:07:46,933] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 03:07:46,935] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 03:07:46,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 03:07:46,938] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 03:07:46,939] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 03:07:46,940] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 03:07:46,941] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 03:07:46,941] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 03:07:46,943] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 03:07:46,944] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 03:07:46,944] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 03:07:46,945] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 03:07:46,945] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 03:07:46,946] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f8d23b55d90>
[2023-09-26 03:07:46,947] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 03:07:46,947] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 03:07:46,948] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 03:07:46,948] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 03:07:46,948] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 03:07:46,950] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 03:07:46,950] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 03:07:46,950] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 03:07:46,951] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 03:07:46,951] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 03:07:46,952] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 03:07:46,952] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 03:07:46,954] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 03:07:46,954] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 03:07:46,955] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 03:07:46,956] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 03:07:46,958] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 03:07:46,959] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 03:07:46,960] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 03:07:46,961] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 03:07:46,962] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 03:07:46,963] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 03:07:46,964] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 03:07:46,965] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 03:07:46,966] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 03:07:46,968] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 03:07:46,969] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 03:07:46,970] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 03:07:46,971] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 03:07:46,972] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 03:07:46,973] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 03:07:46,974] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 03:07:46,975] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 03:07:46,980] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 03:07:46,982] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 03:07:46,983] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 03:07:46,984] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 03:07:46,986] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 03:07:46,987] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 03:07:46,988] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 03:07:46,989] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 03:07:46,991] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 03:07:46,992] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 03:07:46,993] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 03:07:46,994] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 03:07:46,995] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 03:07:46,995] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 03:07:46,996] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 03:07:46,998] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 03:07:46,999] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 03:07:47,000] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 03:07:47,001] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 03:07:47,002] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 03:07:47,003] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 03:07:47,004] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 03:07:47,005] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 03:07:47,006] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 03:07:47,009] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 03:07:47,011] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 03:07:47,012] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 500
eval_loss: 0.7618, accuracy: 0.5312
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.6973, prediction_2_rate: 0.3027
true_1_rate: 0.7175, true_2_rate: 0.3251
log joint likelihood: tensor(-902.2265625000) joint log likelihood: tensor(-7961.2500000000)
r_win_average: 0.4214, r_win_min: -2.3906, r_win_max: 5.5625, r_win_std: 0.9131
eta_win_average: 0.0520, eta_win_min: -0.1895, eta_win_max: 0.3770, eta_win_std: 0.0609
p_win_average: 0.0942, p_win_min: -0.2002, p_win_max: 0.2949, p_win_std: 0.0597

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 500
eval_loss: 0.2336, accuracy: 0.9160
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5391, prediction_2_rate: 0.4609
true_1_rate: 0.9331, true_2_rate: 0.8971
log joint likelihood: tensor(-360.7226562500) joint log likelihood: tensor(-5669.)
r_win_average: 0.1553, r_win_min: -8.3125, r_win_max: 7.8750, r_win_std: 4.7072
eta_win_average: 0.2871, eta_win_min: -0.5391, eta_win_max: 0.5820, eta_win_std: 0.1267
p_win_average: 0.0328, p_win_min: -0.5078, p_win_max: 0.2188, p_win_std: 0.1041

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 500
eval_loss: 0.2722, accuracy: 0.9258
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4668, prediction_2_rate: 0.5332
true_1_rate: 0.8736, true_2_rate: 0.9835
log joint likelihood: tensor(-305.9179687500) joint log likelihood: tensor(-6276.0625000000)
r_win_average: -2.3768, r_win_min: -13.7500, r_win_max: 8.5625, r_win_std: 6.5833
eta_win_average: -0.8022, eta_win_min: -1.0703, eta_win_max: 0.3066, eta_win_std: 0.2006
p_win_average: -0.3009, p_win_min: -0.7031, p_win_max: 0.2002, p_win_std: 0.1251

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 500
eval_loss: 0.1735, accuracy: 0.9414
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.9182, true_2_rate: 0.9671
log joint likelihood: tensor(-321.4687500000) joint log likelihood: tensor(-5146.3125000000)
r_win_average: -1.1941, r_win_min: -10.0625, r_win_max: 9.8750, r_win_std: 5.0406
eta_win_average: -0.9326, eta_win_min: -1.0703, eta_win_max: -0.1035, eta_win_std: 0.1228
p_win_average: 0.3264, p_win_min: 0.0254, p_win_max: 0.5000, p_win_std: 0.0786

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 500
eval_loss: 0.1966, accuracy: 0.9297
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5605, prediction_2_rate: 0.4395
true_1_rate: 0.9665, true_2_rate: 0.8889
log joint likelihood: tensor(-289.2929687500) joint log likelihood: tensor(-3941.7187500000)
r_win_average: 0.9160, r_win_min: -9.1250, r_win_max: 10.3125, r_win_std: 5.3153
eta_win_average: 0.6364, eta_win_min: 0.2031, eta_win_max: 0.8047, eta_win_std: 0.0525
p_win_average: -0.1054, p_win_min: -0.2930, p_win_max: 0.1768, p_win_std: 0.0473

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 500
eval_loss: 0.1484, accuracy: 0.9531
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.9517, true_2_rate: 0.9547
log joint likelihood: tensor(-249.9472656250) joint log likelihood: tensor(-3047.2812500000)
r_win_average: -0.4497, r_win_min: -9.7500, r_win_max: 10.5000, r_win_std: 5.6606
eta_win_average: -0.0258, eta_win_min: -0.1562, eta_win_max: 0.2197, eta_win_std: 0.0426
p_win_average: -0.0412, p_win_min: -0.1963, p_win_max: 0.0649, p_win_std: 0.0423

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 500
eval_loss: 0.1563, accuracy: 0.9453
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5566, prediction_2_rate: 0.4434
true_1_rate: 0.9777, true_2_rate: 0.9095
log joint likelihood: tensor(-315.0488281250) joint log likelihood: tensor(-5450.5000000000)
r_win_average: 0.7254, r_win_min: -9.6250, r_win_max: 11.0625, r_win_std: 5.6390
eta_win_average: 0.5202, eta_win_min: 0.0698, eta_win_max: 0.6289, eta_win_std: 0.0671
p_win_average: -0.0867, p_win_min: -0.2256, p_win_max: 0.1143, p_win_std: 0.0578

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 500
eval_loss: 0.1441, accuracy: 0.9570
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5410, prediction_2_rate: 0.4590
true_1_rate: 0.9740, true_2_rate: 0.9383
log joint likelihood: tensor(-263.4687500000) joint log likelihood: tensor(-3840.6718750000)
r_win_average: 0.6476, r_win_min: -9.7500, r_win_max: 11.3750, r_win_std: 5.6906
eta_win_average: 0.0893, eta_win_min: -0.0476, eta_win_max: 0.3906, eta_win_std: 0.0462
p_win_average: -0.1580, p_win_min: -0.2754, p_win_max: 0.1182, p_win_std: 0.0610

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 500
eval_loss: 0.1297, accuracy: 0.9648
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5332, prediction_2_rate: 0.4668
true_1_rate: 0.9740, true_2_rate: 0.9547
log joint likelihood: tensor(-272.3359375000) joint log likelihood: tensor(-4853.0937500000)
r_win_average: 0.0731, r_win_min: -10.1250, r_win_max: 11.0000, r_win_std: 6.1790
eta_win_average: 0.2967, eta_win_min: 0.1514, eta_win_max: 0.4551, eta_win_std: 0.0287
p_win_average: -0.0331, p_win_min: -0.1709, p_win_max: 0.1875, p_win_std: 0.0470

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 500
eval_loss: 0.1233, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.9628, true_2_rate: 0.9588
log joint likelihood: tensor(-299.4746093750) joint log likelihood: tensor(-4072.1250000000)
r_win_average: 0.1047, r_win_min: -8.4375, r_win_max: 9.1250, r_win_std: 4.7891
eta_win_average: 0.0286, eta_win_min: -0.1807, eta_win_max: 0.1035, eta_win_std: 0.0496
p_win_average: 0.2379, p_win_min: -0.0002, p_win_max: 0.3496, p_win_std: 0.0524

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 500
eval_loss: 0.1270, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.9480, true_2_rate: 0.9753
log joint likelihood: tensor(-220.5615234375) joint log likelihood: tensor(-2360.6484375000)
r_win_average: -0.1419, r_win_min: -10.1250, r_win_max: 12.4375, r_win_std: 6.2217
eta_win_average: 0.1068, eta_win_min: -0.0547, eta_win_max: 0.1572, eta_win_std: 0.0268
p_win_average: 0.1529, p_win_min: -0.0588, p_win_max: 0.4297, p_win_std: 0.0626

------------------------------------------------------------------------------------------
epoch 1 step 330 evaluation
eval_z_samples_size: 500
eval_loss: 0.1316, accuracy: 0.9648
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.9591, true_2_rate: 0.9712
log joint likelihood: tensor(-219.2558593750) joint log likelihood: tensor(-2452.4140625000)
r_win_average: -0.0456, r_win_min: -8.8125, r_win_max: 13.4375, r_win_std: 6.0630
eta_win_average: -0.1605, eta_win_min: -0.2109, eta_win_max: -0.0209, eta_win_std: 0.0235
p_win_average: -0.1898, p_win_min: -0.4727, p_win_max: -0.0654, p_win_std: 0.0435

------------------------------------------------------------------------------------------
epoch 1 step 360 evaluation
eval_z_samples_size: 500
eval_loss: 0.1569, accuracy: 0.9453
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5566, prediction_2_rate: 0.4434
true_1_rate: 0.9777, true_2_rate: 0.9095
log joint likelihood: tensor(-288.2480468750) joint log likelihood: tensor(-2876.5078125000)
r_win_average: 1.7868, r_win_min: -5.4688, r_win_max: 10.8750, r_win_std: 4.5053
eta_win_average: 0.1524, eta_win_min: 0.0820, eta_win_max: 0.1846, eta_win_std: 0.0155
p_win_average: -0.0333, p_win_min: -0.1885, p_win_max: 0.0923, p_win_std: 0.0409

------------------------------------------------------------------------------------------
epoch 1 step 390 evaluation
eval_z_samples_size: 500
eval_loss: 0.1274, accuracy: 0.9590
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.9517, true_2_rate: 0.9671
log joint likelihood: tensor(-194.9682617188) joint log likelihood: tensor(-1696.9687500000)
r_win_average: 0.2649, r_win_min: -9.0625, r_win_max: 10.0625, r_win_std: 6.1661
eta_win_average: -0.1237, eta_win_min: -0.1914, eta_win_max: -0.0615, eta_win_std: 0.0222
p_win_average: -0.1979, p_win_min: -0.3125, p_win_max: 0.0140, p_win_std: 0.0557

------------------------------------------------------------------------------------------
epoch 1 step 420 evaluation
eval_z_samples_size: 500
eval_loss: 0.1138, accuracy: 0.9629
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.9665, true_2_rate: 0.9588
log joint likelihood: tensor(-209.0073242188) joint log likelihood: tensor(-1692.7597656250)
r_win_average: 1.1484, r_win_min: -7.5938, r_win_max: 10.5625, r_win_std: 5.4730
eta_win_average: -0.0598, eta_win_min: -0.1377, eta_win_max: 0.0864, eta_win_std: 0.0287
p_win_average: -0.0505, p_win_min: -0.1689, p_win_max: 0.0352, p_win_std: 0.0386

------------------------------------------------------------------------------------------
epoch 1 step 450 evaluation
eval_z_samples_size: 500
eval_loss: 0.1216, accuracy: 0.9590
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5391, prediction_2_rate: 0.4609
true_1_rate: 0.9740, true_2_rate: 0.9424
log joint likelihood: tensor(-198.4562988281) joint log likelihood: tensor(-1331.7265625000)
r_win_average: 0.7456, r_win_min: -8.0000, r_win_max: 8.8125, r_win_std: 5.5111
eta_win_average: -0.0523, eta_win_min: -0.1069, eta_win_max: -0.0153, eta_win_std: 0.0159
p_win_average: 0.0757, p_win_min: -0.1621, p_win_max: 0.2988, p_win_std: 0.0468

------------------------------------------------------------------------------------------
epoch 1 step 480 evaluation
eval_z_samples_size: 500
eval_loss: 0.1102, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.9628, true_2_rate: 0.9588
log joint likelihood: tensor(-158.8437500000) joint log likelihood: tensor(-998.0380859375)
r_win_average: -0.0801, r_win_min: -11.3125, r_win_max: 9.2500, r_win_std: 6.6359
eta_win_average: 0.0347, eta_win_min: -0.0554, eta_win_max: 0.0884, eta_win_std: 0.0186
p_win_average: 0.1503, p_win_min: -0.0430, p_win_max: 0.3105, p_win_std: 0.0587

------------------------------------------------------------------------------------------
epoch 1 step 510 evaluation
eval_z_samples_size: 500
eval_loss: 0.1077, accuracy: 0.9688
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.9665, true_2_rate: 0.9712
log joint likelihood: tensor(-186.6425781250) joint log likelihood: tensor(-1093.6542968750)
r_win_average: -0.6987, r_win_min: -11.2500, r_win_max: 8.0000, r_win_std: 5.8706
eta_win_average: -0.1308, eta_win_min: -0.1670, eta_win_max: -0.0569, eta_win_std: 0.0222
p_win_average: -0.0368, p_win_min: -0.1973, p_win_max: 0.2051, p_win_std: 0.0882

------------------------------------------------------------------------------------------
epoch 1 step 540 evaluation
eval_z_samples_size: 500
eval_loss: 0.1078, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.9628, true_2_rate: 0.9588
log joint likelihood: tensor(-179.2722167969) joint log likelihood: tensor(-1051.6523437500)
r_win_average: -0.6871, r_win_min: -11.4375, r_win_max: 7.2812, r_win_std: 5.9093
eta_win_average: -0.0640, eta_win_min: -0.1055, eta_win_max: 0.0552, eta_win_std: 0.0202
p_win_average: -0.0810, p_win_min: -0.1807, p_win_max: 0.0635, p_win_std: 0.0422

------------------------------------------------------------------------------------------
epoch 1 step 570 evaluation
eval_z_samples_size: 500
eval_loss: 0.1004, accuracy: 0.9648
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.9591, true_2_rate: 0.9712
log joint likelihood: tensor(-148.8681640625) joint log likelihood: tensor(-829.3730468750)
r_win_average: -0.8015, r_win_min: -13.0000, r_win_max: 8.7500, r_win_std: 6.6176
eta_win_average: -0.0472, eta_win_min: -0.1270, eta_win_max: 0.0361, eta_win_std: 0.0268
p_win_average: 0.1421, p_win_min: -0.0080, p_win_max: 0.2373, p_win_std: 0.0339

------------------------------------------------------------------------------------------
epoch 1 step 600 evaluation
eval_z_samples_size: 500
eval_loss: 0.1093, accuracy: 0.9668
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.9665, true_2_rate: 0.9671
log joint likelihood: tensor(-156.1105957031) joint log likelihood: tensor(-795.2314453125)
r_win_average: 0.0893, r_win_min: -10.8750, r_win_max: 9.5000, r_win_std: 6.1213
eta_win_average: 0.0127, eta_win_min: -0.0669, eta_win_max: 0.1279, eta_win_std: 0.0376
p_win_average: 0.1625, p_win_min: 0.0315, p_win_max: 0.3008, p_win_std: 0.0519

------------------------------------------------------------------------------------------
epoch 1 step 630 evaluation
eval_z_samples_size: 500
eval_loss: 0.1074, accuracy: 0.9668
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.9740, true_2_rate: 0.9588
log joint likelihood: tensor(-184.5336914062) joint log likelihood: tensor(-924.5800781250)
r_win_average: 0.3307, r_win_min: -9.7500, r_win_max: 8.6875, r_win_std: 5.4657
eta_win_average: 0.0657, eta_win_min: 0.0123, eta_win_max: 0.1055, eta_win_std: 0.0141
p_win_average: 0.0602, p_win_min: -0.0977, p_win_max: 0.2676, p_win_std: 0.0766

------------------------------------------------------------------------------------------
epoch 1 step 660 evaluation
eval_z_samples_size: 500
eval_loss: 0.1081, accuracy: 0.9648
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.9591, true_2_rate: 0.9712
log joint likelihood: tensor(-173.4497070312) joint log likelihood: tensor(-897.5800781250)
r_win_average: -0.6950, r_win_min: -11.6250, r_win_max: 8.2500, r_win_std: 5.9303
eta_win_average: 0.0668, eta_win_min: -0.0027, eta_win_max: 0.0942, eta_win_std: 0.0144
p_win_average: 0.0026, p_win_min: -0.0762, p_win_max: 0.1206, p_win_std: 0.0334

------------------------------------------------------------------------------------------
epoch 1 step 690 evaluation
eval_z_samples_size: 500
eval_loss: 0.1159, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.9480, true_2_rate: 0.9753
log joint likelihood: tensor(-170.8867187500) joint log likelihood: tensor(-917.5292968750)
r_win_average: -1.2196, r_win_min: -12.8125, r_win_max: 8.1250, r_win_std: 6.1961
eta_win_average: -0.2873, eta_win_min: -0.3301, eta_win_max: -0.1729, eta_win_std: 0.0247
p_win_average: -0.0075, p_win_min: -0.2393, p_win_max: 0.1660, p_win_std: 0.0760

------------------------------------------------------------------------------------------
epoch 1 step 720 evaluation
eval_z_samples_size: 500
eval_loss: 0.1058, accuracy: 0.9648
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.9628, true_2_rate: 0.9671
log joint likelihood: tensor(-177.3447265625) joint log likelihood: tensor(-957.8994140625)
r_win_average: -0.5594, r_win_min: -11.3750, r_win_max: 8.2500, r_win_std: 5.7402
eta_win_average: -0.0967, eta_win_min: -0.1206, eta_win_max: -0.0425, eta_win_std: 0.0145
p_win_average: 0.0470, p_win_min: -0.0874, p_win_max: 0.1436, p_win_std: 0.0297

------------------------------------------------------------------------------------------
epoch 1 step 750 evaluation
eval_z_samples_size: 500
eval_loss: 0.1043, accuracy: 0.9668
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.9703, true_2_rate: 0.9630
log joint likelihood: tensor(-167.7543945312) joint log likelihood: tensor(-866.1660156250)
r_win_average: -0.0501, r_win_min: -10.9375, r_win_max: 8.7500, r_win_std: 5.8873
eta_win_average: 0.1045, eta_win_min: 0.0039, eta_win_max: 0.1484, eta_win_std: 0.0189
p_win_average: -0.1899, p_win_min: -0.3066, p_win_max: 0.0396, p_win_std: 0.0562

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 500
eval_loss: 0.1159, accuracy: 0.9629
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.9517, true_2_rate: 0.9753
log joint likelihood: tensor(-179.7270507812) joint log likelihood: tensor(-906.2802734375)
r_win_average: -1.1457, r_win_min: -11.5000, r_win_max: 7.5312, r_win_std: 5.7224
eta_win_average: 0.0456, eta_win_min: -0.0825, eta_win_max: 0.1650, eta_win_std: 0.0318
p_win_average: -0.2744, p_win_min: -0.4395, p_win_max: -0.0479, p_win_std: 0.0803

------------------------------------------------------------------------------------------
[2023-09-26 09:53:29,758] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:49,288] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:49,710] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:49,853] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:50,180] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:50,183] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:50,198] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:50,227] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:50,243] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 09:53:57,847] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:57,847] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 09:53:58,155] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:58,155] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 09:53:58,714] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:58,714] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 09:53:58,928] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:58,928] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 09:53:58,947] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:58,947] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 09:53:58,947] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 09:53:58,949] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:58,949] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 09:53:58,961] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:58,961] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 09:53:58,975] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 09:53:58,975] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 742
cuda seed 742
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size10-enn_dim128-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 442
cuda seed 442
torch seed 242
cuda seed 242
torch seed 542
cuda seed 542
torch seed 342
cuda seed 342
torch seed 642
cuda seed 642
torch seed 142
cuda seed 142
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 09:55:04,624] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 09:55:07,376] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 09:55:07,377] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 09:55:07,377] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 09:55:07,382] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 09:55:07,382] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 09:55:07,382] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 09:55:07,382] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 09:55:07,382] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 09:55:07,382] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 09:55:07,382] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
[2023-09-26 09:55:20,605] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 09:55:20,605] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 09:55:20,605] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.43 GB, percent = 6.5%
[2023-09-26 09:55:20,772] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 09:55:20,772] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 09:55:20,773] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.43 GB, percent = 6.5%
[2023-09-26 09:55:20,773] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 09:55:20,930] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 09:55:20,930] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 09:55:20,931] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.43 GB, percent = 6.5%
[2023-09-26 09:55:20,932] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 09:55:20,932] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 09:55:20,932] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 09:55:20,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 09:55:20,932] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 09:55:20,932] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 09:55:20,932] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 09:55:20,932] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 09:55:20,932] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5fb7ae2810>
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 09:55:20,933] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 09:55:20,934] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 09:55:20,934] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 09:55:20,934] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 09:55:20,934] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 09:55:20,934] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 09:55:20,934] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 500
eval_loss: 0.7537, accuracy: 0.5273
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4043, prediction_2_rate: 0.5957
true_1_rate: 0.4349, true_2_rate: 0.6296
log joint likelihood: tensor(-878.6718750000) joint log likelihood: tensor(-8431.7500000000)
r_win_average: 0.0375, r_win_min: -2.9375, r_win_max: 5.0938, r_win_std: 0.9013
eta_win_average: -0.0030, eta_win_min: -0.3086, eta_win_max: 0.1660, eta_win_std: 0.0636
p_win_average: -0.2349, p_win_min: -0.4707, p_win_max: 0.0344, p_win_std: 0.0557

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 500
eval_loss: 0.2598, accuracy: 0.9004
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4766, prediction_2_rate: 0.5234
true_1_rate: 0.8587, true_2_rate: 0.9465
log joint likelihood: tensor(-328.5117187500) joint log likelihood: tensor(-5059.5000000000)
r_win_average: -1.2666, r_win_min: -9.4375, r_win_max: 7.3750, r_win_std: 4.6500
eta_win_average: 0.1877, eta_win_min: -0.2598, eta_win_max: 0.5352, eta_win_std: 0.0964
p_win_average: -0.4979, p_win_min: -0.8594, p_win_max: 0.5117, p_win_std: 0.2245

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 500
eval_loss: 2.5477, accuracy: 0.5273
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.9980, prediction_2_rate: 0.0020
true_1_rate: 1.0000, true_2_rate: 0.0041
log joint likelihood: tensor(-774.6132812500) joint log likelihood: tensor(-8821.4687500000)
r_win_average: 6.0342, r_win_min: -1.3984, r_win_max: 9.1875, r_win_std: 1.6064
eta_win_average: -0.1423, eta_win_min: -0.2832, eta_win_max: 0.2188, eta_win_std: 0.0519
p_win_average: 0.2143, p_win_min: -0.1816, p_win_max: 0.3809, p_win_std: 0.0769

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 500
eval_loss: 1.1129, accuracy: 0.5254
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 1.0000, prediction_2_rate: 0.0000
true_1_rate: 1.0000, true_2_rate: 0.0000
log joint likelihood: tensor(-1323.9960937500) joint log likelihood: tensor(-9737.)
r_win_average: 2.1057, r_win_min: 1.9922, r_win_max: 3.5469, r_win_std: 0.1273
eta_win_average: 0.1754, eta_win_min: 0.1416, eta_win_max: 0.1865, eta_win_std: 0.0050
p_win_average: 0.2237, p_win_min: 0.2100, p_win_max: 0.4414, p_win_std: 0.0181

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 500
eval_loss: 0.6927, accuracy: 0.5254
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 1.0000, prediction_2_rate: 0.0000
true_1_rate: 1.0000, true_2_rate: 0.0000
log joint likelihood: tensor(-1303.3476562500) joint log likelihood: tensor(-6535.)
r_win_average: 0.2473, r_win_min: 0.1689, r_win_max: 0.4883, r_win_std: 0.0670
eta_win_average: 0.3087, eta_win_min: 0.3008, eta_win_max: 0.3184, eta_win_std: 0.0031
p_win_average: -0.1431, p_win_min: -0.1602, p_win_max: -0.1387, p_win_std: 0.0027

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 500
eval_loss: 0.9319, accuracy: 0.4746
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.0000, prediction_2_rate: 1.0000
true_1_rate: 0.0000, true_2_rate: 1.0000
log joint likelihood: tensor(-1314.5390625000) joint log likelihood: tensor(-6851.7500000000)
r_win_average: -1.3309, r_win_min: -1.3984, r_win_max: -1.1250, r_win_std: 0.0545
eta_win_average: -0.1706, eta_win_min: -0.1738, eta_win_max: -0.1680, eta_win_std: 0.0009
p_win_average: -0.0442, p_win_min: -0.0535, p_win_max: -0.0356, p_win_std: 0.0023

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 500
eval_loss: 0.8236, accuracy: 0.4746
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.0000, prediction_2_rate: 1.0000
true_1_rate: 0.0000, true_2_rate: 1.0000
log joint likelihood: tensor(-1288.8359375000) joint log likelihood: tensor(-4610.5625000000)
r_win_average: -0.9481, r_win_min: -1.0000, r_win_max: -0.7578, r_win_std: 0.0418
eta_win_average: -0.3892, eta_win_min: -0.3945, eta_win_max: -0.3867, eta_win_std: 0.0012
p_win_average: 0.3051, p_win_min: 0.2891, p_win_max: 0.3184, p_win_std: 0.0035

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 500
eval_loss: 0.6951, accuracy: 0.5254
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 1.0000, prediction_2_rate: 0.0000
true_1_rate: 1.0000, true_2_rate: 0.0000
log joint likelihood: tensor(-1278.2812500000) joint log likelihood: tensor(-4372.3750000000)
r_win_average: 0.2825, r_win_min: 0.2402, r_win_max: 0.3613, r_win_std: 0.0205
eta_win_average: -0.7028, eta_win_min: -0.7070, eta_win_max: -0.6953, eta_win_std: 0.0015
p_win_average: 0.5904, p_win_min: 0.5781, p_win_max: 0.6250, p_win_std: 0.0061

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 500
eval_loss: 0.8327, accuracy: 0.4746
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.0000, prediction_2_rate: 1.0000
true_1_rate: 0.0000, true_2_rate: 1.0000
log joint likelihood: tensor(-1304.9921875000) joint log likelihood: tensor(-3344.7500000000)
r_win_average: -0.9890, r_win_min: -1.0625, r_win_max: -0.8320, r_win_std: 0.0615
eta_win_average: 0.2314, eta_win_min: 0.2295, eta_win_max: 0.2334, eta_win_std: 0.0008
p_win_average: -0.3291, p_win_min: -0.3379, p_win_max: -0.3125, p_win_std: 0.0045

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 500
eval_loss: 0.6893, accuracy: 0.5254
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 1.0000, prediction_2_rate: 0.0000
true_1_rate: 1.0000, true_2_rate: 0.0000
log joint likelihood: tensor(-1279.0234375000) joint log likelihood: tensor(-3253.1250000000)
r_win_average: 0.1073, r_win_min: 0.0281, r_win_max: 0.2773, r_win_std: 0.0621
eta_win_average: 0.2316, eta_win_min: 0.2285, eta_win_max: 0.2363, eta_win_std: 0.0014
p_win_average: -0.3785, p_win_min: -0.4199, p_win_max: -0.3652, p_win_std: 0.0099

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 500
eval_loss: 0.7060, accuracy: 0.4746
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.0000, prediction_2_rate: 1.0000
true_1_rate: 0.0000, true_2_rate: 1.0000
log joint likelihood: tensor(-1327.0625000000) joint log likelihood: tensor(-2405.8750000000)
r_win_average: -0.2515, r_win_min: -0.3184, r_win_max: -0.1162, r_win_std: 0.0444
eta_win_average: -0.3140, eta_win_min: -0.3184, eta_win_max: -0.3086, eta_win_std: 0.0016
p_win_average: 0.2095, p_win_min: 0.2031, p_win_max: 0.2441, p_win_std: 0.0050

------------------------------------------------------------------------------------------
epoch 1 step 330 evaluation
eval_z_samples_size: 500
eval_loss: 0.7208, accuracy: 0.4746
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.0000, prediction_2_rate: 1.0000
true_1_rate: 0.0000, true_2_rate: 1.0000
log joint likelihood: tensor(-1351.4218750000) joint log likelihood: tensor(-2330.0312500000)
r_win_average: -0.4020, r_win_min: -0.4727, r_win_max: -0.2969, r_win_std: 0.0398
eta_win_average: 0.1664, eta_win_min: 0.1631, eta_win_max: 0.1689, eta_win_std: 0.0010
p_win_average: -0.2687, p_win_min: -0.2812, p_win_max: -0.2617, p_win_std: 0.0027

------------------------------------------------------------------------------------------
epoch 1 step 360 evaluation
eval_z_samples_size: 500
eval_loss: 0.7412, accuracy: 0.5254
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 1.0000, prediction_2_rate: 0.0000
true_1_rate: 1.0000, true_2_rate: 0.0000
log joint likelihood: tensor(-1398.6875000000) joint log likelihood: tensor(-2154.9687500000)
r_win_average: 0.7526, r_win_min: 0.6797, r_win_max: 0.8828, r_win_std: 0.0448
eta_win_average: -0.1263, eta_win_min: -0.1299, eta_win_max: -0.1235, eta_win_std: 0.0014
p_win_average: 0.1308, p_win_min: 0.1206, p_win_max: 0.1367, p_win_std: 0.0024

------------------------------------------------------------------------------------------
epoch 1 step 390 evaluation
eval_z_samples_size: 500
eval_loss: 0.6890, accuracy: 0.5703
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4238, prediction_2_rate: 0.5762
true_1_rate: 0.4944, true_2_rate: 0.6543
log joint likelihood: tensor(-1298.6875000000) joint log likelihood: tensor(-2576.8125000000)
r_win_average: -0.0053, r_win_min: -0.1011, r_win_max: 0.1455, r_win_std: 0.0514
eta_win_average: -0.1574, eta_win_min: -0.1602, eta_win_max: -0.1543, eta_win_std: 0.0010
p_win_average: 0.0536, p_win_min: 0.0427, p_win_max: 0.0605, p_win_std: 0.0031

------------------------------------------------------------------------------------------
epoch 1 step 420 evaluation
eval_z_samples_size: 500
eval_loss: 0.6988, accuracy: 0.4746
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.0000, prediction_2_rate: 1.0000
true_1_rate: 0.0000, true_2_rate: 1.0000
log joint likelihood: tensor(-1301.6718750000) joint log likelihood: tensor(-2650.1250000000)
r_win_average: -0.2111, r_win_min: -0.3691, r_win_max: -0.0019, r_win_std: 0.1052
eta_win_average: 0.2093, eta_win_min: 0.2061, eta_win_max: 0.2129, eta_win_std: 0.0010
p_win_average: -0.1391, p_win_min: -0.1455, p_win_max: -0.1187, p_win_std: 0.0031

------------------------------------------------------------------------------------------
epoch 1 step 450 evaluation
eval_z_samples_size: 500
eval_loss: 0.6866, accuracy: 0.5254
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 1.0000, prediction_2_rate: 0.0000
true_1_rate: 1.0000, true_2_rate: 0.0000
log joint likelihood: tensor(-1360.8593750000) joint log likelihood: tensor(-2109.9375000000)
r_win_average: 0.1806, r_win_min: 0.0104, r_win_max: 0.3457, r_win_std: 0.0916
eta_win_average: -0.0896, eta_win_min: -0.0923, eta_win_max: -0.0869, eta_win_std: 0.0010
p_win_average: 0.0432, p_win_min: 0.0381, p_win_max: 0.0522, p_win_std: 0.0029

------------------------------------------------------------------------------------------
epoch 1 step 480 evaluation
eval_z_samples_size: 500
eval_loss: 0.6970, accuracy: 0.5254
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 1.0000, prediction_2_rate: 0.0000
true_1_rate: 1.0000, true_2_rate: 0.0000
log joint likelihood: tensor(-1473.5000000000) joint log likelihood: tensor(-1923.6250000000)
r_win_average: 0.3971, r_win_min: 0.2441, r_win_max: 0.4961, r_win_std: 0.0717
eta_win_average: 0.2891, eta_win_min: 0.2852, eta_win_max: 0.2930, eta_win_std: 0.0017
p_win_average: -0.2948, p_win_min: -0.2988, p_win_max: -0.2910, p_win_std: 0.0017

------------------------------------------------------------------------------------------
epoch 1 step 510 evaluation
eval_z_samples_size: 500
eval_loss: 0.6879, accuracy: 0.5469
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.2090, prediction_2_rate: 0.7910
true_1_rate: 0.2677, true_2_rate: 0.8560
log joint likelihood: tensor(-1422.6406250000) joint log likelihood: tensor(-1971.8437500000)
r_win_average: -0.1180, r_win_min: -0.4004, r_win_max: 0.0786, r_win_std: 0.1453
eta_win_average: 0.2145, eta_win_min: 0.2109, eta_win_max: 0.2188, eta_win_std: 0.0017
p_win_average: -0.2333, p_win_min: -0.2422, p_win_max: -0.2197, p_win_std: 0.0039

------------------------------------------------------------------------------------------
epoch 1 step 540 evaluation
eval_z_samples_size: 500
eval_loss: 0.6782, accuracy: 0.5449
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5820, prediction_2_rate: 0.4180
true_1_rate: 0.6208, true_2_rate: 0.4609
log joint likelihood: tensor(-1405.7187500000) joint log likelihood: tensor(-1942.5312500000)
r_win_average: -0.0960, r_win_min: -0.5547, r_win_max: 0.2295, r_win_std: 0.2559
eta_win_average: -0.0885, eta_win_min: -0.0933, eta_win_max: -0.0850, eta_win_std: 0.0020
p_win_average: 0.0538, p_win_min: 0.0347, p_win_max: 0.0645, p_win_std: 0.0065

------------------------------------------------------------------------------------------
epoch 1 step 570 evaluation
eval_z_samples_size: 500
eval_loss: 0.6617, accuracy: 0.5781
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.9395, prediction_2_rate: 0.0605
true_1_rate: 0.9926, true_2_rate: 0.1193
log joint likelihood: tensor(-1352.3437500000) joint log likelihood: tensor(-1913.4375000000)
r_win_average: 0.3429, r_win_min: -0.5508, r_win_max: 0.6523, r_win_std: 0.1953
eta_win_average: -0.3361, eta_win_min: -0.3438, eta_win_max: -0.3301, eta_win_std: 0.0022
p_win_average: 0.2481, p_win_min: 0.2402, p_win_max: 0.2715, p_win_std: 0.0056

------------------------------------------------------------------------------------------
epoch 1 step 600 evaluation
eval_z_samples_size: 500
eval_loss: 0.6127, accuracy: 0.6719
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.8027, prediction_2_rate: 0.1973
true_1_rate: 0.9517, true_2_rate: 0.3621
log joint likelihood: tensor(-1154.8750000000) joint log likelihood: tensor(-1885.)
r_win_average: 0.6135, r_win_min: -1.2500, r_win_max: 1.5469, r_win_std: 0.7130
eta_win_average: -0.2209, eta_win_min: -0.2305, eta_win_max: -0.2100, eta_win_std: 0.0064
p_win_average: 0.1681, p_win_min: 0.1348, p_win_max: 0.1904, p_win_std: 0.0145

------------------------------------------------------------------------------------------
epoch 1 step 630 evaluation
eval_z_samples_size: 500
eval_loss: 0.4993, accuracy: 0.6973
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.2578, prediction_2_rate: 0.7422
true_1_rate: 0.4572, true_2_rate: 0.9630
log joint likelihood: tensor(-1023.7031250000) joint log likelihood: tensor(-1472.5625000000)
r_win_average: -0.3485, r_win_min: -2.4844, r_win_max: 2.6094, r_win_std: 1.2141
eta_win_average: -0.1124, eta_win_min: -0.1177, eta_win_max: -0.1084, eta_win_std: 0.0020
p_win_average: 0.1040, p_win_min: 0.0654, p_win_max: 0.1309, p_win_std: 0.0131

------------------------------------------------------------------------------------------
epoch 1 step 660 evaluation
eval_z_samples_size: 500
eval_loss: 0.4224, accuracy: 0.7871
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5898, prediction_2_rate: 0.4102
true_1_rate: 0.8587, true_2_rate: 0.7078
log joint likelihood: tensor(-814.7558593750) joint log likelihood: tensor(-1275.3281250000)
r_win_average: 0.1307, r_win_min: -2.9688, r_win_max: 3.4844, r_win_std: 1.7968
eta_win_average: -0.3998, eta_win_min: -0.4082, eta_win_max: -0.3906, eta_win_std: 0.0043
p_win_average: 0.4230, p_win_min: 0.3906, p_win_max: 0.4434, p_win_std: 0.0093

------------------------------------------------------------------------------------------
epoch 1 step 690 evaluation
eval_z_samples_size: 500
eval_loss: 0.4231, accuracy: 0.8086
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.6230, prediction_2_rate: 0.3770
true_1_rate: 0.9108, true_2_rate: 0.6955
log joint likelihood: tensor(-775.3525390625) joint log likelihood: tensor(-1225.1015625000)
r_win_average: 0.4912, r_win_min: -4.1250, r_win_max: 4.8438, r_win_std: 2.5332
eta_win_average: -0.0032, eta_win_min: -0.0189, eta_win_max: 0.0129, eta_win_std: 0.0093
p_win_average: -0.0367, p_win_min: -0.0603, p_win_max: 0.0036, p_win_std: 0.0165

------------------------------------------------------------------------------------------
epoch 1 step 720 evaluation
eval_z_samples_size: 500
eval_loss: 0.3735, accuracy: 0.8457
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5586, prediction_2_rate: 0.4414
true_1_rate: 0.8848, true_2_rate: 0.8025
log joint likelihood: tensor(-700.3767089844) joint log likelihood: tensor(-1092.9619140625)
r_win_average: 0.3454, r_win_min: -4.6562, r_win_max: 5.8750, r_win_std: 2.7700
eta_win_average: 0.0775, eta_win_min: 0.0659, eta_win_max: 0.0952, eta_win_std: 0.0088
p_win_average: -0.0991, p_win_min: -0.2002, p_win_max: -0.0270, p_win_std: 0.0452

------------------------------------------------------------------------------------------
epoch 1 step 750 evaluation
eval_z_samples_size: 500
eval_loss: 0.3876, accuracy: 0.8438
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5605, prediction_2_rate: 0.4395
true_1_rate: 0.8848, true_2_rate: 0.7984
log joint likelihood: tensor(-718.2797851562) joint log likelihood: tensor(-1125.9990234375)
r_win_average: 0.6614, r_win_min: -3.9062, r_win_max: 5.3750, r_win_std: 2.6017
eta_win_average: 0.0959, eta_win_min: 0.0845, eta_win_max: 0.1094, eta_win_std: 0.0057
p_win_average: -0.1221, p_win_min: -0.1611, p_win_max: -0.0840, p_win_std: 0.0173

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 500
eval_loss: 0.3598, accuracy: 0.8438
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4473, prediction_2_rate: 0.5527
true_1_rate: 0.7770, true_2_rate: 0.9177
log joint likelihood: tensor(-662.9228515625) joint log likelihood: tensor(-1086.2255859375)
r_win_average: -0.0353, r_win_min: -4.2188, r_win_max: 4.5312, r_win_std: 2.5721
eta_win_average: 0.0461, eta_win_min: 0.0325, eta_win_max: 0.0598, eta_win_std: 0.0074
p_win_average: -0.0721, p_win_min: -0.1641, p_win_max: -0.0211, p_win_std: 0.0315

------------------------------------------------------------------------------------------
[2023-09-26 16:41:37,731] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:41:59,610] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:41:59,874] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:42:00,111] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:42:00,116] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:42:00,141] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:42:00,174] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:42:00,223] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:42:00,231] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 16:42:07,150] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:07,150] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 16:42:07,913] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:07,914] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 16:42:08,065] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:08,065] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 16:42:08,065] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 16:42:08,084] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:08,084] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 16:42:08,091] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:08,091] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 16:42:08,122] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:08,122] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 16:42:08,128] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:08,128] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 16:42:08,145] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 16:42:08,145] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 342
cuda seed 342
torch seed 542
cuda seed 542
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size10-enn_dim128-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 142
cuda seed 142
torch seed 442
cuda seed 442
torch seed 242
cuda seed 242
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 16:43:14,382] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 16:43:16,816] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 16:43:16,817] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 16:43:16,817] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 16:43:16,822] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 16:43:16,822] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 16:43:16,822] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 16:43:16,822] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 16:43:16,822] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 16:43:16,822] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 16:43:16,822] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (53602, False)] 
[2023-09-26 16:43:26,551] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 16:43:26,551] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 16:43:26,551] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.41 GB, percent = 6.5%
[2023-09-26 16:43:26,720] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 16:43:26,721] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 16:43:26,721] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.41 GB, percent = 6.5%
[2023-09-26 16:43:26,721] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 16:43:26,880] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 16:43:26,880] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 16:43:26,880] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.41 GB, percent = 6.5%
[2023-09-26 16:43:26,881] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 16:43:26,882] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 16:43:26,882] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 16:43:26,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 16:43:26,882] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 16:43:26,882] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 16:43:26,882] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 16:43:26,882] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 16:43:26,882] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9a87ee2650>
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 16:43:26,883] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 16:43:26,884] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 16:43:26,884] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 16:43:26,884] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 16:43:26,884] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 16:43:26,884] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 500
eval_loss: 0.7537, accuracy: 0.5273
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4043, prediction_2_rate: 0.5957
true_1_rate: 0.4349, true_2_rate: 0.6296
log joint likelihood: tensor(-878.6718750000) joint log likelihood: tensor(-8431.7500000000)
r_win_average: 0.0375, r_win_min: -2.9375, r_win_max: 5.0938, r_win_std: 0.9013
eta_win_average: -0.0030, eta_win_min: -0.3086, eta_win_max: 0.1660, eta_win_std: 0.0636
p_win_average: -0.2349, p_win_min: -0.4707, p_win_max: 0.0344, p_win_std: 0.0557

------------------------------------------------------------------------------------------
epoch 1 step 30 evaluation
eval_z_samples_size: 500
eval_loss: 0.2001, accuracy: 0.9277
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.9219, true_2_rate: 0.9342
log joint likelihood: tensor(-342.7929687500) joint log likelihood: tensor(-6324.8750000000)
r_win_average: 0.2999, r_win_min: -7.8750, r_win_max: 9.3750, r_win_std: 5.2832
eta_win_average: 0.5264, eta_win_min: 0.0608, eta_win_max: 0.7422, eta_win_std: 0.1041
p_win_average: 0.4304, p_win_min: -0.0087, p_win_max: 0.7500, p_win_std: 0.0796

------------------------------------------------------------------------------------------
epoch 1 step 60 evaluation
eval_z_samples_size: 500
eval_loss: 0.1827, accuracy: 0.9434
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5234, prediction_2_rate: 0.4766
true_1_rate: 0.9442, true_2_rate: 0.9424
log joint likelihood: tensor(-237.3378906250) joint log likelihood: tensor(-3028.1640625000)
r_win_average: -0.7454, r_win_min: -13.1250, r_win_max: 9.6875, r_win_std: 6.3819
eta_win_average: -0.2191, eta_win_min: -0.3477, eta_win_max: 0.1143, eta_win_std: 0.0761
p_win_average: 0.4122, p_win_min: 0.0232, p_win_max: 0.7578, p_win_std: 0.0632

------------------------------------------------------------------------------------------
epoch 1 step 90 evaluation
eval_z_samples_size: 500
eval_loss: 0.4230, accuracy: 0.8750
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4121, prediction_2_rate: 0.5879
true_1_rate: 0.7732, true_2_rate: 0.9877
log joint likelihood: tensor(-318.2226562500) joint log likelihood: tensor(-4767.2187500000)
r_win_average: -2.6736, r_win_min: -11.1250, r_win_max: 8.5625, r_win_std: 5.2691
eta_win_average: -0.3617, eta_win_min: -0.5000, eta_win_max: -0.0535, eta_win_std: 0.0592
p_win_average: 0.0573, p_win_min: -0.2637, p_win_max: 0.2080, p_win_std: 0.0799

------------------------------------------------------------------------------------------
epoch 1 step 120 evaluation
eval_z_samples_size: 500
eval_loss: 0.1722, accuracy: 0.9414
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5449, prediction_2_rate: 0.4551
true_1_rate: 0.9628, true_2_rate: 0.9177
log joint likelihood: tensor(-324.1152343750) joint log likelihood: tensor(-6022.4375000000)
r_win_average: 0.7034, r_win_min: -10.6250, r_win_max: 11.3750, r_win_std: 5.5849
eta_win_average: 0.5267, eta_win_min: 0.0718, eta_win_max: 0.7109, eta_win_std: 0.0763
p_win_average: 0.3856, p_win_min: -0.0688, p_win_max: 0.6328, p_win_std: 0.1049

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 500
eval_loss: 0.1478, accuracy: 0.9531
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5410, prediction_2_rate: 0.4590
true_1_rate: 0.9703, true_2_rate: 0.9342
log joint likelihood: tensor(-331.2050781250) joint log likelihood: tensor(-7000.6250000000)
r_win_average: 0.7678, r_win_min: -9.3750, r_win_max: 10.9375, r_win_std: 5.7419
eta_win_average: 0.5738, eta_win_min: 0.0250, eta_win_max: 0.6719, eta_win_std: 0.0805
p_win_average: -0.2619, p_win_min: -0.4609, p_win_max: 0.0200, p_win_std: 0.0724

------------------------------------------------------------------------------------------
epoch 1 step 180 evaluation
eval_z_samples_size: 500
eval_loss: 0.1269, accuracy: 0.9551
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5430, prediction_2_rate: 0.4570
true_1_rate: 0.9740, true_2_rate: 0.9342
log joint likelihood: tensor(-320.1894531250) joint log likelihood: tensor(-5842.1562500000)
r_win_average: 0.0059, r_win_min: -11.0625, r_win_max: 11.3125, r_win_std: 5.6157
eta_win_average: 0.0024, eta_win_min: -0.1299, eta_win_max: 0.2207, eta_win_std: 0.0431
p_win_average: 0.2112, p_win_min: -0.1738, p_win_max: 0.3633, p_win_std: 0.0548

------------------------------------------------------------------------------------------
epoch 1 step 210 evaluation
eval_z_samples_size: 500
eval_loss: 0.1312, accuracy: 0.9570
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5332, prediction_2_rate: 0.4668
true_1_rate: 0.9665, true_2_rate: 0.9465
log joint likelihood: tensor(-288.0820312500) joint log likelihood: tensor(-5502.8125000000)
r_win_average: 0.6212, r_win_min: -11.0625, r_win_max: 11.1875, r_win_std: 6.0399
eta_win_average: 0.7164, eta_win_min: 0.3184, eta_win_max: 0.8711, eta_win_std: 0.0640
p_win_average: -0.4232, p_win_min: -0.5625, p_win_max: -0.0923, p_win_std: 0.0802

------------------------------------------------------------------------------------------
epoch 1 step 240 evaluation
eval_z_samples_size: 500
eval_loss: 0.1336, accuracy: 0.9648
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.9591, true_2_rate: 0.9712
log joint likelihood: tensor(-264.9921875000) joint log likelihood: tensor(-5572.4062500000)
r_win_average: -1.2411, r_win_min: -14.2500, r_win_max: 10.8125, r_win_std: 7.2122
eta_win_average: -0.8828, eta_win_min: -1.1484, eta_win_max: 0.1138, eta_win_std: 0.1853
p_win_average: -0.0126, p_win_min: -0.4668, p_win_max: 0.1387, p_win_std: 0.0684

------------------------------------------------------------------------------------------
epoch 1 step 270 evaluation
eval_z_samples_size: 500
eval_loss: 0.1463, accuracy: 0.9570
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.9331, true_2_rate: 0.9835
log joint likelihood: tensor(-344.9531250000) joint log likelihood: tensor(-6062.8125000000)
r_win_average: -1.0664, r_win_min: -9.7500, r_win_max: 7.7500, r_win_std: 4.7790
eta_win_average: -0.3527, eta_win_min: -0.4785, eta_win_max: -0.1162, eta_win_std: 0.0388
p_win_average: -0.3665, p_win_min: -0.5195, p_win_max: -0.1719, p_win_std: 0.0377

------------------------------------------------------------------------------------------
epoch 1 step 300 evaluation
eval_z_samples_size: 500
eval_loss: 0.1514, accuracy: 0.9551
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.9442, true_2_rate: 0.9671
log joint likelihood: tensor(-216.5742187500) joint log likelihood: tensor(-2354.4687500000)
r_win_average: -0.8621, r_win_min: -11.3125, r_win_max: 10.8750, r_win_std: 6.3562
eta_win_average: 0.5702, eta_win_min: 0.0850, eta_win_max: 0.7266, eta_win_std: 0.1195
p_win_average: -0.1856, p_win_min: -0.2852, p_win_max: 0.1104, p_win_std: 0.0644

------------------------------------------------------------------------------------------
epoch 1 step 330 evaluation
eval_z_samples_size: 500
eval_loss: 0.1291, accuracy: 0.9609
label_1_rate: 0.5254, label_2_rate: 0.4746, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.9517, true_2_rate: 0.9712
log joint likelihood: tensor(-228.9863281250) joint log likelihood: tensor(-2839.1484375000)
r_win_average: 0.0054, r_win_min: -9.5000, r_win_max: 11.5625, r_win_std: 5.9462
eta_win_average: 0.5052, eta_win_min: -0.2871, eta_win_max: 0.6836, eta_win_std: 0.1691
p_win_average: -0.3971, p_win_min: -0.7266, p_win_max: -0.1602, p_win_std: 0.0587

------------------------------------------------------------------------------------------
[2023-09-26 19:40:31,261] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,011] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,266] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,348] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,601] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,625] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,626] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,633] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:48,641] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:40:54,487] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:54,487] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:40:54,973] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:54,973] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:40:55,080] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:55,080] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:40:55,268] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:55,268] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:40:55,268] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:40:55,325] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:55,325] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:40:55,380] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:55,380] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:40:55,418] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:55,418] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:40:55,431] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:40:55,431] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 442
cuda seed 442
torch seed 542
cuda seed 542
torch seed 642
cuda seed 642
torch seed 242
cuda seed 242
torch seed 142
cuda seed 142
torch seed 742
cuda seed 742
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size50-enn_dim64-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 342
cuda seed 342
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:42:01,290] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:42:03,958] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:42:03,959] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:42:03,959] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:42:03,964] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:42:03,964] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:42:03,964] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:42:03,964] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:42:03,965] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:42:03,965] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:42:03,965] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
[2023-09-26 19:42:15,477] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:42:15,478] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:42:15,478] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.44 GB, percent = 6.5%
[2023-09-26 19:42:15,669] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:42:15,669] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:42:15,669] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.44 GB, percent = 6.5%
[2023-09-26 19:42:15,670] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:42:15,850] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:42:15,850] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:42:15,850] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.44 GB, percent = 6.5%
[2023-09-26 19:42:15,852] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:42:15,852] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:42:15,852] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:42:15,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:42:15,852] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:42:15,852] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:42:15,852] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:42:15,852] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:42:15,852] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f73af096790>
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:42:15,853] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:42:15,854] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:42:15,854] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:42:15,854] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:42:15,854] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:42:15,854] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:42:15,854] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
[2023-09-26 19:42:36,006] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,313] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,574] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,645] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,756] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,842] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,855] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,862] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:52,866] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:42:59,046] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:42:59,046] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:42:59,738] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:42:59,738] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:42:59,803] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:42:59,803] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:43:00,018] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:43:00,018] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:43:00,038] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:43:00,039] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:43:00,082] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:43:00,082] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:43:00,082] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:43:00,098] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:43:00,098] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:43:00,100] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:43:00,100] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 442
cuda seed 442
torch seed 642
cuda seed 642
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size50-enn_dim64-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 542
cuda seed 542
torch seed 742
cuda seed 742
torch seed 242
cuda seed 242
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:44:05,656] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:44:08,899] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:44:08,900] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:44:08,900] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:44:08,907] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:44:08,907] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:44:08,907] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:44:08,907] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:44:08,907] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:44:08,907] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:44:08,907] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (26936, False)] 
[2023-09-26 19:44:19,583] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:44:19,583] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:44:19,583] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.43 GB, percent = 6.5%
[2023-09-26 19:44:19,763] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:44:19,764] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:44:19,764] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.43 GB, percent = 6.5%
[2023-09-26 19:44:19,764] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:44:19,933] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:44:19,933] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:44:19,934] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.43 GB, percent = 6.5%
[2023-09-26 19:44:19,935] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:44:19,935] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:44:19,935] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:44:19,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:44:19,936] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f02e5802650>
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:44:19,936] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:44:19,937] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:44:19,937] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
[2023-09-26 19:44:39,940] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:55,522] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:55,637] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:55,872] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:56,009] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:56,013] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:56,018] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:56,031] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:44:56,033] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:45:02,310] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:02,311] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:45:02,513] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:02,513] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:45:02,786] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:02,786] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:45:02,983] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:02,983] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:45:02,985] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:02,985] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:45:02,994] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:02,994] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:45:02,994] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:45:03,007] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:03,007] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:45:03,016] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:45:03,016] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 542
cuda seed 542
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size50-enn_dim128-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
torch seed 442
cuda seed 442
torch seed 242
cuda seed 242
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:46:09,478] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:46:12,500] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:46:12,501] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:46:12,501] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:46:12,506] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:46:12,506] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:46:12,506] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:46:12,506] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:46:12,506] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:46:12,506] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:46:12,506] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
[2023-09-26 19:46:22,830] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:46:22,831] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:46:22,831] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:46:23,016] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:46:23,017] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:46:23,017] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:46:23,017] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:46:23,193] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:46:23,194] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:46:23,194] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:46:23,195] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:46:23,195] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:46:23,195] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:46:23,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:46:23,196] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2a0c5fe790>
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:46:23,196] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:46:23,197] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:46:23,197] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
[2023-09-26 19:46:44,094] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:46:59,964] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:00,026] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:00,268] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:00,346] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:00,480] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:00,490] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:00,503] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:00,518] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:47:06,868] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:06,869] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:47:06,951] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:06,951] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:47:07,278] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:07,278] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:47:07,296] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:07,296] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:47:07,296] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:47:07,310] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:07,310] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:47:07,413] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:07,413] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:47:07,489] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:07,489] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:47:07,490] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:47:07,491] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 642
cuda seed 642
torch seed 742
cuda seed 742
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size50-enn_dim128-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 542
cuda seed 542
torch seed 342
cuda seed 342
torch seed 142
cuda seed 142
torch seed 242
cuda seed 242
torch seed 442
cuda seed 442
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:48:13,029] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:48:15,657] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:48:15,658] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:48:15,658] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:48:15,663] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:48:15,663] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:48:15,663] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:48:15,663] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:48:15,663] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:48:15,663] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:48:15,663] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (54888, False)] 
[2023-09-26 19:48:28,993] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:48:28,994] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:48:28,994] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.41 GB, percent = 6.5%
[2023-09-26 19:48:29,173] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:48:29,174] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:48:29,174] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.41 GB, percent = 6.5%
[2023-09-26 19:48:29,174] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:48:29,344] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:48:29,345] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:48:29,345] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.41 GB, percent = 6.5%
[2023-09-26 19:48:29,346] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:48:29,346] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:48:29,346] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:48:29,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:48:29,347] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5d1055ead0>
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:48:29,347] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:48:29,348] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:48:29,348] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
[2023-09-26 19:48:52,824] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:08,761] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:09,041] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:09,042] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:09,177] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:09,289] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:09,302] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:09,309] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:09,312] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:49:15,539] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:15,539] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:49:16,229] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:16,229] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:49:16,240] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:16,240] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:49:16,248] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:16,249] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:49:16,342] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:16,343] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:49:16,393] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:16,393] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:49:16,411] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:16,411] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:49:16,411] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:49:16,442] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:49:16,442] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 542
cuda seed 542
torch seed 642
cuda seed 642
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
torch seed 742
cuda seed 742
torch seed 442
cuda seed 442
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size100-enn_dim64-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 242
cuda seed 242
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:50:21,933] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:50:24,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:50:24,533] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:50:24,533] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:50:24,538] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:50:24,538] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:50:24,538] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:50:24,538] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:50:24,538] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:50:24,538] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:50:24,538] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
[2023-09-26 19:50:37,785] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:50:37,786] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:50:37,786] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.42 GB, percent = 6.5%
[2023-09-26 19:50:37,963] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:50:37,963] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:50:37,963] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.42 GB, percent = 6.5%
[2023-09-26 19:50:37,964] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:50:38,126] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:50:38,127] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:50:38,127] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.42 GB, percent = 6.5%
[2023-09-26 19:50:38,128] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:50:38,128] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:50:38,128] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:50:38,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:50:38,128] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f66118d2a50>
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:50:38,129] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:50:38,130] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:50:38,130] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
[2023-09-26 19:51:01,960] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:16,937] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:17,239] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:17,363] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:17,573] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:17,723] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:17,735] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:17,748] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:17,762] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:51:23,604] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:23,604] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:51:24,070] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:24,070] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:51:24,257] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:24,258] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:51:24,443] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:24,443] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:51:24,553] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:24,553] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:51:24,554] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:51:24,569] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:24,569] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:51:24,578] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:24,578] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:51:24,586] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:51:24,586] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 242
cuda seed 242
torch seed 442
cuda seed 442
torch seed 342
cuda seed 342
torch seed 742
cuda seed 742
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size100-enn_dim64-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 142
cuda seed 142
torch seed 642
cuda seed 642
torch seed 542
cuda seed 542
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:52:30,185] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:52:33,097] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:52:33,098] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:52:33,098] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:52:33,103] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:52:33,103] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:52:33,103] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:52:33,104] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:52:33,104] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:52:33,104] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:52:33,104] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (27742, False)] 
[2023-09-26 19:52:42,698] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:52:42,699] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:52:42,699] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.38 GB, percent = 6.5%
[2023-09-26 19:52:42,869] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:52:42,870] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:52:42,870] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.38 GB, percent = 6.5%
[2023-09-26 19:52:42,870] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:52:43,029] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:52:43,030] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:52:43,030] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.38 GB, percent = 6.5%
[2023-09-26 19:52:43,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:52:43,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:52:43,031] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:52:43,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:52:43,031] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe20e7661d0>
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:52:43,032] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:52:43,033] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:52:43,033] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
[2023-09-26 19:53:10,938] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,185] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,466] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,821] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,926] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,941] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,941] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,961] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:26,979] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:53:32,797] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:32,797] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:53:32,797] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:53:32,972] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:32,972] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:53:33,584] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:33,584] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:53:33,691] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:33,691] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:53:33,724] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:33,724] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:53:33,734] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:33,735] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:53:33,754] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:33,754] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:53:33,761] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:53:33,762] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 542
cuda seed 542
torch seed 642
cuda seed 642
torch seed 142
cuda seed 142
torch seed 342
cuda seed 342
torch seed 742
cuda seed 742
torch seed 442
cuda seed 442
torch seed 242
cuda seed 242
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size100-enn_dim128-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:54:39,957] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:54:42,085] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:54:42,086] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:54:42,086] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:54:42,091] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:54:42,091] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:54:42,091] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:54:42,091] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:54:42,091] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:54:42,091] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:54:42,091] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
[2023-09-26 19:54:53,580] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:54:53,581] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:54:53,581] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:54:53,755] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:54:53,755] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:54:53,755] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:54:53,755] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:54:53,924] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:54:53,925] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:54:53,925] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:54:53,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:54:53,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:54:53,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:54:53,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:54:53,927] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:54:53,927] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:54:53,927] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f859ee6e050>
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:54:53,928] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:54:53,929] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:54:53,929] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
[2023-09-26 19:55:14,995] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:30,818] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:31,234] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:31,391] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:31,588] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:31,646] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:31,752] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:31,757] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:31,763] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-26 19:55:37,646] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:37,646] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:55:38,237] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:38,237] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:55:38,237] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-26 19:55:38,470] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:38,470] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:55:38,538] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:38,538] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:55:38,667] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:38,667] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:55:38,689] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:38,689] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:55:38,690] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:38,690] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-26 19:55:38,693] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-26 19:55:38,693] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 242
cuda seed 242
torch seed 742
cuda seed 742
torch seed 442
cuda seed 442
torch seed 142
cuda seed 142
torch seed 642
cuda seed 642
torch seed 342
cuda seed 342
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/imdb/IMDBDataset-ref_size100-enn_dim128-num_ref_train1000-lr1e-05-weight_decay0.01-enn_lr0.0003-enn_decay0.5-reward_lr0.0003-reward_decay0.5-gc1-train_batch_size8
torch seed 42
cuda seed 42
torch seed 542
cuda seed 542
training dataset size: 6184
eval dataset size: 8
joint eval dataset size: 256
Total steps:  6184
Warmup steps:  185
[2023-09-26 19:56:44,688] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-26 19:56:47,079] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-26 19:56:47,080] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-26 19:56:47,080] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-26 19:56:47,086] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-26 19:56:47,086] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-26 19:56:47,086] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-26 19:56:47,086] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-26 19:56:47,086] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-26 19:56:47,086] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-26 19:56:47,086] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428309200, False), (402, False), (56494, False)] 
[2023-09-26 19:57:00,407] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-26 19:57:00,408] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-26 19:57:00,408] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.38 GB, percent = 6.5%
[2023-09-26 19:57:00,586] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-26 19:57:00,587] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:57:00,587] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:57:00,587] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-26 19:57:00,749] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-26 19:57:00,750] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-26 19:57:00,750] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.39 GB, percent = 6.5%
[2023-09-26 19:57:00,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-26 19:57:00,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-26 19:57:00,751] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-26 19:57:00,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 2.9999999999999997e-05], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-26 19:57:00,752] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fb0b2aaa3d0>
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-26 19:57:00,752] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-26 19:57:00,753] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-26 19:57:00,753] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
Problem at: /user/al4263/.conda/envs/rlhf_env/lib/python3.11/site-packages/wandb/sdk/wandb_init.py 829 getcaller
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 10, 'num_ref_train': 1000, 'eval_z_size_list': '500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 10, 'num_ref_train': 1000, 'eval_z_size_list': '500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 50, 'num_ref_train': 100, 'eval_z_size_list': '500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 50, 'num_ref_train': 1000, 'eval_z_size_list': '500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 50, 'num_ref_train': 100, 'eval_z_size_list': '500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 50, 'num_ref_train': 1000, 'eval_z_size_list': '500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 100, 'num_ref_train': 100, 'eval_z_size_list': '500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 100, 'num_ref_train': 1000, 'eval_z_size_list': '500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 100, 'num_ref_train': 100, 'eval_z_size_list': '500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'imdb', 'backbone_model': 'openlm-research/open_llama_3b_v2', 'dataset_name': 'IMDBDataset', 'ref_size': 100, 'num_ref_train': 1000, 'eval_z_size_list': '500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.5, 'lr': 1e-05, 'enn_lr': 0.0003, 'enn_decay': 0.5, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 8, 'eval_batch_size': 64, 'eval_steps': 30, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
