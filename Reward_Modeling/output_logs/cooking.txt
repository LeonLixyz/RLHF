[2023-09-18 13:23:09,850] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:16,256] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:28,313] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:28,681] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:28,914] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:29,031] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:29,142] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:29,144] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:29,161] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:29,170] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 13:23:33,264] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:33,264] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 13:23:33,623] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:33,624] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 13:23:33,624] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-18 13:23:34,107] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:34,107] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 13:23:34,152] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:34,152] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 13:23:34,190] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:34,190] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 13:23:34,230] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:34,230] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 13:23:34,234] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:34,235] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 13:23:34,237] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 13:23:34,237] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 342
cuda seed 342
torch seed 742
cuda seed 742
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking/se_cooking_preference-ref_size10-enn_dim64-num_ref_train10-lr1e-05-weight_decay0.01-enn_lr0.001-enn_decay0.1-reward_lr0.0003-reward_decay0.1-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 642
cuda seed 642
torch seed 442
cuda seed 442
torch seed 542
cuda seed 542
torch seed 242
cuda seed 242
torch seed 142
cuda seed 142
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-18 13:24:11,921] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-18 13:24:14,893] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-18 13:24:14,894] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-18 13:24:14,895] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-18 13:24:14,900] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-18 13:24:14,900] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-18 13:24:14,900] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-18 13:24:14,902] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-18 13:24:14,903] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-18 13:24:14,903] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-18 13:24:14,903] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-18 13:24:27,944] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-18 13:24:27,944] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-18 13:24:27,945] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.76 GB, percent = 6.5%
[2023-09-18 13:24:28,043] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-18 13:24:28,044] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 13:24:28,045] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.77 GB, percent = 6.5%
[2023-09-18 13:24:28,046] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-18 13:24:28,138] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-18 13:24:28,138] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 13:24:28,140] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 65.77 GB, percent = 6.5%
[2023-09-18 13:24:28,142] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-18 13:24:28,142] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-18 13:24:28,143] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-18 13:24:28,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-18 13:24:28,145] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-18 13:24:28,146] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-18 13:24:28,146] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-18 13:24:28,146] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-18 13:24:28,147] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-18 13:24:28,148] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-18 13:24:28,148] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-18 13:24:28,148] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-18 13:24:28,150] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-18 13:24:28,150] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-18 13:24:28,151] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa956e61c10>
[2023-09-18 13:24:28,151] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-18 13:24:28,151] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-18 13:24:28,152] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-18 13:24:28,153] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-18 13:24:28,153] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-18 13:24:28,154] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-18 13:24:28,154] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-18 13:24:28,154] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-18 13:24:28,155] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-18 13:24:28,155] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-18 13:24:28,156] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-18 13:24:28,156] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-18 13:24:28,157] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-18 13:24:28,157] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-18 13:24:28,157] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-18 13:24:28,158] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-18 13:24:28,158] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-18 13:24:28,159] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-18 13:24:28,159] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-18 13:24:28,161] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-18 13:24:28,162] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-18 13:24:28,163] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-18 13:24:28,164] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-18 13:24:28,164] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-18 13:24:28,165] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-18 13:24:28,165] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-18 13:24:28,166] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-18 13:24:28,166] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-18 13:24:28,167] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-18 13:24:28,168] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-18 13:24:28,169] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-18 13:24:28,169] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-18 13:24:28,170] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-18 13:24:28,171] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-18 13:24:28,172] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-18 13:24:28,172] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-18 13:24:28,174] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-18 13:24:28,175] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-18 13:24:28,176] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-18 13:24:28,177] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-18 13:24:28,178] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-18 13:24:28,179] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-18 13:24:28,180] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-18 13:24:28,181] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-18 13:24:28,181] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-18 13:24:28,182] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-18 13:24:28,183] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-18 13:24:28,183] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-18 13:24:28,184] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-18 13:24:28,185] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-18 13:24:28,185] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-18 13:24:28,186] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-18 13:24:28,188] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-18 13:24:28,189] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-18 13:24:28,189] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-18 13:24:28,190] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-18 13:24:28,191] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-18 13:24:28,192] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-18 13:24:28,193] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-18 13:24:28,193] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 10
eval_loss: 1.0479, accuracy: 0.5527
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5660, true_2_rate: 0.5385
joint log likelihood: tensor(-5518.0546875000)
r_win_average: -3.1717, r_win_min: -7.5938, r_win_max: 2.8750, r_win_std: 1.7469
r_lose_average: -5.1442, r_lose_min: -10.3125, r_lose_max: 1.2109, r_lose_std: 1.6710
eta_win_average: -1.0420, eta_win_min: -3.1562, eta_win_max: 1.4453, eta_win_std: 0.9331
eta_lose_average: -1.4266, eta_lose_min: -4.1562, eta_lose_max: 1.1328, eta_lose_std: 0.9611
p_win_average: -1.1312, p_win_min: -3.4219, p_win_max: 2.5625, p_win_std: 0.9894
p_lose_average: -1.5618, p_lose_min: -3.7344, p_lose_max: 1.6562, p_lose_std: 0.8941

eval_z_samples_size: 100
eval_loss: 1.2832, accuracy: 0.4883
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.4755, true_2_rate: 0.5020
joint log likelihood: tensor(-5627.4843750000)
r_win_average: -1.5859, r_win_min: -6.5312, r_win_max: 2.2812, r_win_std: 1.4015
r_lose_average: -3.6530, r_lose_min: -8.0625, r_lose_max: -0.0303, r_lose_std: 1.7069
eta_win_average: -0.2298, eta_win_min: -1.1953, eta_win_max: 0.4160, eta_win_std: 0.2666
eta_lose_average: -0.3831, eta_lose_min: -1.3047, eta_lose_max: 0.4961, eta_lose_std: 0.3045
p_win_average: -0.7144, p_win_min: -1.4375, p_win_max: 0.4336, p_win_std: 0.2629
p_lose_average: -0.7574, p_lose_min: -1.5469, p_lose_max: 0.0542, p_lose_std: 0.2582

eval_z_samples_size: 500
eval_loss: 1.1914, accuracy: 0.5000
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.4981, true_2_rate: 0.5020
joint log likelihood: tensor(-5634.5312500000)
r_win_average: -1.0486, r_win_min: -5.2500, r_win_max: 3.1719, r_win_std: 1.3644
r_lose_average: -2.9769, r_lose_min: -7.2812, r_lose_max: 0.5742, r_lose_std: 1.6074
eta_win_average: -0.0901, eta_win_min: -0.4277, eta_win_max: 0.4668, eta_win_std: 0.1447
eta_lose_average: -0.1351, eta_lose_min: -0.5664, eta_lose_max: 0.5234, eta_lose_std: 0.1383
p_win_average: -0.3304, p_win_min: -0.7500, p_win_max: 0.1182, p_win_std: 0.1330
p_lose_average: -0.3155, p_lose_min: -0.8086, p_lose_max: 0.0835, p_lose_std: 0.1352

------------------------------------------------------------------------------------------
epoch 1 step 50 evaluation
eval_z_samples_size: 10
eval_loss: 0.4548, accuracy: 0.7988
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.8038, true_2_rate: 0.7935
joint log likelihood: tensor(-1348.1093750000)
r_win_average: 1.1689, r_win_min: -2.7812, r_win_max: 4.2500, r_win_std: 1.0365
r_lose_average: -0.4529, r_lose_min: -3.7344, r_lose_max: 2.9688, r_lose_std: 1.3417
eta_win_average: 1.9737, eta_win_min: 0.8125, eta_win_max: 2.5625, eta_win_std: 0.3359
eta_lose_average: 2.1422, eta_lose_min: 0.9023, eta_lose_max: 2.7188, eta_lose_std: 0.2403
p_win_average: -0.7895, p_win_min: -1.4062, p_win_max: 0.3555, p_win_std: 0.2757
p_lose_average: -1.0048, p_lose_min: -1.6953, p_lose_max: 0.0255, p_lose_std: 0.2375

eval_z_samples_size: 100
eval_loss: 0.4556, accuracy: 0.7852
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.7925, true_2_rate: 0.7773
joint log likelihood: tensor(-1354.6074218750)
r_win_average: 1.4666, r_win_min: -2.4688, r_win_max: 4.3125, r_win_std: 1.0058
r_lose_average: -0.0982, r_lose_min: -3.0156, r_lose_max: 3.2500, r_lose_std: 1.2833
eta_win_average: 1.1630, eta_win_min: 0.8398, eta_win_max: 1.3984, eta_win_std: 0.0873
eta_lose_average: 1.1921, eta_lose_min: 0.8555, eta_lose_max: 1.3984, eta_lose_std: 0.0661
p_win_average: 0.3160, p_win_min: 0.1182, p_win_max: 0.5039, p_win_std: 0.0459
p_lose_average: 0.3029, p_lose_min: 0.0767, p_lose_max: 0.4414, p_lose_std: 0.0464

eval_z_samples_size: 500
eval_loss: 0.4534, accuracy: 0.7832
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.7925, true_2_rate: 0.7733
joint log likelihood: tensor(-1355.9453125000)
r_win_average: 0.1198, r_win_min: -3.8281, r_win_max: 3.1250, r_win_std: 1.0380
r_lose_average: -1.4648, r_lose_min: -4.4062, r_lose_max: 1.9375, r_lose_std: 1.2805
eta_win_average: 0.1471, eta_win_min: -0.0820, eta_win_max: 0.3047, eta_win_std: 0.0362
eta_lose_average: 0.1319, eta_lose_min: -0.3652, eta_lose_max: 0.2393, eta_lose_std: 0.0353
p_win_average: -0.0153, p_win_min: -0.1699, p_win_max: 0.1543, p_win_std: 0.0280
p_lose_average: -0.0035, p_lose_min: -0.1338, p_lose_max: 0.1523, p_lose_std: 0.0265

------------------------------------------------------------------------------------------
epoch 1 step 100 evaluation
eval_z_samples_size: 10
eval_loss: 0.4583, accuracy: 0.7930
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.8000, true_2_rate: 0.7854
joint log likelihood: tensor(-1283.0156250000)
r_win_average: 1.5045, r_win_min: -1.2891, r_win_max: 3.0156, r_win_std: 0.7494
r_lose_average: 0.1468, r_lose_min: -3.7188, r_lose_max: 2.6406, r_lose_std: 1.1986
eta_win_average: 0.3453, eta_win_min: 0.0172, eta_win_max: 0.6562, eta_win_std: 0.0850
eta_lose_average: 0.3539, eta_lose_min: -0.3359, eta_lose_max: 0.6445, eta_lose_std: 0.1108
p_win_average: 0.7316, p_win_min: 0.2393, p_win_max: 1.2266, p_win_std: 0.1425
p_lose_average: 0.8447, p_lose_min: 0.1943, p_lose_max: 1.3047, p_lose_std: 0.1666

eval_z_samples_size: 100
eval_loss: 0.4463, accuracy: 0.8047
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5137, prediction_2_rate: 0.4863
true_1_rate: 0.8075, true_2_rate: 0.8016
joint log likelihood: tensor(-1284.1250000000)
r_win_average: 1.0266, r_win_min: -2.2500, r_win_max: 2.5781, r_win_std: 0.7977
r_lose_average: -0.4503, r_lose_min: -4.4375, r_lose_max: 2.2812, r_lose_std: 1.3217
eta_win_average: 0.2979, eta_win_min: 0.2002, eta_win_max: 0.3809, eta_win_std: 0.0292
eta_lose_average: 0.3244, eta_lose_min: 0.2139, eta_lose_max: 0.4531, eta_lose_std: 0.0323
p_win_average: 0.2931, p_win_min: 0.1846, p_win_max: 0.4844, p_win_std: 0.0359
p_lose_average: 0.2852, p_lose_min: 0.1406, p_lose_max: 0.6133, p_lose_std: 0.0411

eval_z_samples_size: 500
eval_loss: 0.4460, accuracy: 0.8008
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5176, prediction_2_rate: 0.4824
true_1_rate: 0.8075, true_2_rate: 0.7935
joint log likelihood: tensor(-1286.3554687500)
r_win_average: 0.4789, r_win_min: -2.7656, r_win_max: 2.0000, r_win_std: 0.8022
r_lose_average: -1.0005, r_lose_min: -5.0625, r_lose_max: 1.6250, r_lose_std: 1.3159
eta_win_average: 0.1025, eta_win_min: 0.0205, eta_win_max: 0.1758, eta_win_std: 0.0195
eta_lose_average: 0.1102, eta_lose_min: 0.0025, eta_lose_max: 0.1826, eta_lose_std: 0.0195
p_win_average: -0.0587, p_win_min: -0.1631, p_win_max: 0.0342, p_win_std: 0.0247
p_lose_average: -0.0513, p_lose_min: -0.2119, p_lose_max: 0.0483, p_lose_std: 0.0270

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 10
eval_loss: 0.3975, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4805, prediction_2_rate: 0.5195
true_1_rate: 0.7887, true_2_rate: 0.8502
joint log likelihood: tensor(-1092.0605468750)
r_win_average: 3.3341, r_win_min: -1.1094, r_win_max: 6.0000, r_win_std: 1.1743
r_lose_average: 1.2261, r_lose_min: -3.4688, r_lose_max: 4.8438, r_lose_std: 1.6627
eta_win_average: 2.6142, eta_win_min: 1.8359, eta_win_max: 3.2500, eta_win_std: 0.1088
eta_lose_average: 2.6366, eta_lose_min: 2.2500, eta_lose_max: 3.1406, eta_lose_std: 0.1140
p_win_average: 1.4376, p_win_min: 0.4590, p_win_max: 2.5469, p_win_std: 0.1825
p_lose_average: 1.5996, p_lose_min: 0.9453, p_lose_max: 2.3906, p_lose_std: 0.1941

eval_z_samples_size: 100
eval_loss: 0.3945, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8113, true_2_rate: 0.8421
joint log likelihood: tensor(-1087.2165527344)
r_win_average: -1.6065, r_win_min: -6.0000, r_win_max: 0.9609, r_win_std: 1.2809
r_lose_average: -3.9284, r_lose_min: -9.7500, r_lose_max: -0.0991, r_lose_std: 1.8143
eta_win_average: -0.7811, eta_win_min: -0.9258, eta_win_max: -0.6016, eta_win_std: 0.0562
eta_lose_average: -0.8361, eta_lose_min: -1.0000, eta_lose_max: -0.6094, eta_lose_std: 0.0579
p_win_average: -0.1136, p_win_min: -0.3398, p_win_max: 0.0130, p_win_std: 0.0529
p_lose_average: -0.0764, p_lose_min: -0.3340, p_lose_max: 0.0972, p_lose_std: 0.0538

eval_z_samples_size: 500
eval_loss: 0.3943, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.8151, true_2_rate: 0.8421
joint log likelihood: tensor(-1086.4631347656)
r_win_average: -0.6830, r_win_min: -5.0312, r_win_max: 1.8906, r_win_std: 1.2648
r_lose_average: -2.9830, r_lose_min: -8.7500, r_lose_max: 0.8633, r_lose_std: 1.8018
eta_win_average: 0.0090, eta_win_min: -0.0659, eta_win_max: 0.0742, eta_win_std: 0.0194
eta_lose_average: 0.0261, eta_lose_min: -0.0569, eta_lose_max: 0.0972, eta_lose_std: 0.0193
p_win_average: 0.0198, p_win_min: -0.0391, p_win_max: 0.1191, p_win_std: 0.0214
p_lose_average: 0.0069, p_lose_min: -0.0747, p_lose_max: 0.0942, p_lose_std: 0.0239

------------------------------------------------------------------------------------------
epoch 1 step 200 evaluation
eval_z_samples_size: 10
eval_loss: 0.3848, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8075, true_2_rate: 0.8462
joint log likelihood: tensor(-1043.5986328125)
r_win_average: 0.1498, r_win_min: -3.4688, r_win_max: 2.3906, r_win_std: 0.9752
r_lose_average: -1.5513, r_lose_min: -5.0312, r_lose_max: 1.3516, r_lose_std: 1.2511
eta_win_average: 1.8938, eta_win_min: 1.2891, eta_win_max: 2.5469, eta_win_std: 0.1305
eta_lose_average: 2.0250, eta_lose_min: 1.5938, eta_lose_max: 2.5000, eta_lose_std: 0.1362
p_win_average: -1.5297, p_win_min: -1.8438, p_win_max: -0.9922, p_win_std: 0.1249
p_lose_average: -1.5940, p_lose_min: -2.0000, p_lose_max: -1.2031, p_lose_std: 0.1123

eval_z_samples_size: 100
eval_loss: 0.3843, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4824, prediction_2_rate: 0.5176
true_1_rate: 0.7925, true_2_rate: 0.8502
joint log likelihood: tensor(-1049.7744140625)
r_win_average: -0.1218, r_win_min: -3.7500, r_win_max: 2.3281, r_win_std: 0.9785
r_lose_average: -1.8534, r_lose_min: -5.3750, r_lose_max: 1.0234, r_lose_std: 1.2854
eta_win_average: -0.1683, eta_win_min: -0.2695, eta_win_max: -0.0669, eta_win_std: 0.0331
eta_lose_average: -0.1453, eta_lose_min: -0.2949, eta_lose_max: 0.1118, eta_lose_std: 0.0372
p_win_average: 0.2601, p_win_min: 0.0884, p_win_max: 0.4277, p_win_std: 0.0422
p_lose_average: 0.2751, p_lose_min: 0.0942, p_lose_max: 0.3945, p_lose_std: 0.0383

eval_z_samples_size: 500
eval_loss: 0.3835, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4844, prediction_2_rate: 0.5156
true_1_rate: 0.8000, true_2_rate: 0.8543
joint log likelihood: tensor(-1049.6259765625)
r_win_average: -0.2789, r_win_min: -4.0000, r_win_max: 2.0781, r_win_std: 0.9960
r_lose_average: -2.0361, r_lose_min: -5.5938, r_lose_max: 0.9219, r_lose_std: 1.2999
eta_win_average: -0.0440, eta_win_min: -0.0874, eta_win_max: 0.0199, eta_win_std: 0.0136
eta_lose_average: -0.0474, eta_lose_min: -0.0908, eta_lose_max: 0.0811, eta_lose_std: 0.0137
p_win_average: -0.0213, p_win_min: -0.1104, p_win_max: 0.0547, p_win_std: 0.0222
p_lose_average: -0.0056, p_lose_min: -0.0850, p_lose_max: 0.0698, p_lose_std: 0.0229

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 10
eval_loss: 0.3838, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4805, prediction_2_rate: 0.5195
true_1_rate: 0.7925, true_2_rate: 0.8543
joint log likelihood: tensor(-1021.4350585938)
r_win_average: -2.0028, r_win_min: -7.2500, r_win_max: 1.0156, r_win_std: 1.4170
r_lose_average: -4.4907, r_lose_min: -9.7500, r_lose_max: -0.1973, r_lose_std: 1.8508
eta_win_average: -3.1634, eta_win_min: -4.1875, eta_win_max: -1.9531, eta_win_std: 0.2805
eta_lose_average: -3.4305, eta_lose_min: -4.2188, eta_lose_max: -2.3750, eta_lose_std: 0.2517
p_win_average: 1.5161, p_win_min: 0.4414, p_win_max: 2.2031, p_win_std: 0.2303
p_lose_average: 1.5930, p_lose_min: 0.6211, p_lose_max: 2.3125, p_lose_std: 0.2034

eval_z_samples_size: 100
eval_loss: 0.3752, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8038, true_2_rate: 0.8381
joint log likelihood: tensor(-1027.6538085938)
r_win_average: -0.0013, r_win_min: -4.9062, r_win_max: 2.8750, r_win_std: 1.3107
r_lose_average: -2.2882, r_lose_min: -6.7188, r_lose_max: 1.6875, r_lose_std: 1.7038
eta_win_average: 0.6897, eta_win_min: 0.5391, eta_win_max: 0.7617, eta_win_std: 0.0345
eta_lose_average: 0.7224, eta_lose_min: 0.6133, eta_lose_max: 0.8086, eta_lose_std: 0.0304
p_win_average: -0.3386, p_win_min: -0.4746, p_win_max: -0.1572, p_win_std: 0.0410
p_lose_average: -0.3544, p_lose_min: -0.4922, p_lose_max: -0.1816, p_lose_std: 0.0384

eval_z_samples_size: 500
eval_loss: 0.3755, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8075, true_2_rate: 0.8421
joint log likelihood: tensor(-1028.7077636719)
r_win_average: -0.0205, r_win_min: -4.9062, r_win_max: 2.9062, r_win_std: 1.3087
r_lose_average: -2.3061, r_lose_min: -6.7188, r_lose_max: 1.6875, r_lose_std: 1.7092
eta_win_average: 0.2962, eta_win_min: 0.2246, eta_win_max: 0.3496, eta_win_std: 0.0234
eta_lose_average: 0.3160, eta_lose_min: 0.2246, eta_lose_max: 0.3750, eta_lose_std: 0.0219
p_win_average: 0.0361, p_win_min: -0.0137, p_win_max: 0.1108, p_win_std: 0.0163
p_lose_average: 0.0339, p_lose_min: -0.0148, p_lose_max: 0.0845, p_lose_std: 0.0149

------------------------------------------------------------------------------------------
[2023-09-18 14:51:48,845] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:02,761] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:02,981] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:03,094] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:03,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:03,228] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:03,250] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:03,264] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:03,272] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 14:52:07,600] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:07,601] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 14:52:08,202] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:08,202] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 14:52:08,202] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-18 14:52:08,284] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:08,284] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 14:52:08,429] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:08,429] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 14:52:08,449] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:08,449] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 14:52:08,475] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:08,475] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 14:52:08,484] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:08,484] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 14:52:08,487] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 14:52:08,487] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 242
cuda seed 242
torch seed 442
cuda seed 442
torch seed 342
cuda seed 342
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
torch seed 142
cuda seed 142
torch seed 542
cuda seed 542
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking/se_cooking_preference-ref_size10-enn_dim64-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.001-enn_decay0.1-reward_lr0.0003-reward_decay0.1-gc1-train_batch_size4
torch seed 42
cuda seed 42
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-18 14:52:46,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-18 14:52:48,884] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-18 14:52:48,884] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-18 14:52:48,886] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-18 14:52:48,891] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-18 14:52:48,891] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-18 14:52:48,891] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-18 14:52:48,892] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-18 14:52:48,892] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-18 14:52:48,894] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-18 14:52:48,894] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-18 14:53:01,865] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-18 14:53:01,866] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-18 14:53:01,868] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 64.72 GB, percent = 6.4%
[2023-09-18 14:53:01,973] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-18 14:53:01,974] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 14:53:01,976] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 64.72 GB, percent = 6.4%
[2023-09-18 14:53:01,977] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-18 14:53:02,070] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-18 14:53:02,070] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 14:53:02,072] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 64.72 GB, percent = 6.4%
[2023-09-18 14:53:02,074] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-18 14:53:02,074] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-18 14:53:02,075] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-18 14:53:02,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-18 14:53:02,077] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-18 14:53:02,078] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-18 14:53:02,079] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-18 14:53:02,079] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-18 14:53:02,079] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-18 14:53:02,080] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-18 14:53:02,081] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-18 14:53:02,081] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-18 14:53:02,083] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-18 14:53:02,083] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-18 14:53:02,084] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f9e23283a10>
[2023-09-18 14:53:02,084] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-18 14:53:02,085] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-18 14:53:02,086] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-18 14:53:02,086] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-18 14:53:02,087] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-18 14:53:02,087] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-18 14:53:02,089] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-18 14:53:02,089] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-18 14:53:02,090] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-18 14:53:02,090] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-18 14:53:02,090] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-18 14:53:02,090] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-18 14:53:02,090] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-18 14:53:02,090] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-18 14:53:02,091] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-18 14:53:02,091] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-18 14:53:02,091] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-18 14:53:02,091] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-18 14:53:02,091] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-18 14:53:02,093] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-18 14:53:02,093] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-18 14:53:02,094] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-18 14:53:02,094] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-18 14:53:02,094] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-18 14:53:02,094] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-18 14:53:02,095] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-18 14:53:02,095] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-18 14:53:02,095] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-18 14:53:02,095] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-18 14:53:02,096] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-18 14:53:02,096] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-18 14:53:02,096] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-18 14:53:02,096] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-18 14:53:02,098] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-18 14:53:02,098] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-18 14:53:02,098] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-18 14:53:02,099] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-18 14:53:02,099] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-18 14:53:02,100] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-18 14:53:02,100] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-18 14:53:02,100] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-18 14:53:02,100] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-18 14:53:02,102] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-18 14:53:02,102] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-18 14:53:02,102] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-18 14:53:02,102] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-18 14:53:02,103] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-18 14:53:02,103] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-18 14:53:02,103] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-18 14:53:02,103] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-18 14:53:02,103] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-18 14:53:02,105] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-18 14:53:02,105] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-18 14:53:02,105] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-18 14:53:02,105] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-18 14:53:02,106] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-18 14:53:02,107] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-18 14:53:02,107] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-18 14:53:02,107] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-18 14:53:02,107] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 10
eval_loss: 1.0479, accuracy: 0.5527
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5660, true_2_rate: 0.5385
joint log likelihood: tensor(-5518.0546875000)
r_win_average: -3.1717, r_win_min: -7.5938, r_win_max: 2.8750, r_win_std: 1.7469
r_lose_average: -5.1442, r_lose_min: -10.3125, r_lose_max: 1.2109, r_lose_std: 1.6710
eta_win_average: -1.0420, eta_win_min: -3.1562, eta_win_max: 1.4453, eta_win_std: 0.9331
eta_lose_average: -1.4266, eta_lose_min: -4.1562, eta_lose_max: 1.1328, eta_lose_std: 0.9611
p_win_average: -1.1312, p_win_min: -3.4219, p_win_max: 2.5625, p_win_std: 0.9894
p_lose_average: -1.5618, p_lose_min: -3.7344, p_lose_max: 1.6562, p_lose_std: 0.8941

eval_z_samples_size: 100
eval_loss: 1.2832, accuracy: 0.4883
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.4755, true_2_rate: 0.5020
joint log likelihood: tensor(-5627.4843750000)
r_win_average: -1.5859, r_win_min: -6.5312, r_win_max: 2.2812, r_win_std: 1.4015
r_lose_average: -3.6530, r_lose_min: -8.0625, r_lose_max: -0.0303, r_lose_std: 1.7069
eta_win_average: -0.2298, eta_win_min: -1.1953, eta_win_max: 0.4160, eta_win_std: 0.2666
eta_lose_average: -0.3831, eta_lose_min: -1.3047, eta_lose_max: 0.4961, eta_lose_std: 0.3045
p_win_average: -0.7144, p_win_min: -1.4375, p_win_max: 0.4336, p_win_std: 0.2629
p_lose_average: -0.7574, p_lose_min: -1.5469, p_lose_max: 0.0542, p_lose_std: 0.2582

eval_z_samples_size: 500
eval_loss: 1.1914, accuracy: 0.5000
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.4981, true_2_rate: 0.5020
joint log likelihood: tensor(-5634.5312500000)
r_win_average: -1.0486, r_win_min: -5.2500, r_win_max: 3.1719, r_win_std: 1.3644
r_lose_average: -2.9769, r_lose_min: -7.2812, r_lose_max: 0.5742, r_lose_std: 1.6074
eta_win_average: -0.0901, eta_win_min: -0.4277, eta_win_max: 0.4668, eta_win_std: 0.1447
eta_lose_average: -0.1351, eta_lose_min: -0.5664, eta_lose_max: 0.5234, eta_lose_std: 0.1383
p_win_average: -0.3304, p_win_min: -0.7500, p_win_max: 0.1182, p_win_std: 0.1330
p_lose_average: -0.3155, p_lose_min: -0.8086, p_lose_max: 0.0835, p_lose_std: 0.1352

------------------------------------------------------------------------------------------
epoch 1 step 50 evaluation
eval_z_samples_size: 10
eval_loss: 0.4631, accuracy: 0.7852
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.8000, true_2_rate: 0.7692
joint log likelihood: tensor(-1598.3437500000)
r_win_average: -3.4457, r_win_min: -7.5625, r_win_max: 0.5664, r_win_std: 1.2422
r_lose_average: -5.1941, r_lose_min: -8.5000, r_lose_max: -0.8203, r_lose_std: 1.4405
eta_win_average: -5.0787, eta_win_min: -6.0625, eta_win_max: -3.3750, eta_win_std: 0.4306
eta_lose_average: -5.4664, eta_lose_min: -6.2188, eta_lose_max: -3.4531, eta_lose_std: 0.3678
p_win_average: 1.9293, p_win_min: 0.9844, p_win_max: 3.2188, p_win_std: 0.2622
p_lose_average: 2.0999, p_lose_min: 1.0000, p_lose_max: 3.0000, p_lose_std: 0.2508

eval_z_samples_size: 100
eval_loss: 0.4546, accuracy: 0.7969
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5254, prediction_2_rate: 0.4746
true_1_rate: 0.8113, true_2_rate: 0.7814
joint log likelihood: tensor(-1561.8359375000)
r_win_average: 2.9192, r_win_min: -0.1357, r_win_max: 5.3438, r_win_std: 0.8542
r_lose_average: 1.5937, r_lose_min: -1.0234, r_lose_max: 4.6875, r_lose_std: 1.0788
eta_win_average: 3.6773, eta_win_min: 2.7812, eta_win_max: 4.2500, eta_win_std: 0.2134
eta_lose_average: 3.8723, eta_lose_min: 2.4688, eta_lose_max: 4.4062, eta_lose_std: 0.2131
p_win_average: -0.4696, p_win_min: -0.6992, p_win_max: -0.2656, p_win_std: 0.0638
p_lose_average: -0.4429, p_lose_min: -0.8477, p_lose_max: -0.2451, p_lose_std: 0.0638

eval_z_samples_size: 500
eval_loss: 0.4473, accuracy: 0.7969
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.8075, true_2_rate: 0.7854
joint log likelihood: tensor(-1571.5937500000)
r_win_average: -0.1388, r_win_min: -3.6875, r_win_max: 2.7188, r_win_std: 0.9804
r_lose_average: -1.6397, r_lose_min: -4.5312, r_lose_max: 1.7734, r_lose_std: 1.2354
eta_win_average: 0.1778, eta_win_min: 0.0532, eta_win_max: 0.2871, eta_win_std: 0.0339
eta_lose_average: 0.2075, eta_lose_min: 0.0635, eta_lose_max: 0.2852, eta_lose_std: 0.0311
p_win_average: -0.0297, p_win_min: -0.1592, p_win_max: 0.0361, p_win_std: 0.0270
p_lose_average: -0.0102, p_lose_min: -0.1572, p_lose_max: 0.0698, p_lose_std: 0.0256

------------------------------------------------------------------------------------------
epoch 1 step 100 evaluation
eval_z_samples_size: 10
eval_loss: 0.4565, accuracy: 0.7871
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.7774, true_2_rate: 0.7976
joint log likelihood: tensor(-1605.8652343750)
r_win_average: 1.1788, r_win_min: -2.0312, r_win_max: 3.1094, r_win_std: 0.8241
r_lose_average: -0.3223, r_lose_min: -4.4688, r_lose_max: 2.3594, r_lose_std: 1.3778
eta_win_average: 0.5152, eta_win_min: 0.2148, eta_win_max: 1.1641, eta_win_std: 0.1225
eta_lose_average: 0.5796, eta_lose_min: 0.1689, eta_lose_max: 1.0938, eta_lose_std: 0.1295
p_win_average: 0.2059, p_win_min: -0.2090, p_win_max: 0.6836, p_win_std: 0.1001
p_lose_average: 0.1969, p_lose_min: -0.4258, p_lose_max: 0.6406, p_lose_std: 0.1025

eval_z_samples_size: 100
eval_loss: 0.4473, accuracy: 0.7988
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.7887, true_2_rate: 0.8097
joint log likelihood: tensor(-1563.1953125000)
r_win_average: 0.0676, r_win_min: -3.2188, r_win_max: 1.9453, r_win_std: 0.8955
r_lose_average: -1.5493, r_lose_min: -5.9688, r_lose_max: 1.4062, r_lose_std: 1.5056
eta_win_average: -0.4716, eta_win_min: -0.6523, eta_win_max: -0.2871, eta_win_std: 0.0534
eta_lose_average: -0.5138, eta_lose_min: -0.7422, eta_lose_max: -0.3145, eta_lose_std: 0.0546
p_win_average: 0.0789, p_win_min: -0.1079, p_win_max: 0.2617, p_win_std: 0.0402
p_lose_average: 0.0660, p_lose_min: -0.0967, p_lose_max: 0.3711, p_lose_std: 0.0474

eval_z_samples_size: 500
eval_loss: 0.4480, accuracy: 0.7949
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.7962, true_2_rate: 0.7935
joint log likelihood: tensor(-1564.2187500000)
r_win_average: 0.4850, r_win_min: -2.5625, r_win_max: 2.2812, r_win_std: 0.8657
r_lose_average: -1.0921, r_lose_min: -5.4375, r_lose_max: 1.7578, r_lose_std: 1.4591
eta_win_average: -0.1191, eta_win_min: -0.2305, eta_win_max: 0.0461, eta_win_std: 0.0416
eta_lose_average: -0.1602, eta_lose_min: -0.2832, eta_lose_max: 0.0018, eta_lose_std: 0.0466
p_win_average: 0.1428, p_win_min: 0.0703, p_win_max: 0.2383, p_win_std: 0.0276
p_lose_average: 0.1703, p_lose_min: 0.0659, p_lose_max: 0.2773, p_lose_std: 0.0342

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 10
eval_loss: 0.4150, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8000, true_2_rate: 0.8381
joint log likelihood: tensor(-1202.7597656250)
r_win_average: 4.8078, r_win_min: 2.2031, r_win_max: 6.9062, r_win_std: 0.9080
r_lose_average: 3.2542, r_lose_min: -0.4102, r_lose_max: 5.9375, r_lose_std: 1.2013
eta_win_average: 5.9685, eta_win_min: 4.4375, eta_win_max: 7.6250, eta_win_std: 0.4827
eta_lose_average: 6.5114, eta_lose_min: 4.4062, eta_lose_max: 8.3750, eta_lose_std: 0.5891
p_win_average: -0.3288, p_win_min: -0.8867, p_win_max: 0.3379, p_win_std: 0.1760
p_lose_average: -0.1875, p_lose_min: -0.8789, p_lose_max: 0.6133, p_lose_std: 0.2306

eval_z_samples_size: 100
eval_loss: 0.3955, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8038, true_2_rate: 0.8381
joint log likelihood: tensor(-1224.6718750000)
r_win_average: -1.2623, r_win_min: -5.8125, r_win_max: 1.4062, r_win_std: 1.3376
r_lose_average: -3.6704, r_lose_min: -8.8125, r_lose_max: 0.5312, r_lose_std: 1.8964
eta_win_average: -1.1300, eta_win_min: -1.5078, eta_win_max: -0.5547, eta_win_std: 0.1739
eta_lose_average: -1.2892, eta_lose_min: -1.6562, eta_lose_max: -0.6328, eta_lose_std: 0.1528
p_win_average: 0.6762, p_win_min: 0.3457, p_win_max: 0.9258, p_win_std: 0.0700
p_lose_average: 0.7116, p_lose_min: 0.4941, p_lose_max: 1.2109, p_lose_std: 0.0761

eval_z_samples_size: 500
eval_loss: 0.3940, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8038, true_2_rate: 0.8340
joint log likelihood: tensor(-1225.6367187500)
r_win_average: -1.4949, r_win_min: -5.8750, r_win_max: 1.1250, r_win_std: 1.2830
r_lose_average: -3.8167, r_lose_min: -8.9375, r_lose_max: 0.2793, r_lose_std: 1.8284
eta_win_average: -0.6397, eta_win_min: -0.7617, eta_win_max: -0.5469, eta_win_std: 0.0278
eta_lose_average: -0.6660, eta_lose_min: -0.7578, eta_lose_max: -0.5156, eta_lose_std: 0.0292
p_win_average: -0.0477, p_win_min: -0.1147, p_win_max: 0.0605, p_win_std: 0.0214
p_lose_average: -0.0563, p_lose_min: -0.1157, p_lose_max: 0.0752, p_lose_std: 0.0204

------------------------------------------------------------------------------------------
epoch 1 step 200 evaluation
eval_z_samples_size: 10
eval_loss: 0.3887, accuracy: 0.8320
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4746, prediction_2_rate: 0.5254
true_1_rate: 0.7962, true_2_rate: 0.8704
joint log likelihood: tensor(-1129.1054687500)
r_win_average: -4.6667, r_win_min: -8.4375, r_win_max: -2.0938, r_win_std: 1.1108
r_lose_average: -6.5580, r_lose_min: -11.0625, r_lose_max: -2.9688, r_lose_std: 1.4324
eta_win_average: -6.0064, eta_win_min: -6.6875, eta_win_max: -5.2188, eta_win_std: 0.2059
eta_lose_average: -6.1897, eta_lose_min: -6.9062, eta_lose_max: -5.4688, eta_lose_std: 0.2126
p_win_average: 2.0118, p_win_min: 1.3594, p_win_max: 3.0156, p_win_std: 0.2374
p_lose_average: 2.0671, p_lose_min: 1.2031, p_lose_max: 3.1562, p_lose_std: 0.2256

eval_z_samples_size: 100
eval_loss: 0.3860, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4707, prediction_2_rate: 0.5293
true_1_rate: 0.7887, true_2_rate: 0.8704
joint log likelihood: tensor(-1122.3222656250)
r_win_average: 0.8702, r_win_min: -2.4531, r_win_max: 3.2656, r_win_std: 0.9782
r_lose_average: -0.8561, r_lose_min: -4.6562, r_lose_max: 2.0156, r_lose_std: 1.2696
eta_win_average: 0.7343, eta_win_min: 0.5312, eta_win_max: 1.2578, eta_win_std: 0.0757
eta_lose_average: 0.7419, eta_lose_min: 0.4980, eta_lose_max: 1.0625, eta_lose_std: 0.0738
p_win_average: 0.7998, p_win_min: 0.5195, p_win_max: 0.9531, p_win_std: 0.0703
p_lose_average: 0.8446, p_lose_min: 0.5664, p_lose_max: 1.0312, p_lose_std: 0.0628

eval_z_samples_size: 500
eval_loss: 0.3831, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4805, prediction_2_rate: 0.5195
true_1_rate: 0.7962, true_2_rate: 0.8583
joint log likelihood: tensor(-1120.4951171875)
r_win_average: -0.7036, r_win_min: -4.1562, r_win_max: 1.6484, r_win_std: 1.0029
r_lose_average: -2.4735, r_lose_min: -6.4062, r_lose_max: 0.4414, r_lose_std: 1.3025
eta_win_average: -0.1446, eta_win_min: -0.1777, eta_win_max: -0.1030, eta_win_std: 0.0108
eta_lose_average: -0.1470, eta_lose_min: -0.1816, eta_lose_max: -0.1069, eta_lose_std: 0.0118
p_win_average: 0.1033, p_win_min: 0.0525, p_win_max: 0.1650, p_win_std: 0.0181
p_lose_average: 0.1174, p_lose_min: 0.0532, p_lose_max: 0.1875, p_lose_std: 0.0173

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 10
eval_loss: 0.3796, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8038, true_2_rate: 0.8502
joint log likelihood: tensor(-1083.7705078125)
r_win_average: -5.8942, r_win_min: -11.1875, r_win_max: -2.4219, r_win_std: 1.5063
r_lose_average: -8.4149, r_lose_min: -13.6875, r_lose_max: -3.8750, r_lose_std: 1.8661
eta_win_average: -4.1844, eta_win_min: -4.8750, eta_win_max: -3.3281, eta_win_std: 0.2579
eta_lose_average: -4.3948, eta_lose_min: -5.1562, eta_lose_max: -3.5156, eta_lose_std: 0.2179
p_win_average: -0.9894, p_win_min: -1.4922, p_win_max: -0.2471, p_win_std: 0.1617
p_lose_average: -0.9967, p_lose_min: -1.3594, p_lose_max: -0.5430, p_lose_std: 0.1428

eval_z_samples_size: 100
eval_loss: 0.3794, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8075, true_2_rate: 0.8421
joint log likelihood: tensor(-1085.5073242188)
r_win_average: -0.3518, r_win_min: -5.1250, r_win_max: 2.5000, r_win_std: 1.3052
r_lose_average: -2.6554, r_lose_min: -7.2812, r_lose_max: 1.2578, r_lose_std: 1.7168
eta_win_average: -0.1877, eta_win_min: -0.5273, eta_win_max: 0.1167, eta_win_std: 0.1054
eta_lose_average: -0.3214, eta_lose_min: -0.6797, eta_lose_max: 0.0918, eta_lose_std: 0.1176
p_win_average: 0.5543, p_win_min: 0.0991, p_win_max: 0.9727, p_win_std: 0.1347
p_lose_average: 0.6915, p_lose_min: 0.1904, p_lose_max: 1.0703, p_lose_std: 0.1358

eval_z_samples_size: 500
eval_loss: 0.3770, accuracy: 0.8340
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8113, true_2_rate: 0.8583
joint log likelihood: tensor(-1088.3862304688)
r_win_average: 0.1680, r_win_min: -4.6250, r_win_max: 2.9844, r_win_std: 1.3199
r_lose_average: -2.1549, r_lose_min: -6.9688, r_lose_max: 1.8516, r_lose_std: 1.7333
eta_win_average: 0.9784, eta_win_min: 0.7930, eta_win_max: 1.0859, eta_win_std: 0.0340
eta_lose_average: 0.9768, eta_lose_min: 0.8516, eta_lose_max: 1.0781, eta_lose_std: 0.0369
p_win_average: -0.0924, p_win_min: -0.1738, p_win_max: -0.0140, p_win_std: 0.0241
p_lose_average: -0.1056, p_lose_min: -0.1924, p_lose_max: -0.0403, p_lose_std: 0.0255

------------------------------------------------------------------------------------------
[2023-09-18 16:20:08,359] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:22,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:22,993] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:23,062] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:23,279] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:23,279] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:23,309] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:23,350] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:23,364] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 16:20:27,986] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:27,986] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 16:20:28,412] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:28,412] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 16:20:28,448] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:28,448] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 16:20:28,580] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:28,580] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 16:20:28,666] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:28,666] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 16:20:28,666] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-18 16:20:28,687] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:28,687] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 16:20:28,687] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:28,689] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 16:20:28,706] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 16:20:28,706] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 242
cuda seed 242
torch seed 642
cuda seed 642
torch seed 342
cuda seed 342
torch seed 142
cuda seed 142
torch seed 442
cuda seed 442
torch seed 542
cuda seed 542
torch seed 742
cuda seed 742
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking/se_cooking_preference-ref_size10-enn_dim64-num_ref_train500-lr1e-05-weight_decay0.01-enn_lr0.001-enn_decay0.1-reward_lr0.0003-reward_decay0.1-gc1-train_batch_size4
torch seed 42
cuda seed 42
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-18 16:21:06,646] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-18 16:21:08,958] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-18 16:21:08,959] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-18 16:21:08,960] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-18 16:21:08,965] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-18 16:21:08,965] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-18 16:21:08,965] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-18 16:21:08,966] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-18 16:21:08,968] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-18 16:21:08,968] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-18 16:21:08,968] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (26290, False)] 
[2023-09-18 16:21:18,008] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-18 16:21:18,009] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-18 16:21:18,010] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.43 GB, percent = 6.3%
[2023-09-18 16:21:18,110] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-18 16:21:18,111] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 16:21:18,112] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.43 GB, percent = 6.3%
[2023-09-18 16:21:18,114] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-18 16:21:18,229] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-18 16:21:18,230] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 16:21:18,231] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.43 GB, percent = 6.3%
[2023-09-18 16:21:18,233] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-18 16:21:18,233] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-18 16:21:18,234] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-18 16:21:18,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-18 16:21:18,236] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-18 16:21:18,237] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-18 16:21:18,238] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-18 16:21:18,238] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-18 16:21:18,238] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-18 16:21:18,240] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-18 16:21:18,241] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-18 16:21:18,241] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-18 16:21:18,242] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-18 16:21:18,242] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-18 16:21:18,242] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fad92115090>
[2023-09-18 16:21:18,243] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-18 16:21:18,243] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-18 16:21:18,244] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-18 16:21:18,244] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-18 16:21:18,244] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-18 16:21:18,246] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-18 16:21:18,246] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-18 16:21:18,246] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-18 16:21:18,247] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-18 16:21:18,247] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-18 16:21:18,247] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-18 16:21:18,248] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-18 16:21:18,248] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-18 16:21:18,252] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-18 16:21:18,252] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-18 16:21:18,255] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-18 16:21:18,255] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-18 16:21:18,259] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-18 16:21:18,259] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-18 16:21:18,261] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-18 16:21:18,263] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-18 16:21:18,263] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-18 16:21:18,265] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-18 16:21:18,265] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-18 16:21:18,267] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-18 16:21:18,267] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-18 16:21:18,268] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-18 16:21:18,268] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-18 16:21:18,269] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-18 16:21:18,270] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-18 16:21:18,270] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-18 16:21:18,272] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-18 16:21:18,272] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-18 16:21:18,273] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-18 16:21:18,274] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-18 16:21:18,275] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-18 16:21:18,276] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-18 16:21:18,277] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-18 16:21:18,278] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-18 16:21:18,278] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-18 16:21:18,280] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-18 16:21:18,281] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-18 16:21:18,281] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-18 16:21:18,282] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-18 16:21:18,282] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-18 16:21:18,283] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-18 16:21:18,284] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-18 16:21:18,284] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-18 16:21:18,286] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-18 16:21:18,288] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-18 16:21:18,288] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-18 16:21:18,289] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-18 16:21:18,289] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-18 16:21:18,290] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-18 16:21:18,291] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-18 16:21:18,292] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-18 16:21:18,293] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-18 16:21:18,294] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-18 16:21:18,296] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-18 16:21:18,296] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 10
eval_loss: 1.0479, accuracy: 0.5527
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.5660, true_2_rate: 0.5385
joint log likelihood: tensor(-5518.0546875000)
r_win_average: -3.1717, r_win_min: -7.5938, r_win_max: 2.8750, r_win_std: 1.7469
r_lose_average: -5.1442, r_lose_min: -10.3125, r_lose_max: 1.2109, r_lose_std: 1.6710
eta_win_average: -1.0420, eta_win_min: -3.1562, eta_win_max: 1.4453, eta_win_std: 0.9331
eta_lose_average: -1.4266, eta_lose_min: -4.1562, eta_lose_max: 1.1328, eta_lose_std: 0.9611
p_win_average: -1.1312, p_win_min: -3.4219, p_win_max: 2.5625, p_win_std: 0.9894
p_lose_average: -1.5618, p_lose_min: -3.7344, p_lose_max: 1.6562, p_lose_std: 0.8941

eval_z_samples_size: 100
eval_loss: 1.2832, accuracy: 0.4883
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.4755, true_2_rate: 0.5020
joint log likelihood: tensor(-5627.4843750000)
r_win_average: -1.5859, r_win_min: -6.5312, r_win_max: 2.2812, r_win_std: 1.4015
r_lose_average: -3.6530, r_lose_min: -8.0625, r_lose_max: -0.0303, r_lose_std: 1.7069
eta_win_average: -0.2298, eta_win_min: -1.1953, eta_win_max: 0.4160, eta_win_std: 0.2666
eta_lose_average: -0.3831, eta_lose_min: -1.3047, eta_lose_max: 0.4961, eta_lose_std: 0.3045
p_win_average: -0.7144, p_win_min: -1.4375, p_win_max: 0.4336, p_win_std: 0.2629
p_lose_average: -0.7574, p_lose_min: -1.5469, p_lose_max: 0.0542, p_lose_std: 0.2582

eval_z_samples_size: 500
eval_loss: 1.1914, accuracy: 0.5000
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.4981, true_2_rate: 0.5020
joint log likelihood: tensor(-5634.5312500000)
r_win_average: -1.0486, r_win_min: -5.2500, r_win_max: 3.1719, r_win_std: 1.3644
r_lose_average: -2.9769, r_lose_min: -7.2812, r_lose_max: 0.5742, r_lose_std: 1.6074
eta_win_average: -0.0901, eta_win_min: -0.4277, eta_win_max: 0.4668, eta_win_std: 0.1447
eta_lose_average: -0.1351, eta_lose_min: -0.5664, eta_lose_max: 0.5234, eta_lose_std: 0.1383
p_win_average: -0.3304, p_win_min: -0.7500, p_win_max: 0.1182, p_win_std: 0.1330
p_lose_average: -0.3155, p_lose_min: -0.8086, p_lose_max: 0.0835, p_lose_std: 0.1352

------------------------------------------------------------------------------------------
epoch 1 step 50 evaluation
eval_z_samples_size: 10
eval_loss: 0.4956, accuracy: 0.7676
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5156, prediction_2_rate: 0.4844
true_1_rate: 0.7736, true_2_rate: 0.7611
joint log likelihood: tensor(-1663.2500000000)
r_win_average: 3.9066, r_win_min: 1.1953, r_win_max: 5.5625, r_win_std: 0.6798
r_lose_average: 2.8795, r_lose_min: 0.6836, r_lose_max: 5.2500, r_lose_std: 0.8478
eta_win_average: 2.8270, eta_win_min: 1.0078, eta_win_max: 4.4062, eta_win_std: 0.6212
eta_lose_average: 3.2282, eta_lose_min: 0.4004, eta_lose_max: 4.7500, eta_lose_std: 0.7279
p_win_average: 1.4150, p_win_min: 0.7305, p_win_max: 1.9453, p_win_std: 0.1533
p_lose_average: 1.4086, p_lose_min: 0.8633, p_lose_max: 1.8125, p_lose_std: 0.1459

eval_z_samples_size: 100
eval_loss: 0.4502, accuracy: 0.7871
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5312, prediction_2_rate: 0.4688
true_1_rate: 0.8075, true_2_rate: 0.7652
joint log likelihood: tensor(-1641.7500000000)
r_win_average: -0.6255, r_win_min: -4.3125, r_win_max: 1.8984, r_win_std: 0.9257
r_lose_average: -2.0625, r_lose_min: -4.9062, r_lose_max: 1.2109, r_lose_std: 1.1889
eta_win_average: -0.5336, eta_win_min: -0.9805, eta_win_max: -0.1318, eta_win_std: 0.1466
eta_lose_average: -0.4188, eta_lose_min: -1.1797, eta_lose_max: -0.0476, eta_lose_std: 0.1369
p_win_average: 0.1702, p_win_min: -0.0674, p_win_max: 0.2734, p_win_std: 0.0410
p_lose_average: 0.1854, p_lose_min: -0.0072, p_lose_max: 0.2969, p_lose_std: 0.0343

eval_z_samples_size: 500
eval_loss: 0.4509, accuracy: 0.7949
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.8113, true_2_rate: 0.7773
joint log likelihood: tensor(-1641.1171875000)
r_win_average: 0.6466, r_win_min: -3.3125, r_win_max: 3.7656, r_win_std: 1.0458
r_lose_average: -0.9181, r_lose_min: -4.0000, r_lose_max: 2.7344, r_lose_std: 1.2896
eta_win_average: 0.7931, eta_win_min: 0.5508, eta_win_max: 1.1094, eta_win_std: 0.0660
eta_lose_average: 0.7642, eta_lose_min: 0.6680, eta_lose_max: 1.0156, eta_lose_std: 0.0402
p_win_average: 0.1121, p_win_min: -0.0684, p_win_max: 0.2461, p_win_std: 0.0488
p_lose_average: 0.1501, p_lose_min: -0.0327, p_lose_max: 0.2793, p_lose_std: 0.0414

------------------------------------------------------------------------------------------
epoch 1 step 100 evaluation
eval_z_samples_size: 10
eval_loss: 0.4541, accuracy: 0.7949
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.7887, true_2_rate: 0.8016
joint log likelihood: tensor(-1410.7998046875)
r_win_average: 1.5023, r_win_min: -2.0156, r_win_max: 3.3594, r_win_std: 0.8649
r_lose_average: -0.0250, r_lose_min: -4.2500, r_lose_max: 2.8438, r_lose_std: 1.4217
eta_win_average: -0.6819, eta_win_min: -1.2031, eta_win_max: -0.1729, eta_win_std: 0.1618
eta_lose_average: -0.8094, eta_lose_min: -1.3203, eta_lose_max: -0.1309, eta_lose_std: 0.1887
p_win_average: 1.6838, p_win_min: 0.7734, p_win_max: 2.4531, p_win_std: 0.1922
p_lose_average: 1.8224, p_lose_min: 1.0156, p_lose_max: 2.8281, p_lose_std: 0.2093

eval_z_samples_size: 100
eval_loss: 0.4519, accuracy: 0.7930
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.7811, true_2_rate: 0.8057
joint log likelihood: tensor(-1436.4296875000)
r_win_average: -0.1342, r_win_min: -3.5312, r_win_max: 1.6953, r_win_std: 0.8735
r_lose_average: -1.7001, r_lose_min: -5.8438, r_lose_max: 1.1797, r_lose_std: 1.4258
eta_win_average: -0.2948, eta_win_min: -0.6328, eta_win_max: -0.1387, eta_win_std: 0.0834
eta_lose_average: -0.2574, eta_lose_min: -0.6797, eta_lose_max: -0.0913, eta_lose_std: 0.0825
p_win_average: -0.3418, p_win_min: -0.5547, p_win_max: 0.0154, p_win_std: 0.0757
p_lose_average: -0.4038, p_lose_min: -0.6680, p_lose_max: -0.1021, p_lose_std: 0.0741

eval_z_samples_size: 500
eval_loss: 0.4490, accuracy: 0.8008
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.8000, true_2_rate: 0.8016
joint log likelihood: tensor(-1453.4062500000)
r_win_average: 0.2684, r_win_min: -3.0938, r_win_max: 2.0312, r_win_std: 0.8631
r_lose_average: -1.2906, r_lose_min: -5.4375, r_lose_max: 1.5469, r_lose_std: 1.4149
eta_win_average: 0.3083, eta_win_min: 0.1973, eta_win_max: 0.4023, eta_win_std: 0.0271
eta_lose_average: 0.3270, eta_lose_min: 0.2090, eta_lose_max: 0.4316, eta_lose_std: 0.0315
p_win_average: -0.5444, p_win_min: -0.6953, p_win_max: -0.2871, p_win_std: 0.0495
p_lose_average: -0.5762, p_lose_min: -0.8477, p_lose_max: -0.3262, p_lose_std: 0.0529

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 10
eval_loss: 0.4119, accuracy: 0.8145
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.7925, true_2_rate: 0.8381
joint log likelihood: tensor(-1312.9248046875)
r_win_average: 2.8774, r_win_min: -0.1270, r_win_max: 5.5625, r_win_std: 1.0139
r_lose_average: 1.1049, r_lose_min: -2.9688, r_lose_max: 4.5938, r_lose_std: 1.3779
eta_win_average: 3.1380, eta_win_min: 1.7891, eta_win_max: 4.5000, eta_win_std: 0.4860
eta_lose_average: 3.6273, eta_lose_min: 1.5703, eta_lose_max: 4.9688, eta_lose_std: 0.5689
p_win_average: 0.5160, p_win_min: 0.1592, p_win_max: 0.8477, p_win_std: 0.0891
p_lose_average: 0.4957, p_lose_min: 0.1445, p_lose_max: 0.8984, p_lose_std: 0.1013

eval_z_samples_size: 100
eval_loss: 0.3936, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.8075, true_2_rate: 0.8340
joint log likelihood: tensor(-1316.0546875000)
r_win_average: -2.0430, r_win_min: -6.0000, r_win_max: 0.5352, r_win_std: 1.1898
r_lose_average: -4.2141, r_lose_min: -8.9375, r_lose_max: -0.3418, r_lose_std: 1.7099
eta_win_average: -1.9815, eta_win_min: -2.4219, eta_win_max: -1.5625, eta_win_std: 0.1205
eta_lose_average: -1.9126, eta_lose_min: -2.3906, eta_lose_max: -1.4922, eta_lose_std: 0.1309
p_win_average: 0.6925, p_win_min: 0.3613, p_win_max: 0.9062, p_win_std: 0.0673
p_lose_average: 0.7386, p_lose_min: 0.4766, p_lose_max: 0.9844, p_lose_std: 0.0628

eval_z_samples_size: 500
eval_loss: 0.3926, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.8038, true_2_rate: 0.8462
joint log likelihood: tensor(-1312.6269531250)
r_win_average: -0.2801, r_win_min: -4.5625, r_win_max: 2.2969, r_win_std: 1.2452
r_lose_average: -2.5381, r_lose_min: -7.5625, r_lose_max: 1.4609, r_lose_std: 1.7788
eta_win_average: 0.3309, eta_win_min: 0.2461, eta_win_max: 0.4004, eta_win_std: 0.0147
eta_lose_average: 0.3315, eta_lose_min: 0.2656, eta_lose_max: 0.3848, eta_lose_std: 0.0147
p_win_average: 0.1412, p_win_min: 0.0520, p_win_max: 0.2246, p_win_std: 0.0304
p_lose_average: 0.1721, p_lose_min: 0.0620, p_lose_max: 0.2480, p_lose_std: 0.0310

------------------------------------------------------------------------------------------
epoch 1 step 200 evaluation
eval_z_samples_size: 10
eval_loss: 0.3999, accuracy: 0.8184
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4766, prediction_2_rate: 0.5234
true_1_rate: 0.7849, true_2_rate: 0.8543
joint log likelihood: tensor(-1209.6660156250)
r_win_average: 4.1863, r_win_min: 1.2344, r_win_max: 7.0625, r_win_std: 0.9266
r_lose_average: 2.6112, r_lose_min: -1.1250, r_lose_max: 5.4688, r_lose_std: 1.1616
eta_win_average: 4.6883, eta_win_min: 3.3750, eta_win_max: 5.3125, eta_win_std: 0.3910
eta_lose_average: 4.9325, eta_lose_min: 3.2656, eta_lose_max: 5.7812, eta_lose_std: 0.3570
p_win_average: 0.0426, p_win_min: -0.4395, p_win_max: 0.6914, p_win_std: 0.1467
p_lose_average: -0.0406, p_lose_min: -0.5898, p_lose_max: 0.3789, p_lose_std: 0.1467

eval_z_samples_size: 100
eval_loss: 0.3818, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4824, prediction_2_rate: 0.5176
true_1_rate: 0.7962, true_2_rate: 0.8543
joint log likelihood: tensor(-1232.4316406250)
r_win_average: -1.4380, r_win_min: -5.0312, r_win_max: 0.9297, r_win_std: 1.0402
r_lose_average: -3.2602, r_lose_min: -7.2500, r_lose_max: -0.2354, r_lose_std: 1.3412
eta_win_average: -0.5120, eta_win_min: -0.6211, eta_win_max: -0.3379, eta_win_std: 0.0452
eta_lose_average: -0.5480, eta_lose_min: -0.6953, eta_lose_max: -0.3887, eta_lose_std: 0.0409
p_win_average: -0.3966, p_win_min: -0.5430, p_win_max: -0.2129, p_win_std: 0.0474
p_lose_average: -0.4162, p_lose_min: -0.5781, p_lose_max: -0.2812, p_lose_std: 0.0400

eval_z_samples_size: 500
eval_loss: 0.3843, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4785, prediction_2_rate: 0.5215
true_1_rate: 0.7925, true_2_rate: 0.8583
joint log likelihood: tensor(-1235.1718750000)
r_win_average: -0.9895, r_win_min: -4.4062, r_win_max: 1.3047, r_win_std: 0.9838
r_lose_average: -2.7301, r_lose_min: -6.4688, r_lose_max: 0.1079, r_lose_std: 1.2784
eta_win_average: -0.7241, eta_win_min: -0.8594, eta_win_max: -0.5430, eta_win_std: 0.0342
eta_lose_average: -0.7246, eta_lose_min: -0.8438, eta_lose_max: -0.6016, eta_lose_std: 0.0331
p_win_average: 0.2638, p_win_min: 0.1699, p_win_max: 0.3555, p_win_std: 0.0308
p_lose_average: 0.2906, p_lose_min: 0.2002, p_lose_max: 0.3867, p_lose_std: 0.0293

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 10
eval_loss: 0.3792, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4766, prediction_2_rate: 0.5234
true_1_rate: 0.7925, true_2_rate: 0.8623
joint log likelihood: tensor(-1202.0009765625)
r_win_average: 1.4028, r_win_min: -2.7812, r_win_max: 4.5312, r_win_std: 1.2020
r_lose_average: -0.7397, r_lose_min: -5.2812, r_lose_max: 3.1562, r_lose_std: 1.6039
eta_win_average: 1.6259, eta_win_min: 0.5703, eta_win_max: 2.2188, eta_win_std: 0.2973
eta_lose_average: 1.8535, eta_lose_min: 0.8359, eta_lose_max: 2.5000, eta_lose_std: 0.2618
p_win_average: 0.4272, p_win_min: 0.1152, p_win_max: 0.7109, p_win_std: 0.1032
p_lose_average: 0.3471, p_lose_min: -0.0210, p_lose_max: 0.6133, p_lose_std: 0.1128

eval_z_samples_size: 100
eval_loss: 0.3767, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8075, true_2_rate: 0.8462
joint log likelihood: tensor(-1196.2451171875)
r_win_average: -0.0300, r_win_min: -4.9062, r_win_max: 2.8594, r_win_std: 1.3702
r_lose_average: -2.4180, r_lose_min: -7.2812, r_lose_max: 1.6484, r_lose_std: 1.7678
eta_win_average: 1.4169, eta_win_min: 1.2031, eta_win_max: 1.6953, eta_win_std: 0.0848
eta_lose_average: 1.3640, eta_lose_min: 1.1562, eta_lose_max: 1.6875, eta_lose_std: 0.0717
p_win_average: -0.8025, p_win_min: -1.0156, p_win_max: -0.5547, p_win_std: 0.0632
p_lose_average: -0.8357, p_lose_min: -1.0469, p_lose_max: -0.5898, p_lose_std: 0.0613

eval_z_samples_size: 500
eval_loss: 0.3752, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.8038, true_2_rate: 0.8543
joint log likelihood: tensor(-1205.9863281250)
r_win_average: -0.6972, r_win_min: -5.3750, r_win_max: 2.1562, r_win_std: 1.2874
r_lose_average: -2.9605, r_lose_min: -7.5000, r_lose_max: 1.0078, r_lose_std: 1.6889
eta_win_average: -0.0366, eta_win_min: -0.1465, eta_win_max: 0.0618, eta_win_std: 0.0390
eta_lose_average: -0.0025, eta_lose_min: -0.1367, eta_lose_max: 0.0786, eta_lose_std: 0.0340
p_win_average: -0.0163, p_win_min: -0.0618, p_win_max: 0.0439, p_win_std: 0.0171
p_lose_average: -0.0111, p_lose_min: -0.0569, p_lose_max: 0.0320, p_lose_std: 0.0159

------------------------------------------------------------------------------------------
[2023-09-18 17:48:43,608] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,185] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,304] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,568] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,569] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,614] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,657] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,725] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:48:57,731] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 17:49:02,474] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:02,474] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 17:49:02,496] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:02,496] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 17:49:03,039] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:03,039] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 17:49:03,049] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:03,049] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 17:49:03,054] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:03,054] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 17:49:03,054] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-18 17:49:03,058] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:03,059] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 17:49:03,060] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:03,060] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 17:49:03,075] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 17:49:03,075] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 342
cuda seed 342
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
torch seed 142
cuda seed 142
torch seed 242
cuda seed 242
torch seed 442
cuda seed 442
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking/se_cooking_preference-ref_size10-enn_dim128-num_ref_train10-lr1e-05-weight_decay0.01-enn_lr0.001-enn_decay0.1-reward_lr0.0003-reward_decay0.1-gc1-train_batch_size4
torch seed 42torch seed
 542
cuda seed 42
cuda seed 542
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-18 17:49:40,836] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-18 17:49:43,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-18 17:49:43,088] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-18 17:49:43,089] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-18 17:49:43,094] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-18 17:49:43,094] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-18 17:49:43,094] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-18 17:49:43,095] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-18 17:49:43,096] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-18 17:49:43,096] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-18 17:49:43,098] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-18 17:49:56,063] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-18 17:49:56,064] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-18 17:49:56,065] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.47 GB, percent = 6.3%
[2023-09-18 17:49:56,166] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-18 17:49:56,167] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 17:49:56,168] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.47 GB, percent = 6.3%
[2023-09-18 17:49:56,170] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-18 17:49:56,262] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-18 17:49:56,263] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 17:49:56,264] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.47 GB, percent = 6.3%
[2023-09-18 17:49:56,266] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-18 17:49:56,267] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-18 17:49:56,268] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-18 17:49:56,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-18 17:49:56,270] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-18 17:49:56,271] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-18 17:49:56,272] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-18 17:49:56,272] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-18 17:49:56,274] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-18 17:49:56,274] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-18 17:49:56,275] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-18 17:49:56,275] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-18 17:49:56,276] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-18 17:49:56,276] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-18 17:49:56,276] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f572758be10>
[2023-09-18 17:49:56,277] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-18 17:49:56,277] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-18 17:49:56,278] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-18 17:49:56,278] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-18 17:49:56,278] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-18 17:49:56,279] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-18 17:49:56,280] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-18 17:49:56,281] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-18 17:49:56,282] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-18 17:49:56,282] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-18 17:49:56,282] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-18 17:49:56,282] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-18 17:49:56,282] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-18 17:49:56,282] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-18 17:49:56,283] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-18 17:49:56,284] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-18 17:49:56,284] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-18 17:49:56,285] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-18 17:49:56,285] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-18 17:49:56,286] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-18 17:49:56,287] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-18 17:49:56,288] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-18 17:49:56,289] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-18 17:49:56,290] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-18 17:49:56,291] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-18 17:49:56,291] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-18 17:49:56,292] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-18 17:49:56,292] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-18 17:49:56,293] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-18 17:49:56,294] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-18 17:49:56,294] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-18 17:49:56,295] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-18 17:49:56,296] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-18 17:49:56,298] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 10
eval_loss: 1.1035, accuracy: 0.5508
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.5547, true_2_rate: 0.5466
joint log likelihood: tensor(-6026.9687500000)
r_win_average: -4.2373, r_win_min: -9.1250, r_win_max: 1.4766, r_win_std: 1.7288
r_lose_average: -6.3375, r_lose_min: -11.6875, r_lose_max: -1.5078, r_lose_std: 1.5926
eta_win_average: -1.2640, eta_win_min: -3.7500, eta_win_max: 1.7500, eta_win_std: 0.8522
eta_lose_average: -1.7289, eta_lose_min: -4.5938, eta_lose_max: 0.6875, eta_lose_std: 0.8353
p_win_average: -2.1518, p_win_min: -4.5000, p_win_max: 0.8125, p_win_std: 0.9741
p_lose_average: -2.2765, p_lose_min: -5.1875, p_lose_max: 0.5586, p_lose_std: 0.9590

eval_z_samples_size: 100
eval_loss: 1.2949, accuracy: 0.5078
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5170, true_2_rate: 0.4980
joint log likelihood: tensor(-5962.9687500000)
r_win_average: -0.5011, r_win_min: -4.6875, r_win_max: 3.7188, r_win_std: 1.4510
r_lose_average: -2.6510, r_lose_min: -7.0938, r_lose_max: 1.1797, r_lose_std: 1.7417
eta_win_average: -0.4239, eta_win_min: -1.3125, eta_win_max: 0.5977, eta_win_std: 0.2789
eta_lose_average: -0.3901, eta_lose_min: -1.3125, eta_lose_max: 0.6211, eta_lose_std: 0.2998
p_win_average: 0.5772, p_win_min: -0.7617, p_win_max: 1.6797, p_win_std: 0.4425
p_lose_average: 0.2388, p_lose_min: -1.1328, p_lose_max: 1.4922, p_lose_std: 0.4913

eval_z_samples_size: 500
eval_loss: 1.1865, accuracy: 0.5039
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5132, true_2_rate: 0.4939
joint log likelihood: tensor(-5953.4375000000)
r_win_average: -0.7616, r_win_min: -4.9688, r_win_max: 3.1406, r_win_std: 1.3591
r_lose_average: -2.6833, r_lose_min: -6.7500, r_lose_max: 0.8945, r_lose_std: 1.5836
eta_win_average: 0.0247, eta_win_min: -0.4941, eta_win_max: 0.4160, eta_win_std: 0.1512
eta_lose_average: -0.0758, eta_lose_min: -0.5273, eta_lose_max: 0.3496, eta_lose_std: 0.1756
p_win_average: -0.1567, p_win_min: -0.6094, p_win_max: 0.4785, p_win_std: 0.1957
p_lose_average: -0.0832, p_lose_min: -0.5391, p_lose_max: 0.4590, p_lose_std: 0.1986

------------------------------------------------------------------------------------------
epoch 1 step 50 evaluation
eval_z_samples_size: 10
eval_loss: 0.4915, accuracy: 0.7695
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.7698, true_2_rate: 0.7692
joint log likelihood: tensor(-1471.1484375000)
r_win_average: 7.2574, r_win_min: 4.6875, r_win_max: 9.8750, r_win_std: 0.7907
r_lose_average: 6.2009, r_lose_min: 3.8438, r_lose_max: 8.8750, r_lose_std: 0.8604
eta_win_average: 5.5353, eta_win_min: 3.9219, eta_win_max: 6.4688, eta_win_std: 0.2851
eta_lose_average: 5.7619, eta_lose_min: 3.2656, eta_lose_max: 6.6562, eta_lose_std: 0.3504
p_win_average: 1.8872, p_win_min: 0.4668, p_win_max: 2.4375, p_win_std: 0.2492
p_lose_average: 1.9505, p_lose_min: 0.7773, p_lose_max: 2.2812, p_lose_std: 0.1896

eval_z_samples_size: 100
eval_loss: 0.4563, accuracy: 0.8008
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.8113, true_2_rate: 0.7895
joint log likelihood: tensor(-1460.5585937500)
r_win_average: 1.8897, r_win_min: -1.0703, r_win_max: 5.1875, r_win_std: 0.9245
r_lose_average: 0.5882, r_lose_min: -1.8906, r_lose_max: 3.7500, r_lose_std: 1.0332
eta_win_average: 1.8183, eta_win_min: 0.9766, eta_win_max: 2.1562, eta_win_std: 0.1481
eta_lose_average: 1.9289, eta_lose_min: 1.0000, eta_lose_max: 2.2344, eta_lose_std: 0.1231
p_win_average: 0.2114, p_win_min: -0.0393, p_win_max: 0.4180, p_win_std: 0.0504
p_lose_average: 0.1972, p_lose_min: 0.0801, p_lose_max: 0.3477, p_lose_std: 0.0381

eval_z_samples_size: 500
eval_loss: 0.4512, accuracy: 0.8008
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5215, prediction_2_rate: 0.4785
true_1_rate: 0.8113, true_2_rate: 0.7895
joint log likelihood: tensor(-1468.2812500000)
r_win_average: 0.0233, r_win_min: -3.0625, r_win_max: 3.1094, r_win_std: 0.9142
r_lose_average: -1.2987, r_lose_min: -3.8281, r_lose_max: 1.8594, r_lose_std: 1.0593
eta_win_average: 0.5284, eta_win_min: -0.0398, eta_win_max: 0.8320, eta_win_std: 0.1482
eta_lose_average: 0.6311, eta_lose_min: 0.0204, eta_lose_max: 0.8086, eta_lose_std: 0.1039
p_win_average: -0.3656, p_win_min: -0.7070, p_win_max: 0.0003, p_win_std: 0.0571
p_lose_average: -0.3913, p_lose_min: -0.5469, p_lose_max: -0.1592, p_lose_std: 0.0488

------------------------------------------------------------------------------------------
epoch 1 step 100 evaluation
eval_z_samples_size: 10
eval_loss: 0.4597, accuracy: 0.7832
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.7660, true_2_rate: 0.8016
joint log likelihood: tensor(-1281.0488281250)
r_win_average: 3.6150, r_win_min: 0.0923, r_win_max: 5.5312, r_win_std: 0.9215
r_lose_average: 2.0020, r_lose_min: -2.6562, r_lose_max: 5.0625, r_lose_std: 1.5343
eta_win_average: 4.2182, eta_win_min: 3.2031, eta_win_max: 4.9375, eta_win_std: 0.1592
eta_lose_average: 4.3176, eta_lose_min: 3.4062, eta_lose_max: 4.7812, eta_lose_std: 0.1583
p_win_average: -1.2569, p_win_min: -2.1562, p_win_max: -0.3750, p_win_std: 0.1701
p_lose_average: -1.3859, p_lose_min: -2.1719, p_lose_max: -0.7656, p_lose_std: 0.1821

eval_z_samples_size: 100
eval_loss: 0.4512, accuracy: 0.7930
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.7849, true_2_rate: 0.8016
joint log likelihood: tensor(-1305.8945312500)
r_win_average: 1.2001, r_win_min: -2.2656, r_win_max: 3.0156, r_win_std: 0.9162
r_lose_average: -0.4276, r_lose_min: -5.0000, r_lose_max: 2.5156, r_lose_std: 1.5150
eta_win_average: -0.1758, eta_win_min: -0.2852, eta_win_max: -0.0233, eta_win_std: 0.0319
eta_lose_average: -0.2003, eta_lose_min: -0.2969, eta_lose_max: -0.0302, eta_lose_std: 0.0335
p_win_average: 0.7182, p_win_min: 0.5742, p_win_max: 0.8359, p_win_std: 0.0322
p_lose_average: 0.7073, p_lose_min: 0.5664, p_lose_max: 0.8320, p_lose_std: 0.0316

eval_z_samples_size: 500
eval_loss: 0.4512, accuracy: 0.7891
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.7774, true_2_rate: 0.8016
joint log likelihood: tensor(-1304.7812500000)
r_win_average: 0.2757, r_win_min: -3.0156, r_win_max: 2.1250, r_win_std: 0.8771
r_lose_average: -1.2955, r_lose_min: -5.7500, r_lose_max: 1.6094, r_lose_std: 1.4557
eta_win_average: -0.0611, eta_win_min: -0.2773, eta_win_max: 0.0977, eta_win_std: 0.0560
eta_lose_average: -0.0167, eta_lose_min: -0.2500, eta_lose_max: 0.1396, eta_lose_std: 0.0594
p_win_average: -0.3207, p_win_min: -0.4355, p_win_max: -0.1582, p_win_std: 0.0352
p_lose_average: -0.3451, p_lose_min: -0.4688, p_lose_max: -0.2012, p_lose_std: 0.0348

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 10
eval_loss: 0.3984, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4863, prediction_2_rate: 0.5137
true_1_rate: 0.7962, true_2_rate: 0.8462
joint log likelihood: tensor(-1089.6352539062)
r_win_average: -1.7235, r_win_min: -6.0938, r_win_max: 1.1875, r_win_std: 1.3395
r_lose_average: -4.1058, r_lose_min: -10.6875, r_lose_max: 0.0938, r_lose_std: 1.9286
eta_win_average: -1.0459, eta_win_min: -1.9922, eta_win_max: -0.0383, eta_win_std: 0.1813
eta_lose_average: -1.1456, eta_lose_min: -2.0312, eta_lose_max: -0.7031, eta_lose_std: 0.2037
p_win_average: -0.1797, p_win_min: -1.6406, p_win_max: 0.2559, p_win_std: 0.2107
p_lose_average: -0.1878, p_lose_min: -1.2500, p_lose_max: 0.3105, p_lose_std: 0.2031

eval_z_samples_size: 100
eval_loss: 0.3965, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.8151, true_2_rate: 0.8381
joint log likelihood: tensor(-1113.7431640625)
r_win_average: -1.5620, r_win_min: -6.2188, r_win_max: 1.1641, r_win_std: 1.3157
r_lose_average: -3.9139, r_lose_min: -9.2500, r_lose_max: 0.1699, r_lose_std: 1.8438
eta_win_average: -0.0761, eta_win_min: -0.3516, eta_win_max: 0.2930, eta_win_std: 0.0516
eta_lose_average: -0.0985, eta_lose_min: -0.2930, eta_lose_max: 0.0549, eta_lose_std: 0.0487
p_win_average: -0.9970, p_win_min: -1.4219, p_win_max: -0.7656, p_win_std: 0.0595
p_lose_average: -1.0339, p_lose_min: -1.2344, p_lose_max: -0.8164, p_lose_std: 0.0630

eval_z_samples_size: 500
eval_loss: 0.3938, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8151, true_2_rate: 0.8462
joint log likelihood: tensor(-1118.5454101562)
r_win_average: -1.0282, r_win_min: -5.5938, r_win_max: 1.6641, r_win_std: 1.2719
r_lose_average: -3.3041, r_lose_min: -8.5625, r_lose_max: 0.6953, r_lose_std: 1.7811
eta_win_average: -0.8674, eta_win_min: -1.0703, eta_win_max: -0.6641, eta_win_std: 0.0364
eta_lose_average: -0.8665, eta_lose_min: -1.0156, eta_lose_max: -0.7539, eta_lose_std: 0.0311
p_win_average: 0.3282, p_win_min: 0.1206, p_win_max: 0.4336, p_win_std: 0.0371
p_lose_average: 0.3438, p_lose_min: 0.0688, p_lose_max: 0.4609, p_lose_std: 0.0393

------------------------------------------------------------------------------------------
epoch 1 step 200 evaluation
eval_z_samples_size: 10
eval_loss: 0.3877, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8075, true_2_rate: 0.8381
joint log likelihood: tensor(-1095.1210937500)
r_win_average: -0.1588, r_win_min: -3.7188, r_win_max: 2.1719, r_win_std: 1.0408
r_lose_average: -1.9457, r_lose_min: -5.4688, r_lose_max: 1.3125, r_lose_std: 1.3211
eta_win_average: 1.5077, eta_win_min: 0.9297, eta_win_max: 1.8672, eta_win_std: 0.0968
eta_lose_average: 1.5297, eta_lose_min: 1.2344, eta_lose_max: 1.8594, eta_lose_std: 0.0929
p_win_average: -1.3028, p_win_min: -1.7109, p_win_max: -0.8281, p_win_std: 0.1011
p_lose_average: -1.3400, p_lose_min: -1.9062, p_lose_max: -0.9883, p_lose_std: 0.1043

eval_z_samples_size: 100
eval_loss: 0.3823, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.8000, true_2_rate: 0.8421
joint log likelihood: tensor(-1064.3115234375)
r_win_average: -0.5740, r_win_min: -4.1875, r_win_max: 1.8203, r_win_std: 1.0205
r_lose_average: -2.3406, r_lose_min: -5.8125, r_lose_max: 0.7656, r_lose_std: 1.3014
eta_win_average: -0.2572, eta_win_min: -0.3535, eta_win_max: -0.1289, eta_win_std: 0.0230
eta_lose_average: -0.2463, eta_lose_min: -0.3555, eta_lose_max: -0.1689, eta_lose_std: 0.0236
p_win_average: 0.0457, p_win_min: -0.0669, p_win_max: 0.2637, p_win_std: 0.0404
p_lose_average: 0.0418, p_lose_min: -0.0469, p_lose_max: 0.1895, p_lose_std: 0.0336

eval_z_samples_size: 500
eval_loss: 0.3833, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.8038, true_2_rate: 0.8462
joint log likelihood: tensor(-1069.4775390625)
r_win_average: -0.4204, r_win_min: -4.0312, r_win_max: 2.0312, r_win_std: 1.0151
r_lose_average: -2.1804, r_lose_min: -5.7812, r_lose_max: 0.7383, r_lose_std: 1.3103
eta_win_average: 0.3804, eta_win_min: 0.1475, eta_win_max: 0.4980, eta_win_std: 0.0642
eta_lose_average: 0.4244, eta_lose_min: 0.2207, eta_lose_max: 0.5469, eta_lose_std: 0.0489
p_win_average: -0.4383, p_win_min: -0.6484, p_win_max: -0.2285, p_win_std: 0.0434
p_lose_average: -0.4690, p_lose_min: -0.5742, p_lose_max: -0.3125, p_lose_std: 0.0386

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 10
eval_loss: 0.3782, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5020, prediction_2_rate: 0.4980
true_1_rate: 0.8189, true_2_rate: 0.8381
joint log likelihood: tensor(-1044.1105957031)
r_win_average: -0.2122, r_win_min: -5.1875, r_win_max: 2.6406, r_win_std: 1.3521
r_lose_average: -2.5302, r_lose_min: -6.7188, r_lose_max: 1.7188, r_lose_std: 1.6824
eta_win_average: -2.4259, eta_win_min: -3.0312, eta_win_max: -1.1172, eta_win_std: 0.2679
eta_lose_average: -2.6618, eta_lose_min: -3.2344, eta_lose_max: -1.9219, eta_lose_std: 0.2051
p_win_average: 2.7311, p_win_min: 1.6406, p_win_max: 3.5000, p_win_std: 0.2301
p_lose_average: 2.9144, p_lose_min: 2.1719, p_lose_max: 3.5469, p_lose_std: 0.2175

eval_z_samples_size: 100
eval_loss: 0.3745, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8113, true_2_rate: 0.8502
joint log likelihood: tensor(-1044.3664550781)
r_win_average: -2.5566, r_win_min: -7.5312, r_win_max: 0.3477, r_win_std: 1.3127
r_lose_average: -4.8243, r_lose_min: -9.0625, r_lose_max: -0.8945, r_lose_std: 1.6867
eta_win_average: -1.0457, eta_win_min: -1.2812, eta_win_max: -0.6758, eta_win_std: 0.0590
eta_lose_average: -1.0330, eta_lose_min: -1.3359, eta_lose_max: -0.8789, eta_lose_std: 0.0557
p_win_average: -0.9952, p_win_min: -1.3281, p_win_max: -0.7227, p_win_std: 0.0678
p_lose_average: -1.0072, p_lose_min: -1.2031, p_lose_max: -0.7852, p_lose_std: 0.0653

eval_z_samples_size: 500
eval_loss: 0.3760, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8075, true_2_rate: 0.8462
joint log likelihood: tensor(-1042.0251464844)
r_win_average: -0.8309, r_win_min: -5.7500, r_win_max: 2.0000, r_win_std: 1.3221
r_lose_average: -3.1105, r_lose_min: -7.2188, r_lose_max: 0.8398, r_lose_std: 1.6959
eta_win_average: -0.3381, eta_win_min: -0.3945, eta_win_max: -0.2812, eta_win_std: 0.0165
eta_lose_average: -0.3331, eta_lose_min: -0.3906, eta_lose_max: -0.2500, eta_lose_std: 0.0183
p_win_average: 0.0226, p_win_min: -0.0505, p_win_max: 0.1445, p_win_std: 0.0297
p_lose_average: 0.0077, p_lose_min: -0.0596, p_lose_max: 0.1157, p_lose_std: 0.0250

------------------------------------------------------------------------------------------
[2023-09-18 19:17:04,862] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:20,737] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:20,913] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:20,954] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:21,131] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:21,198] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:21,205] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:21,217] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:21,222] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 19:17:25,769] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:25,769] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 19:17:26,303] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:26,303] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 19:17:26,327] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:26,328] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 19:17:26,346] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:26,346] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 19:17:26,346] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-18 19:17:26,527] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:26,527] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 19:17:26,529] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:26,529] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 19:17:26,538] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:26,538] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 19:17:26,555] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 19:17:26,555] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 342
cuda seed 342
torch seed 442
cuda seed 442
torch seed 742
cuda seed 742
torch seed 642
cuda seed 642
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking/se_cooking_preference-ref_size10-enn_dim128-num_ref_train100-lr1e-05-weight_decay0.01-enn_lr0.001-enn_decay0.1-reward_lr0.0003-reward_decay0.1-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 542
cuda seed 542
torch seed 242
cuda seed 242
torch seed 142
cuda seed 142
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-18 19:18:04,504] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-18 19:18:06,656] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-18 19:18:06,657] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-18 19:18:06,657] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-18 19:18:06,662] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-18 19:18:06,662] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-18 19:18:06,662] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-18 19:18:06,662] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-18 19:18:06,662] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-18 19:18:06,662] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-18 19:18:06,662] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-18 19:18:19,730] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-18 19:18:19,731] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-18 19:18:19,732] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.46 GB, percent = 6.3%
[2023-09-18 19:18:19,839] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-18 19:18:19,839] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 19:18:19,842] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.46 GB, percent = 6.3%
[2023-09-18 19:18:19,843] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-18 19:18:19,937] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-18 19:18:19,938] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 19:18:19,940] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.46 GB, percent = 6.3%
[2023-09-18 19:18:19,944] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-18 19:18:19,944] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-18 19:18:19,945] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-18 19:18:19,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-18 19:18:19,947] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-18 19:18:19,948] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-18 19:18:19,949] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-18 19:18:19,949] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-18 19:18:19,949] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-18 19:18:19,949] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-18 19:18:19,950] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-18 19:18:19,950] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-18 19:18:19,950] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-18 19:18:19,950] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-18 19:18:19,952] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f10b2bfee10>
[2023-09-18 19:18:19,952] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-18 19:18:19,952] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-18 19:18:19,952] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-18 19:18:19,953] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-18 19:18:19,953] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-18 19:18:19,953] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-18 19:18:19,953] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-18 19:18:19,953] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-18 19:18:19,954] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-18 19:18:19,954] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-18 19:18:19,954] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-18 19:18:19,954] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-18 19:18:19,954] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-18 19:18:19,955] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-18 19:18:19,955] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-18 19:18:19,955] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-18 19:18:19,955] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-18 19:18:19,955] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-18 19:18:19,956] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-18 19:18:19,956] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-18 19:18:19,957] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-18 19:18:19,958] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-18 19:18:19,958] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-18 19:18:19,958] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-18 19:18:19,958] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-18 19:18:19,960] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-18 19:18:19,960] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-18 19:18:19,960] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-18 19:18:19,960] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-18 19:18:19,961] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-18 19:18:19,961] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-18 19:18:19,961] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-18 19:18:19,961] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-18 19:18:19,961] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-18 19:18:19,962] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-18 19:18:19,963] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-18 19:18:19,963] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-18 19:18:19,964] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-18 19:18:19,964] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-18 19:18:19,964] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-18 19:18:19,964] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-18 19:18:19,966] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-18 19:18:19,966] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-18 19:18:19,966] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-18 19:18:19,968] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-18 19:18:19,968] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-18 19:18:19,968] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-18 19:18:19,968] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-18 19:18:19,969] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-18 19:18:19,969] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-18 19:18:19,969] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-18 19:18:19,970] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-18 19:18:19,970] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-18 19:18:19,970] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-18 19:18:19,971] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-18 19:18:19,972] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-18 19:18:19,973] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-18 19:18:19,973] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-18 19:18:19,974] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-18 19:18:19,974] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 10
eval_loss: 1.1035, accuracy: 0.5508
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.5547, true_2_rate: 0.5466
joint log likelihood: tensor(-6026.9687500000)
r_win_average: -4.2373, r_win_min: -9.1250, r_win_max: 1.4766, r_win_std: 1.7288
r_lose_average: -6.3375, r_lose_min: -11.6875, r_lose_max: -1.5078, r_lose_std: 1.5926
eta_win_average: -1.2640, eta_win_min: -3.7500, eta_win_max: 1.7500, eta_win_std: 0.8522
eta_lose_average: -1.7289, eta_lose_min: -4.5938, eta_lose_max: 0.6875, eta_lose_std: 0.8353
p_win_average: -2.1518, p_win_min: -4.5000, p_win_max: 0.8125, p_win_std: 0.9741
p_lose_average: -2.2765, p_lose_min: -5.1875, p_lose_max: 0.5586, p_lose_std: 0.9590

eval_z_samples_size: 100
eval_loss: 1.2949, accuracy: 0.5078
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5170, true_2_rate: 0.4980
joint log likelihood: tensor(-5962.9687500000)
r_win_average: -0.5011, r_win_min: -4.6875, r_win_max: 3.7188, r_win_std: 1.4510
r_lose_average: -2.6510, r_lose_min: -7.0938, r_lose_max: 1.1797, r_lose_std: 1.7417
eta_win_average: -0.4239, eta_win_min: -1.3125, eta_win_max: 0.5977, eta_win_std: 0.2789
eta_lose_average: -0.3901, eta_lose_min: -1.3125, eta_lose_max: 0.6211, eta_lose_std: 0.2998
p_win_average: 0.5772, p_win_min: -0.7617, p_win_max: 1.6797, p_win_std: 0.4425
p_lose_average: 0.2388, p_lose_min: -1.1328, p_lose_max: 1.4922, p_lose_std: 0.4913

eval_z_samples_size: 500
eval_loss: 1.1865, accuracy: 0.5039
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5132, true_2_rate: 0.4939
joint log likelihood: tensor(-5953.4375000000)
r_win_average: -0.7616, r_win_min: -4.9688, r_win_max: 3.1406, r_win_std: 1.3591
r_lose_average: -2.6833, r_lose_min: -6.7500, r_lose_max: 0.8945, r_lose_std: 1.5836
eta_win_average: 0.0247, eta_win_min: -0.4941, eta_win_max: 0.4160, eta_win_std: 0.1512
eta_lose_average: -0.0758, eta_lose_min: -0.5273, eta_lose_max: 0.3496, eta_lose_std: 0.1756
p_win_average: -0.1567, p_win_min: -0.6094, p_win_max: 0.4785, p_win_std: 0.1957
p_lose_average: -0.0832, p_lose_min: -0.5391, p_lose_max: 0.4590, p_lose_std: 0.1986

------------------------------------------------------------------------------------------
epoch 1 step 50 evaluation
eval_z_samples_size: 10
eval_loss: 0.4763, accuracy: 0.7637
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5117, prediction_2_rate: 0.4883
true_1_rate: 0.7660, true_2_rate: 0.7611
joint log likelihood: tensor(-1477.4921875000)
r_win_average: 0.7837, r_win_min: -2.3125, r_win_max: 2.9688, r_win_std: 0.8136
r_lose_average: -0.4472, r_lose_min: -3.1406, r_lose_max: 2.1250, r_lose_std: 1.0432
eta_win_average: 1.8364, eta_win_min: -0.3809, eta_win_max: 2.6406, eta_win_std: 0.5316
eta_lose_average: 2.0151, eta_lose_min: 0.0884, eta_lose_max: 2.8281, eta_lose_std: 0.4907
p_win_average: -0.7937, p_win_min: -1.5547, p_win_max: -0.3789, p_win_std: 0.1560
p_lose_average: -0.7439, p_lose_min: -1.7344, p_lose_max: -0.2539, p_lose_std: 0.1925

eval_z_samples_size: 100
eval_loss: 0.4509, accuracy: 0.7910
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8000, true_2_rate: 0.7814
joint log likelihood: tensor(-1440.5156250000)
r_win_average: 0.0241, r_win_min: -3.9375, r_win_max: 3.1719, r_win_std: 1.0455
r_lose_average: -1.5264, r_lose_min: -4.4375, r_lose_max: 1.9766, r_lose_std: 1.2644
eta_win_average: -0.0260, eta_win_min: -0.2178, eta_win_max: 0.1074, eta_win_std: 0.0378
eta_lose_average: -0.0070, eta_lose_min: -0.2227, eta_lose_max: 0.1064, eta_lose_std: 0.0356
p_win_average: 0.2458, p_win_min: 0.0864, p_win_max: 0.3789, p_win_std: 0.0358
p_lose_average: 0.2627, p_lose_min: 0.0908, p_lose_max: 0.3633, p_lose_std: 0.0356

eval_z_samples_size: 500
eval_loss: 0.4514, accuracy: 0.7910
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8000, true_2_rate: 0.7814
joint log likelihood: tensor(-1430.9375000000)
r_win_average: -0.0275, r_win_min: -4.1250, r_win_max: 3.0469, r_win_std: 1.0706
r_lose_average: -1.6172, r_lose_min: -4.6250, r_lose_max: 2.0469, r_lose_std: 1.3093
eta_win_average: 0.2548, eta_win_min: 0.1416, eta_win_max: 0.3301, eta_win_std: 0.0182
eta_lose_average: 0.2620, eta_lose_min: 0.2012, eta_lose_max: 0.3340, eta_lose_std: 0.0160
p_win_average: -0.0866, p_win_min: -0.2051, p_win_max: 0.0361, p_win_std: 0.0281
p_lose_average: -0.0971, p_lose_min: -0.1943, p_lose_max: 0.0109, p_lose_std: 0.0248

------------------------------------------------------------------------------------------
epoch 1 step 100 evaluation
eval_z_samples_size: 10
eval_loss: 0.4695, accuracy: 0.7832
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5000, prediction_2_rate: 0.5000
true_1_rate: 0.7736, true_2_rate: 0.7935
joint log likelihood: tensor(-1286.4238281250)
r_win_average: 4.6216, r_win_min: 1.8438, r_win_max: 6.2500, r_win_std: 0.7510
r_lose_average: 3.2911, r_lose_min: -0.5391, r_lose_max: 5.7500, r_lose_std: 1.2237
eta_win_average: 4.6153, eta_win_min: 3.4531, eta_win_max: 5.9062, eta_win_std: 0.2640
eta_lose_average: 4.8859, eta_lose_min: 3.4062, eta_lose_max: 5.8750, eta_lose_std: 0.3338
p_win_average: -0.5892, p_win_min: -1.2031, p_win_max: 0.2344, p_win_std: 0.1369
p_lose_average: -0.6711, p_lose_min: -1.2344, p_lose_max: -0.0635, p_lose_std: 0.1502

eval_z_samples_size: 100
eval_loss: 0.4468, accuracy: 0.7969
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.7925, true_2_rate: 0.8016
joint log likelihood: tensor(-1330.1445312500)
r_win_average: -0.3402, r_win_min: -3.8906, r_win_max: 1.6094, r_win_std: 0.8776
r_lose_average: -1.9288, r_lose_min: -5.9062, r_lose_max: 0.8438, r_lose_std: 1.4240
eta_win_average: -0.8925, eta_win_min: -1.1016, eta_win_max: -0.6016, eta_win_std: 0.0622
eta_lose_average: -0.9439, eta_lose_min: -1.1328, eta_lose_max: -0.6562, eta_lose_std: 0.0635
p_win_average: -0.0489, p_win_min: -0.2363, p_win_max: 0.2471, p_win_std: 0.0449
p_lose_average: -0.0563, p_lose_min: -0.2070, p_lose_max: 0.1172, p_lose_std: 0.0445

eval_z_samples_size: 500
eval_loss: 0.4475, accuracy: 0.7988
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5078, prediction_2_rate: 0.4922
true_1_rate: 0.7962, true_2_rate: 0.8016
joint log likelihood: tensor(-1331.3046875000)
r_win_average: 0.1759, r_win_min: -3.2812, r_win_max: 2.0781, r_win_std: 0.8591
r_lose_average: -1.3766, r_lose_min: -5.2812, r_lose_max: 1.4531, r_lose_std: 1.4040
eta_win_average: -0.2695, eta_win_min: -0.3477, eta_win_max: -0.1689, eta_win_std: 0.0194
eta_lose_average: -0.2825, eta_lose_min: -0.3477, eta_lose_max: -0.1836, eta_lose_std: 0.0190
p_win_average: -0.1569, p_win_min: -0.2305, p_win_max: -0.0640, p_win_std: 0.0190
p_lose_average: -0.1642, p_lose_min: -0.3223, p_lose_max: -0.0786, p_lose_std: 0.0227

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 10
eval_loss: 0.4009, accuracy: 0.8164
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.7962, true_2_rate: 0.8381
joint log likelihood: tensor(-1112.0786132812)
r_win_average: 2.0921, r_win_min: -1.9219, r_win_max: 4.5938, r_win_std: 1.1953
r_lose_average: -0.0303, r_lose_min: -4.4688, r_lose_max: 3.8125, r_lose_std: 1.6260
eta_win_average: 1.2669, eta_win_min: 0.3535, eta_win_max: 2.2500, eta_win_std: 0.2312
eta_lose_average: 1.4014, eta_lose_min: 0.7695, eta_lose_max: 2.1094, eta_lose_std: 0.2180
p_win_average: 1.4917, p_win_min: 1.0625, p_win_max: 2.3750, p_win_std: 0.1431
p_lose_average: 1.4745, p_lose_min: 0.7344, p_lose_max: 1.9609, p_lose_std: 0.1450

eval_z_samples_size: 100
eval_loss: 0.3931, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4961, prediction_2_rate: 0.5039
true_1_rate: 0.8075, true_2_rate: 0.8381
joint log likelihood: tensor(-1146.2758789062)
r_win_average: 0.3615, r_win_min: -3.7344, r_win_max: 2.8750, r_win_std: 1.2049
r_lose_average: -1.8110, r_lose_min: -6.8125, r_lose_max: 2.2031, r_lose_std: 1.7197
eta_win_average: 0.4282, eta_win_min: 0.0996, eta_win_max: 0.6992, eta_win_std: 0.1061
eta_lose_average: 0.5097, eta_lose_min: 0.1973, eta_lose_max: 0.7812, eta_lose_std: 0.1052
p_win_average: 0.5956, p_win_min: 0.1650, p_win_max: 0.8203, p_win_std: 0.0627
p_lose_average: 0.5899, p_lose_min: 0.3125, p_lose_max: 0.7539, p_lose_std: 0.0596

eval_z_samples_size: 500
eval_loss: 0.3938, accuracy: 0.8281
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4902, prediction_2_rate: 0.5098
true_1_rate: 0.8075, true_2_rate: 0.8502
joint log likelihood: tensor(-1139.0224609375)
r_win_average: -0.6432, r_win_min: -4.9062, r_win_max: 1.8594, r_win_std: 1.2528
r_lose_average: -2.9075, r_lose_min: -7.8438, r_lose_max: 1.0703, r_lose_std: 1.7726
eta_win_average: -0.0830, eta_win_min: -0.1201, eta_win_max: 0.0142, eta_win_std: 0.0142
eta_lose_average: -0.0906, eta_lose_min: -0.1406, eta_lose_max: -0.0160, eta_lose_std: 0.0139
p_win_average: 0.1010, p_win_min: 0.0110, p_win_max: 0.1611, p_win_std: 0.0244
p_lose_average: 0.0948, p_lose_min: 0.0020, p_lose_max: 0.1631, p_lose_std: 0.0269

------------------------------------------------------------------------------------------
epoch 1 step 200 evaluation
eval_z_samples_size: 10
eval_loss: 0.3835, accuracy: 0.8203
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4824, prediction_2_rate: 0.5176
true_1_rate: 0.7925, true_2_rate: 0.8502
joint log likelihood: tensor(-1123.7607421875)
r_win_average: -2.3701, r_win_min: -6.4375, r_win_max: 0.2432, r_win_std: 1.1691
r_lose_average: -4.4028, r_lose_min: -8.8750, r_lose_max: -0.4883, r_lose_std: 1.5138
eta_win_average: 0.4411, eta_win_min: -0.1582, eta_win_max: 1.1797, eta_win_std: 0.2340
eta_lose_average: 0.2766, eta_lose_min: -0.3613, eta_lose_max: 1.1484, eta_lose_std: 0.1925
p_win_average: -2.4115, p_win_min: -3.0625, p_win_max: -1.9453, p_win_std: 0.1535
p_lose_average: -2.5208, p_lose_min: -3.0156, p_lose_max: -2.1094, p_lose_std: 0.1656

eval_z_samples_size: 100
eval_loss: 0.3831, accuracy: 0.8301
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8075, true_2_rate: 0.8543
joint log likelihood: tensor(-1114.9726562500)
r_win_average: 1.2103, r_win_min: -2.1250, r_win_max: 3.5312, r_win_std: 0.9810
r_lose_average: -0.5019, r_lose_min: -4.0000, r_lose_max: 2.4531, r_lose_std: 1.2595
eta_win_average: 1.2263, eta_win_min: 0.8047, eta_win_max: 1.4609, eta_win_std: 0.0697
eta_lose_average: 1.2824, eta_lose_min: 0.8555, eta_lose_max: 1.5547, eta_lose_std: 0.0696
p_win_average: 0.3817, p_win_min: 0.2598, p_win_max: 0.6758, p_win_std: 0.0445
p_lose_average: 0.3782, p_lose_min: 0.1826, p_lose_max: 0.5352, p_lose_std: 0.0395

eval_z_samples_size: 500
eval_loss: 0.3823, accuracy: 0.8223
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4844, prediction_2_rate: 0.5156
true_1_rate: 0.7962, true_2_rate: 0.8502
joint log likelihood: tensor(-1118.2480468750)
r_win_average: -0.4771, r_win_min: -3.9219, r_win_max: 1.8984, r_win_std: 1.0164
r_lose_average: -2.2625, r_lose_min: -5.9688, r_lose_max: 0.6797, r_lose_std: 1.3089
eta_win_average: -0.0563, eta_win_min: -0.1050, eta_win_max: -0.0074, eta_win_std: 0.0143
eta_lose_average: -0.0644, eta_lose_min: -0.1108, eta_lose_max: -0.0004, eta_lose_std: 0.0152
p_win_average: -0.0242, p_win_min: -0.0908, p_win_max: 0.0493, p_win_std: 0.0211
p_lose_average: -0.0347, p_lose_min: -0.1045, p_lose_max: 0.0461, p_lose_std: 0.0216

------------------------------------------------------------------------------------------
epoch 1 evaluation
eval_z_samples_size: 10
eval_loss: 0.3899, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4980, prediction_2_rate: 0.5020
true_1_rate: 0.8113, true_2_rate: 0.8381
joint log likelihood: tensor(-1081.4443359375)
r_win_average: 1.1123, r_win_min: -3.2031, r_win_max: 3.9688, r_win_std: 1.3250
r_lose_average: -1.1620, r_lose_min: -6.0938, r_lose_max: 2.8125, r_lose_std: 1.6926
eta_win_average: 2.8803, eta_win_min: 1.9531, eta_win_max: 3.7812, eta_win_std: 0.2564
eta_lose_average: 2.9672, eta_lose_min: 2.0156, eta_lose_max: 4.2812, eta_lose_std: 0.2401
p_win_average: -1.2078, p_win_min: -1.6484, p_win_max: -0.0347, p_win_std: 0.1964
p_lose_average: -1.2597, p_lose_min: -1.7109, p_lose_max: -0.6914, p_lose_std: 0.1639

eval_z_samples_size: 100
eval_loss: 0.3789, accuracy: 0.8262
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4883, prediction_2_rate: 0.5117
true_1_rate: 0.8038, true_2_rate: 0.8502
joint log likelihood: tensor(-1087.8115234375)
r_win_average: -0.0726, r_win_min: -4.7500, r_win_max: 2.9531, r_win_std: 1.3445
r_lose_average: -2.3946, r_lose_min: -6.8750, r_lose_max: 1.6953, r_lose_std: 1.7293
eta_win_average: 1.0411, eta_win_min: 0.8359, eta_win_max: 1.2422, eta_win_std: 0.0652
eta_lose_average: 1.0923, eta_lose_min: 0.8633, eta_lose_max: 1.3281, eta_lose_std: 0.0685
p_win_average: -0.5563, p_win_min: -0.8320, p_win_max: -0.0698, p_win_std: 0.1142
p_lose_average: -0.6147, p_lose_min: -0.8672, p_lose_max: -0.1953, p_lose_std: 0.0950

eval_z_samples_size: 500
eval_loss: 0.3765, accuracy: 0.8340
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4922, prediction_2_rate: 0.5078
true_1_rate: 0.8151, true_2_rate: 0.8543
joint log likelihood: tensor(-1086.0258789062)
r_win_average: -0.4899, r_win_min: -5.1562, r_win_max: 2.2812, r_win_std: 1.3290
r_lose_average: -2.8185, r_lose_min: -7.5000, r_lose_max: 1.1953, r_lose_std: 1.7242
eta_win_average: -0.0775, eta_win_min: -0.1650, eta_win_max: 0.0151, eta_win_std: 0.0284
eta_lose_average: -0.1053, eta_lose_min: -0.2021, eta_lose_max: -0.0190, eta_lose_std: 0.0298
p_win_average: 0.1438, p_win_min: -0.0615, p_win_max: 0.3457, p_win_std: 0.0453
p_lose_average: 0.1605, p_lose_min: 0.0168, p_lose_max: 0.3223, p_lose_std: 0.0409

------------------------------------------------------------------------------------------
[2023-09-18 20:45:41,670] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:56,596] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:56,790] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:56,904] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:56,967] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:57,071] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:57,072] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:57,084] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:45:57,087] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-09-18 20:46:02,142] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,142] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 20:46:02,668] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,668] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 20:46:02,671] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,671] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 20:46:02,800] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,800] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 20:46:02,803] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,803] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 20:46:02,818] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,818] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 20:46:02,818] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-09-18 20:46:02,823] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,823] [INFO] [comm.py:616:init_distributed] cdb=None
[2023-09-18 20:46:02,843] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[2023-09-18 20:46:02,843] [INFO] [comm.py:616:init_distributed] cdb=None
torch seed 742
cuda seed 742
torch seed 342
cuda seed 342
torch seed 442
cuda seed 442
wandb_dir /shared/share_mala/leon/Logs/wandb_logs/reward-enn/cooking/se_cooking_preference-ref_size10-enn_dim128-num_ref_train500-lr1e-05-weight_decay0.01-enn_lr0.001-enn_decay0.1-reward_lr0.0003-reward_decay0.1-gc1-train_batch_size4
torch seed 42
cuda seed 42
torch seed 242
cuda seed 242
torch seed 542
cuda seed 542
torch seed 642
cuda seed 642
torch seed 142
cuda seed 142
training dataset size: 1808
eval dataset size: 8
joint eval dataset size: 256
Total steps:  1808
Warmup steps:  54
[2023-09-18 20:46:40,800] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[2023-09-18 20:46:43,074] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-09-18 20:46:43,075] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-09-18 20:46:43,077] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-09-18 20:46:43,083] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2023-09-18 20:46:43,083] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2023-09-18 20:46:43,083] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2023-09-18 20:46:43,083] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[2023-09-18 20:46:43,083] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[2023-09-18 20:46:43,083] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[2023-09-18 20:46:43,083] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
Rank: 0 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 6 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 1 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 2 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 4 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 5 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 7 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
Rank: 3 partition count [8, 8, 8] and sizes[(428310000, False), (402, False), (53602, False)] 
[2023-09-18 20:46:56,134] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[2023-09-18 20:46:56,135] [INFO] [utils.py:786:see_memory_usage] MA 8.0 GB         Max_MA 8.0 GB         CA 8.0 GB         Max_CA 8 GB 
[2023-09-18 20:46:56,135] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.46 GB, percent = 6.3%
[2023-09-18 20:46:56,238] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[2023-09-18 20:46:56,238] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 15.98 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 20:46:56,239] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.46 GB, percent = 6.3%
[2023-09-18 20:46:56,241] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[2023-09-18 20:46:56,335] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[2023-09-18 20:46:56,336] [INFO] [utils.py:786:see_memory_usage] MA 11.19 GB         Max_MA 11.19 GB         CA 15.98 GB         Max_CA 16 GB 
[2023-09-18 20:46:56,338] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 63.46 GB, percent = 6.3%
[2023-09-18 20:46:56,340] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-09-18 20:46:56,340] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-09-18 20:46:56,342] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2023-09-18 20:46:56,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.0000000000000002e-06, 2.9999999999999997e-05, 0.0001], mom=[(0.9, 0.999), (0.9, 0.999), (0.9, 0.999)]
[2023-09-18 20:46:56,343] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[2023-09-18 20:46:56,345] [INFO] [config.py:964:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-09-18 20:46:56,346] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-09-18 20:46:56,346] [INFO] [config.py:964:print]   amp_enabled .................. False
[2023-09-18 20:46:56,346] [INFO] [config.py:964:print]   amp_params ................... False
[2023-09-18 20:46:56,346] [INFO] [config.py:964:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-09-18 20:46:56,347] [INFO] [config.py:964:print]   bfloat16_enabled ............. True
[2023-09-18 20:46:56,347] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[2023-09-18 20:46:56,347] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[2023-09-18 20:46:56,347] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[2023-09-18 20:46:56,349] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f22ac26b590>
[2023-09-18 20:46:56,349] [INFO] [config.py:964:print]   communication_data_type ...... None
[2023-09-18 20:46:56,349] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-09-18 20:46:56,349] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[2023-09-18 20:46:56,351] [INFO] [config.py:964:print]   curriculum_params_legacy ..... False
[2023-09-18 20:46:56,351] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-09-18 20:46:56,351] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[2023-09-18 20:46:56,351] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[2023-09-18 20:46:56,351] [INFO] [config.py:964:print]   disable_allgather ............ False
[2023-09-18 20:46:56,352] [INFO] [config.py:964:print]   dump_state ................... False
[2023-09-18 20:46:56,352] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... None
[2023-09-18 20:46:56,352] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[2023-09-18 20:46:56,352] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[2023-09-18 20:46:56,352] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-09-18 20:46:56,353] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[2023-09-18 20:46:56,353] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[2023-09-18 20:46:56,353] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[2023-09-18 20:46:56,353] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[2023-09-18 20:46:56,353] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[2023-09-18 20:46:56,354] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[2023-09-18 20:46:56,354] [INFO] [config.py:964:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-09-18 20:46:56,355] [INFO] [config.py:964:print]   fp16_auto_cast ............... None
[2023-09-18 20:46:56,356] [INFO] [config.py:964:print]   fp16_enabled ................. False
[2023-09-18 20:46:56,356] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[2023-09-18 20:46:56,356] [INFO] [config.py:964:print]   global_rank .................. 0
[2023-09-18 20:46:56,356] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[2023-09-18 20:46:56,356] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[2023-09-18 20:46:56,357] [INFO] [config.py:964:print]   gradient_clipping ............ 0.0
[2023-09-18 20:46:56,357] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[2023-09-18 20:46:56,357] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-09-18 20:46:56,358] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 1
[2023-09-18 20:46:56,358] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[2023-09-18 20:46:56,358] [INFO] [config.py:964:print]   loss_scale ................... 1.0
[2023-09-18 20:46:56,358] [INFO] [config.py:964:print]   memory_breakdown ............. False
[2023-09-18 20:46:56,360] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[2023-09-18 20:46:56,360] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[2023-09-18 20:46:56,360] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-09-18 20:46:56,361] [INFO] [config.py:964:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-09-18 20:46:56,361] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[2023-09-18 20:46:56,362] [INFO] [config.py:964:print]   optimizer_name ............... None
[2023-09-18 20:46:56,362] [INFO] [config.py:964:print]   optimizer_params ............. None
[2023-09-18 20:46:56,363] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-09-18 20:46:56,363] [INFO] [config.py:964:print]   pld_enabled .................. False
[2023-09-18 20:46:56,364] [INFO] [config.py:964:print]   pld_params ................... False
[2023-09-18 20:46:56,364] [INFO] [config.py:964:print]   prescale_gradients ........... False
[2023-09-18 20:46:56,364] [INFO] [config.py:964:print]   scheduler_name ............... None
[2023-09-18 20:46:56,366] [INFO] [config.py:964:print]   scheduler_params ............. None
[2023-09-18 20:46:56,366] [INFO] [config.py:964:print]   sparse_attention ............. None
[2023-09-18 20:46:56,366] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[2023-09-18 20:46:56,367] [INFO] [config.py:964:print]   steps_per_print .............. inf
[2023-09-18 20:46:56,367] [INFO] [config.py:964:print]   train_batch_size ............. 8
[2023-09-18 20:46:56,367] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[2023-09-18 20:46:56,368] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[2023-09-18 20:46:56,368] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[2023-09-18 20:46:56,368] [INFO] [config.py:964:print]   world_size ................... 8
[2023-09-18 20:46:56,369] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  True
[2023-09-18 20:46:56,371] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[2023-09-18 20:46:56,372] [INFO] [config.py:964:print]   zero_enabled ................. True
[2023-09-18 20:46:56,372] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[2023-09-18 20:46:56,372] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[2023-09-18 20:46:56,374] [INFO] [config.py:950:print_user_config]   json = {
    "train_batch_size": 8, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 1, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "none", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "none", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": false
    }, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
--------------------------------------start training--------------------------------------
epoch 0
eval before training
eval_z_samples_size: 10
eval_loss: 1.1035, accuracy: 0.5508
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5059, prediction_2_rate: 0.4941
true_1_rate: 0.5547, true_2_rate: 0.5466
joint log likelihood: tensor(-6026.9687500000)
r_win_average: -4.2373, r_win_min: -9.1250, r_win_max: 1.4766, r_win_std: 1.7288
r_lose_average: -6.3375, r_lose_min: -11.6875, r_lose_max: -1.5078, r_lose_std: 1.5926
eta_win_average: -1.2640, eta_win_min: -3.7500, eta_win_max: 1.7500, eta_win_std: 0.8522
eta_lose_average: -1.7289, eta_lose_min: -4.5938, eta_lose_max: 0.6875, eta_lose_std: 0.8353
p_win_average: -2.1518, p_win_min: -4.5000, p_win_max: 0.8125, p_win_std: 0.9741
p_lose_average: -2.2765, p_lose_min: -5.1875, p_lose_max: 0.5586, p_lose_std: 0.9590

eval_z_samples_size: 100
eval_loss: 1.2949, accuracy: 0.5078
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5170, true_2_rate: 0.4980
joint log likelihood: tensor(-5962.9687500000)
r_win_average: -0.5011, r_win_min: -4.6875, r_win_max: 3.7188, r_win_std: 1.4510
r_lose_average: -2.6510, r_lose_min: -7.0938, r_lose_max: 1.1797, r_lose_std: 1.7417
eta_win_average: -0.4239, eta_win_min: -1.3125, eta_win_max: 0.5977, eta_win_std: 0.2789
eta_lose_average: -0.3901, eta_lose_min: -1.3125, eta_lose_max: 0.6211, eta_lose_std: 0.2998
p_win_average: 0.5772, p_win_min: -0.7617, p_win_max: 1.6797, p_win_std: 0.4425
p_lose_average: 0.2388, p_lose_min: -1.1328, p_lose_max: 1.4922, p_lose_std: 0.4913

eval_z_samples_size: 500
eval_loss: 1.1865, accuracy: 0.5039
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5098, prediction_2_rate: 0.4902
true_1_rate: 0.5132, true_2_rate: 0.4939
joint log likelihood: tensor(-5953.4375000000)
r_win_average: -0.7616, r_win_min: -4.9688, r_win_max: 3.1406, r_win_std: 1.3591
r_lose_average: -2.6833, r_lose_min: -6.7500, r_lose_max: 0.8945, r_lose_std: 1.5836
eta_win_average: 0.0247, eta_win_min: -0.4941, eta_win_max: 0.4160, eta_win_std: 0.1512
eta_lose_average: -0.0758, eta_lose_min: -0.5273, eta_lose_max: 0.3496, eta_lose_std: 0.1756
p_win_average: -0.1567, p_win_min: -0.6094, p_win_max: 0.4785, p_win_std: 0.1957
p_lose_average: -0.0832, p_lose_min: -0.5391, p_lose_max: 0.4590, p_lose_std: 0.1986

------------------------------------------------------------------------------------------
epoch 1 step 50 evaluation
eval_z_samples_size: 10
eval_loss: 0.4614, accuracy: 0.7949
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5195, prediction_2_rate: 0.4805
true_1_rate: 0.8038, true_2_rate: 0.7854
joint log likelihood: tensor(-1589.1875000000)
r_win_average: -0.7132, r_win_min: -5.0312, r_win_max: 3.4688, r_win_std: 1.2259
r_lose_average: -2.4032, r_lose_min: -5.6875, r_lose_max: 1.5859, r_lose_std: 1.4149
eta_win_average: 0.7159, eta_win_min: 0.1904, eta_win_max: 1.5781, eta_win_std: 0.2069
eta_lose_average: 0.6886, eta_lose_min: 0.0579, eta_lose_max: 1.5781, eta_lose_std: 0.1577
p_win_average: -1.1504, p_win_min: -2.3125, p_win_max: -0.3828, p_win_std: 0.2036
p_lose_average: -1.2245, p_lose_min: -1.6250, p_lose_max: -0.4922, p_lose_std: 0.1450

eval_z_samples_size: 100
eval_loss: 0.4465, accuracy: 0.8027
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.8189, true_2_rate: 0.7854
joint log likelihood: tensor(-1576.5000000000)
r_win_average: -0.9507, r_win_min: -4.7500, r_win_max: 1.7578, r_win_std: 0.9888
r_lose_average: -2.5075, r_lose_min: -5.6562, r_lose_max: 0.7461, r_lose_std: 1.2877
eta_win_average: -0.4676, eta_win_min: -1.0859, eta_win_max: -0.1099, eta_win_std: 0.1497
eta_lose_average: -0.4311, eta_lose_min: -1.5391, eta_lose_max: -0.0557, eta_lose_std: 0.1223
p_win_average: -0.2066, p_win_min: -0.5820, p_win_max: 0.1836, p_win_std: 0.0798
p_lose_average: -0.2068, p_lose_min: -0.6641, p_lose_max: 0.0378, p_lose_std: 0.0687

eval_z_samples_size: 500
eval_loss: 0.4495, accuracy: 0.7949
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5273, prediction_2_rate: 0.4727
true_1_rate: 0.8113, true_2_rate: 0.7773
joint log likelihood: tensor(-1580.5898437500)
r_win_average: -0.5395, r_win_min: -4.6250, r_win_max: 2.4844, r_win_std: 1.0608
r_lose_average: -2.1412, r_lose_min: -5.3125, r_lose_max: 1.5156, r_lose_std: 1.3254
eta_win_average: -0.0950, eta_win_min: -0.2578, eta_win_max: 0.0136, eta_win_std: 0.0344
eta_lose_average: -0.0766, eta_lose_min: -0.2715, eta_lose_max: 0.0137, eta_lose_std: 0.0306
p_win_average: -0.1710, p_win_min: -0.2676, p_win_max: 0.0571, p_win_std: 0.0398
p_lose_average: -0.1920, p_lose_min: -0.2891, p_lose_max: -0.0454, p_lose_std: 0.0294

------------------------------------------------------------------------------------------
epoch 1 step 100 evaluation
eval_z_samples_size: 10
eval_loss: 0.4739, accuracy: 0.7793
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.7736, true_2_rate: 0.7854
joint log likelihood: tensor(-1632.5195312500)
r_win_average: 2.3209, r_win_min: -0.7227, r_win_max: 4.0312, r_win_std: 0.7509
r_lose_average: 1.0254, r_lose_min: -2.3281, r_lose_max: 3.6250, r_lose_std: 1.1891
eta_win_average: 2.5772, eta_win_min: 1.6016, eta_win_max: 3.5156, eta_win_std: 0.2738
eta_lose_average: 2.7442, eta_lose_min: 1.4922, eta_lose_max: 3.6719, eta_lose_std: 0.3359
p_win_average: -0.7684, p_win_min: -1.1328, p_win_max: -0.0703, p_win_std: 0.1256
p_lose_average: -0.7505, p_lose_min: -1.6172, p_lose_max: -0.2695, p_lose_std: 0.1543

eval_z_samples_size: 100
eval_loss: 0.4497, accuracy: 0.8066
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.8000, true_2_rate: 0.8138
joint log likelihood: tensor(-1721.0156250000)
r_win_average: 0.2611, r_win_min: -3.2188, r_win_max: 2.0312, r_win_std: 0.8478
r_lose_average: -1.2640, r_lose_min: -5.2500, r_lose_max: 1.5703, r_lose_std: 1.4017
eta_win_average: -0.8740, eta_win_min: -1.1016, eta_win_max: -0.6719, eta_win_std: 0.0533
eta_lose_average: -0.8915, eta_lose_min: -1.1094, eta_lose_max: -0.7188, eta_lose_std: 0.0536
p_win_average: 0.5993, p_win_min: 0.4551, p_win_max: 0.7461, p_win_std: 0.0345
p_lose_average: 0.6199, p_lose_min: 0.5234, p_lose_max: 0.7734, p_lose_std: 0.0375

eval_z_samples_size: 500
eval_loss: 0.4497, accuracy: 0.8027
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.5039, prediction_2_rate: 0.4961
true_1_rate: 0.7962, true_2_rate: 0.8097
joint log likelihood: tensor(-1723.5156250000)
r_win_average: 1.0011, r_win_min: -2.2656, r_win_max: 2.7812, r_win_std: 0.8242
r_lose_average: -0.4989, r_lose_min: -4.3750, r_lose_max: 2.2500, r_lose_std: 1.3738
eta_win_average: 0.1118, eta_win_min: 0.0074, eta_win_max: 0.2090, eta_win_std: 0.0236
eta_lose_average: 0.1231, eta_lose_min: 0.0522, eta_lose_max: 0.2432, eta_lose_std: 0.0270
p_win_average: 0.3532, p_win_min: 0.2324, p_win_max: 0.4531, p_win_std: 0.0302
p_lose_average: 0.3705, p_lose_min: 0.2305, p_lose_max: 0.4590, p_lose_std: 0.0335

------------------------------------------------------------------------------------------
epoch 1 step 150 evaluation
eval_z_samples_size: 10
eval_loss: 0.4001, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8075, true_2_rate: 0.8421
joint log likelihood: tensor(-1440.1333007812)
r_win_average: 1.0319, r_win_min: -2.2188, r_win_max: 3.7500, r_win_std: 1.0567
r_lose_average: -0.8425, r_lose_min: -5.2500, r_lose_max: 2.7344, r_lose_std: 1.4639
eta_win_average: 2.0918, eta_win_min: 1.1250, eta_win_max: 3.0625, eta_win_std: 0.3067
eta_lose_average: 2.4160, eta_lose_min: 1.0703, eta_lose_max: 3.2500, eta_lose_std: 0.3371
p_win_average: -0.2847, p_win_min: -0.7070, p_win_max: 0.0172, p_win_std: 0.1108
p_lose_average: -0.2167, p_lose_min: -0.6016, p_lose_max: 0.1426, p_lose_std: 0.1183

eval_z_samples_size: 100
eval_loss: 0.3977, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8075, true_2_rate: 0.8421
joint log likelihood: tensor(-1478.1855468750)
r_win_average: -2.4821, r_win_min: -7.2812, r_win_max: 0.3945, r_win_std: 1.3843
r_lose_average: -4.9487, r_lose_min: -10.1250, r_lose_max: -0.6758, r_lose_std: 1.9224
eta_win_average: -0.8980, eta_win_min: -1.2422, eta_win_max: -0.5156, eta_win_std: 0.0927
eta_lose_average: -0.9865, eta_lose_min: -1.3438, eta_lose_max: -0.7695, eta_lose_std: 0.1001
p_win_average: -0.8191, p_win_min: -1.1250, p_win_max: -0.4805, p_win_std: 0.0880
p_lose_average: -0.9104, p_lose_min: -1.1641, p_lose_max: -0.5977, p_lose_std: 0.0956

eval_z_samples_size: 500
eval_loss: 0.3926, accuracy: 0.8242
label_1_rate: 0.5176, label_2_rate: 0.4824, prediction_1_rate: 0.4941, prediction_2_rate: 0.5059
true_1_rate: 0.8075, true_2_rate: 0.8421
joint log likelihood: tensor(-1472.2343750000)
r_win_average: -1.1507, r_win_min: -5.6562, r_win_max: 1.4766, r_win_std: 1.2967
r_lose_average: -3.4877, r_lose_min: -8.5625, r_lose_max: 0.5547, r_lose_std: 1.8262
eta_win_average: -0.5309, eta_win_min: -0.6797, eta_win_max: -0.3418, eta_win_std: 0.0469
eta_lose_average: -0.5825, eta_lose_min: -0.8438, eta_lose_max: -0.3574, eta_lose_std: 0.0567
p_win_average: 0.1435, p_win_min: -0.0669, p_win_max: 0.4199, p_win_std: 0.0392
p_lose_average: 0.1491, p_lose_min: -0.0060, p_lose_max: 0.2617, p_lose_std: 0.0357

------------------------------------------------------------------------------------------
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '10,100,500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '10,100,500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 500, 'eval_z_size_list': '10,100,500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '10,100,500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '10,100,500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 500, 'eval_z_size_list': '10,100,500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 10, 'eval_z_size_list': '10,100,500', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 100, 'eval_z_size_list': '10,100,500', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 10, 'num_ref_train': 500, 'eval_z_size_list': '10,100,500', 'hidden_size': 256, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 50, 'num_ref_train': 10, 'eval_z_size_list': '10,100,500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 50, 'num_ref_train': 100, 'eval_z_size_list': '10,100,500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 50, 'num_ref_train': 500, 'eval_z_size_list': '10,100,500', 'hidden_size': 64, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 50, 'num_ref_train': 10, 'eval_z_size_list': '10,100,500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 50, 'num_ref_train': 100, 'eval_z_size_list': '10,100,500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
{'seed': 42, 'user': 'leon', 'project': 'reward-enn', 'wandb_project': 'cooking', 'backbone_model': '/shared/share_mala/leon/llama-3b-sft-cooking', 'dataset_name': 'se_cooking_preference', 'ref_size': 50, 'num_ref_train': 500, 'eval_z_size_list': '10,100,500', 'hidden_size': 128, 'output_size': 1, 'enn_gain': 1.0, 'lmbda': 1.0, 'reward_gain': 1, 'reward_lr': 0.0003, 'reward_decay': 0.1, 'lr': 1e-05, 'enn_lr': 0.001, 'enn_decay': 0.1, 'num_epochs': 1, 'warmup_ratio': 0.03, 'gradient_acc': 1, 'weight_decay': 0.01, 'train_batch_size': 4, 'eval_batch_size': 64, 'eval_steps': 50, 'max_length': 512, 'flash_attn': False, 'bf16': True, 'fp16': False}
